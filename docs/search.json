[
  {
    "objectID": "tools/index.html",
    "href": "tools/index.html",
    "title": "Stan Toolkit",
    "section": "",
    "text": "A curated collection of tools and interfaces to help you work effectively with Stan across various programming environments and stages of your modeling workflow."
  },
  {
    "objectID": "tools/index.html#language-specific-stan-interfaces",
    "href": "tools/index.html#language-specific-stan-interfaces",
    "title": "Stan Toolkit",
    "section": "Language-Specific Stan Interfaces",
    "text": "Language-Specific Stan Interfaces\nWrite, compile, and run Stan models directly within your programming environment.\n\n\n\n\n\n\n\n\nLanguage\nTool\nDescription\n\n\n\n\nR\nCmdStanR\nInterface to Stan for R, based on CmdStan. Recommended interface for R users.\n\n\nPython\nCmdStanPy\nInterface to Stan for Python, based on CmdStan. Recommended interface for Python users.\n\n\nWeb\nStan Playground\nBrowser-based editor and runtime environment for Stan models. Highly recommended for new users.\n\n\nJulia\nStan.jl\nInterface to Stan for Julia users.\n\n\nMATLAB\nMatlabStan\nInterface to Stan for MATLAB users.\n\n\nShell\nCmdStan\nCommand-line interface to Stan, usable from any shell environment.\n\n\nR\nRStan\nR interface to Stan.\n\n\nPython\nPyStan\nLegacy Python interface to Stan.\n\n\n\nFor detailed installation instructions see Getting Started."
  },
  {
    "objectID": "tools/index.html#high-level-modeling-interfaces",
    "href": "tools/index.html#high-level-modeling-interfaces",
    "title": "Stan Toolkit",
    "section": "High-Level Modeling Interfaces",
    "text": "High-Level Modeling Interfaces\nSimplify model specification in R.\n\n\n\n\n\n\n\n\nLanguage\nTool\nDescription\n\n\n\n\nR\nbrms\nUse extended lme4-like formula syntax to specify and fit multivariate and multilevel models in Stan. Requires CmdStanR or RStan, plus C++ compiler.\n\n\nR\nrstanarm\nProvides robust and efficient pre-compiled Stan versions of R model-fitting packages. Easily installed from CRAN, no C++ compiler needed."
  },
  {
    "objectID": "tools/index.html#visualization-diagnostics-and-validation-tools",
    "href": "tools/index.html#visualization-diagnostics-and-validation-tools",
    "title": "Stan Toolkit",
    "section": "Visualization, Diagnostics, and Validation Tools",
    "text": "Visualization, Diagnostics, and Validation Tools\nValidate, visualize, and compare fitted models to ensure robust results.\n\n\n\n\n\n\n\n\nLanguage\nTool\nDescription\n\n\n\n\nR\nbayesplot\nPlotting functions for posterior analysis, diagnostics, and model checking.\n\n\nR\nloo\nEfficient leave-one-out cross-validation and WAIC for Bayesian models.\n\n\nR\nposterior\nTools for working with posterior distributions.\n\n\nR\nprojpred\nProjection predictive variable selection for Bayesian models.\n\n\nPython\nArviZ\nExploratory analysis of Bayesian models with extensive visualization capabilities.\n\n\nJulia\nArviZ.jl\nJulia interface to ArviZ for Bayesian analysis.\n\n\nWeb\nMCMC Monitor\nWeb-based tool for monitoring MCMC diagnostics."
  },
  {
    "objectID": "tools/index.html#developer-tools-and-apis",
    "href": "tools/index.html#developer-tools-and-apis",
    "title": "Stan Toolkit",
    "section": "Developer Tools and APIs",
    "text": "Developer Tools and APIs\nAccess Stan’s computational backend for advanced applications and development.\n\n\n\n\n\n\n\n\nLanguage\nTool\nDescription\n\n\n\n\nC++\nStan Math Library\nAutomatic differentiation and mathematical functions used by Stan.\n\n\nMultiple Languages\nBridgeStan\nLibrary providing bindings to a model’s log densities, gradients, and more for C++, Python, Julia, R, and Rust.\n\n\nR\nrstantools\nTools for developers of R packages interfacing with Stan."
  },
  {
    "objectID": "tools/index.html#editor-and-ide-support",
    "href": "tools/index.html#editor-and-ide-support",
    "title": "Stan Toolkit",
    "section": "Editor and IDE Support",
    "text": "Editor and IDE Support\nEnhance your coding experience with Stan language support in your favorite editor.\n\n\n\n\n\n\n\n\nEditor/IDE\nTool\nDescription\n\n\n\n\nRStudio\nBuilt-in Support\nRStudio 1.2+ includes Stan syntax highlighting and code snippets. (Source code)\n\n\nVisual Studio Code\nStan VSCode Extension\nStan language support with syntax highlighting, autocompletion, and snippets.\n\n\nEmacs\nstan-mode\nMajor mode for Stan with syntax highlighting and indentation.\n\n\nVim\nstan-vim\nSyntax highlighting, indentation, and code folding for Stan in Vim.\n\n\nAtom\nlanguage-stan\nStan language support in Atom editor.\n\n\nSublime Text\nSublimeStan\nSyntax highlighting for Stan in Sublime Text.\n\n\nJupyterLab\njupyterlab-stan-highlight\nSyntax highlighting for Stan code blocks in JupyterLab.\n\n\nJavascript\nPrism  Highlight.js\nLightweight syntax highlighting library - (Source code).  Syntax highlighter written in javascript - (Source code)\n\n\nMarkdown Editors\nPandoc  Pygments\nStan syntax highlighting for document formats. (Source code)Python highlighter for Stan code blocks (Source code)\n\n\nLaTeX\nlstbayes\nLaTeX listings for Stan syntax highlighting."
  },
  {
    "objectID": "tools/index.html#software-packages-built-on-stan",
    "href": "tools/index.html#software-packages-built-on-stan",
    "title": "Stan Toolkit",
    "section": "Software Packages built on Stan",
    "text": "Software Packages built on Stan\n\n\n\n\n\n\n\n\nLanguage\nTool\nDescription\n\n\n\n\nPython, R\nPROPHET\nForecasting at scale for time series data.\n\n\nR\nblavaan\nR package for Bayesian latent variable analysis.\n\n\nR\nrethinking\nSoftware for book Statistical Rethinking\n\n\n\nAdditional R packages on CRAN which are built on Stan can be found in the reverse links from the packages rstan and rstanarm."
  },
  {
    "objectID": "learn-stan/stancon/StanCon2020-program.html",
    "href": "learn-stan/stancon/StanCon2020-program.html",
    "title": "StanCon 2020. A 24h Global Event.",
    "section": "",
    "text": "StanCon Thursday, 13 August 2020.\nThe conference was a 24-hour event with three main sessions spanning across different time zones (British Summer Time, Eastern Time and Pacific Time).\n\n\nThe sessions were recorded and uploaded to YouTube. The plenary talks and the discussions can be found in the recording for the live sessions.\nThe abstracts to the talks are included in the video descriptions.\n\n\nPlaylists: - Session 1 - Session 2 - Session 3 - All videos\n\n\n\n\n\n\nBST\nEST\nPT\n\n\n\n\n\n\n\n\n\n\n\n08:30\n03:30\n00:30\nVirtual coffee\n\n\n09:00\n04:00\n01:00\nLive welcome from StanCon UK Committee\n\n\n09:04\n04:04\n01:04\n6 x 1 minute pitches\n\n\n09:10\n04:10\n01:10\nPlenary: Hierarchical Models for Covid -\n\n\n\n\n\nidentifying effects of lockdown and an R package,\n\n\n\n\n\nSeth Flaxman, Imperial College, London\n\n\n\n\n\nvideo\n\n\n09:50\n04:50\n01:50\nDiscussion\n\n\n10:00\n05:00\n02:00\n6 x 1 minute pitches, followed by 9 minute discussions:\n\n\n10:06\n05:06\n02:06\nHierarchical Bayes and logistic growth: Predicting diagnosed\n\n\n\n\n\nCOVID19 cases and the corresponding burden on health care\n\n\n\n\n\nsystems, Brynjólfur Gauti Jónsson, Birgir Hrafnkelsson,\n\n\n\n\n\nJón Magnús Jónsson, Jóhanna Jakobsdóttir,\n\n\n\n\n\nBergdís Björg Sigurjónsdóttir,Þórarinn Jónmundsson,\n\n\n\n\n\nSigríður Haraldsdóttir, Unnur A. Valdimarsdóttir,\n\n\n\n\n\nThor Aspelund, Directorate of Health (Iceland),\n\n\n\n\n\nCenter of Public Health Sciences (University of Iceland),\n\n\n\n\n\nDepartment of Mathematics (University of Iceland),\n\n\n\n\n\nIcelandic Heart Association, National University Hospital of Iceland\n\n\n\n\n\nvideo\n\n\n10:15\n05:15\n02:15\nProcess fault detection using Stan, Jerzy Baranowski,\n\n\n\n\n\nWaldemar Bauer, Rafał Mularczyk, Bartłomiej Gondek,\n\n\n\n\n\nAGH University of Science & Technology, Kraków, Poland\n\n\n\n\n\nvideo\n\n\n10:24\n05:24\n02:24\nFailure prediction in hierarchical equipment system:\n\n\n\n\n\nspline fitting naval ship failure, Hyunji Moon, Jinwoo Choi,\n\n\n\n\n\nHyeonseop Lee, Seoul National University\n\n\n\n\n\nvideo\n\n\n10:33\n05:33\n02:33\nbaggr: a new meta-analysis package using Stan,\n\n\n\n\n\nWitold Wiecek and Rachael Meager, London School of Economics\n\n\n\n\n\nvideo\n\n\n10:42\n05:42\n02:42\nTowards an Interface for Streaming Stan,\n\n\n\n\n\nSimon Maskell and Alessandro Varsi, University of Liverpool\n\n\n\n\n\nvideo\n\n\n10:51\n05:51\n02:51\nEnforcing stationarity through the prior in vector autoregressions,\n\n\n\n\n\nSarah Heaps, Newcastle University\n\n\n\n\n\nvideo\n\n\n11:00\n06:00\n03:00\nVirtual coffee and poster session\n\n\n13:30\n08:30\n05:30\nEnd of session\n\n\n\n\n\n\n\n\n\nBST\nEST\nPT\n\n\n\n\n\n16:30\n11:30\n08:30\nVirtual coffee\n\n\n17:00\n12:00\n09:00\nLive welcome from StanCon UK + US Committee\n\n\n17:10\n12:10\n09:10\nCode of Conduct working group, Steve Bronder and Lauren Kennedy\n\n\n\n\n\nvideo\n\n\n17:14\n12:14\n09:14\nStan Governing Body update, Jonah Gabry\n\n\n\n\n\nvideo\n\n\n17:20\n12:20\n09:20\n6 x 1 minute pitches\n\n\n17:32\n12:32\n09:32\nDeveloper talk 1: posteriordb: a database with data, models and\n\n\n\n\n\nposteriors, Aki Vehtari, Måns Magnusson, Aalto University\n\n\n\n\n\nvideo\n\n\n17:39\n12:39\n09:39\nDeveloper talk 2: Scaling Stan’s performance with reduce_sum in practice,\n\n\n\n\n\nSebastian Weber, Ben Bales, Steve Bronder, Mitzi Morris,\n\n\n\n\n\nRok Češnovar, Imperial College of London COVID-19 team,\n\n\n\n\n\nNovartis Pharma AG, Basel, Switzerland, Columbia University,\n\n\n\n\n\nUniversity of Ljubljana\n\n\n\n\n\nvideo\n\n\n17:46\n12:46\n09:46\nDeveloper talk 3: The State of GPU Computation Support for Stan,\n\n\n\n\n\nSteve Bronder, Erik Štrumbelj, Rok Češnovar, Davor Sluga,\n\n\n\n\n\nJure Demšar, Tadej Ciglarič, Sean Talts, Columbia University,\n\n\n\n\n\nUniversity of Ljubljana\n\n\n\n\n\nvideo\n\n\n17:53\n12:53\n09:53\nDeveloper talk 4: Approximate Bayesian inference for latent Gaussian\n\n\n\n\n\nmodels in Stan, Charles Margossian, Aki Vehtari, Daniel Simpson,\n\n\n\n\n\nRaj Agrawal, Columbia University, Department of Statistics;\n\n\n\n\n\nAalto University, Department of Computer Science;\n\n\n\n\n\nUniversity of Toronto, Department of Statistical Sciences;\n\n\n\n\n\nMassachusetts Institute of Technology, CSAIL\n\n\n\n\n\nvideo, code and notebook\n\n\n18:00\n13:00\n10:00\nPlenary: Multi-Source Information Modeling, Curation, and\n\n\n\n\n\nFusion Enabling Transdisciplinary Decision-Making: A Case for Space!\n\n\n\n\n\nMoriba Jah, Oden Institute for Computational Engineering & Sciences\n\n\n\n\n\nvideo\n\n\n18:50\n13:50\n10:50\nDiscussion\n\n\n19:00\n14:00\n11:00\n6 x 1 minute pitches, followed by 9 minute discussions\n\n\n19:06\n14:06\n11:06\nUsing Stan for Spatial Smoothing of Regression Parameters,\n\n\n\n\n\nWade Brorsen, Bart Niyizibi, Davood Pourvina, Eunchun Park,\n\n\n\n\n\nOklahoma State University\n\n\n\n\n\nvideo\n\n\n19:15\n14:15\n11:15\nMultivariate Synthetic Control to Estimate Missing Data in\n\n\n\n\n\nHierarchically Correlated Series, Sean Pinkney, Tyler Morrison,\n\n\n\n\n\nGaurav Shahane, Comscore, Inc.\n\n\n\n\n\nvideo\n\n\n19:24\n14:24\n11:24\nBayesian Causal Effect Estimation with Stan: Parametric and\n\n\n\n\n\nNonparametric Approaches, Arman Oganisian, Jason A. Roy,\n\n\n\n\n\nUniversity of Pennsylvania\n\n\n\n\n\nvideo\n\n\n19:33\n14:33\n11:33\nBuilding effective uncertainty visualizations with Stan/brms,\n\n\n\n\n\ntidybayes, and ggdist, Matthew Kay, University of Michigan\n\n\n\n\n\nvideo\n\n\n19:42\n14:42\n11:42\nAPI for iterative interrogation of stanfit objects in R,\n\n\n\n\n\nJonathan Sidi, Associate Director of Modeling and Simulation at\n\n\n\n\n\nSage Therapeutics\n\n\n\n\n\nvideo\n\n\n19:51\n14:51\n11:51\nBayesian Inference without Probability Density Functions,\n\n\n\n\n\nBen Goodrich, Columbia University\n\n\n\n\n\nvideo\n\n\n20:00\n15:00\n12:00\nVirtual coffee and poster session\n\n\n21:30\n16:30\n13:30\nEnd of session\n\n\n\n\n\n\n\n\n\nBST\nEST\nPT\n\n\n\n\n\n00:30\n19:30\n16:30\nVirtual coffee\n\n\n01:00\n20:00\n17:00\nLive welcome from StanCon Committee\n\n\n01:04\n20:04\n17:04\n6 x 1 minute pitches\n\n\n01:10\n20:10\n17:10\nPlenary: STAN and US Politics, David Shor\n\n\n\n\n\nvideo\n\n\n01:50\n20:50\n17:50\nDiscussion\n\n\n02:00\n21:00\n18:00\n6 x 1 minute pitches, followed by 9 minute discussions:\n\n\n02:06\n21:06\n18:06\nSpatial models for plant neighborhood dynamics in Stan,\n\n\n\n\n\nCristina Barber, Andrii Zaiats, Cara Applestein, Trevor Caughlin,\n\n\n\n\n\nBoise State University\n\n\n\n\n\nvideo\n\n\n02:15\n21:15\n18:15\n​Stress-testing the Dawid–Skene model: Analysis of repeated\n\n\n\n\n\ncategorical ratings, Damjan Vukcevic, Jeffrey Pullin, Lyle Gurrin,\n\n\n\n\n\nUniversity of Melbourne\n\n\n\n\n\nvideo\n\n\n02:24\n21:24\n18:24\nArviZ, InferenceData, and NetCDF: A unified file format for Bayesians,\n\n\n\n\n\nRavin Kumar, Oriol Abril-Pla, Osvaldo Martin, Piyush Gautam,\n\n\n\n\n\nAri Hartikainen , Alexandre Andorra, ArviZ (all)\n\n\n\n\n\nvideo (English)\n\n\n\n\n\nvideo (Catalan)\n\n\n\n\n\nvideo (French)\n\n\n\n\n\nvideo (Finnish)\n\n\n​02:33\n21:33\n18:33\nOrbit: Probabilistic Forecast with Exponential Smoothing,\n\n\n\n\n\nEdwin Ng, Zhishi Wang, Steve Yang, Uber\n\n\n\n\n\nvideo\n\n\n02:42\n21:42\n18:42\nEffortless frequentist covariances of posterior expectations in Stan,\n\n\n\n\n\nRyan Giordano, Tamara Brodercik, MIT\n\n\n\n\n\nvideo\n\n\n02:51\n21:51\n18:51\nVirtual coffee and poster session\n\n\n05:30\n00:30\n21:30\nEnd of session\n\n\n\n\n\n\n\n\n\n\n\n\nWe can’t do this without the support of generous sponsors!\nOrganizers:\n\nSusana Marquez. The Rockefeller Foundation.\nDaniel Lee. Generable.\nKelli Cassidy. The University of Liverpool.\nSimon Maskell. The University of Liverpool."
  },
  {
    "objectID": "learn-stan/stancon/StanCon2020-program.html#program",
    "href": "learn-stan/stancon/StanCon2020-program.html#program",
    "title": "StanCon 2020. A 24h Global Event.",
    "section": "",
    "text": "The sessions were recorded and uploaded to YouTube. The plenary talks and the discussions can be found in the recording for the live sessions.\nThe abstracts to the talks are included in the video descriptions.\n\n\nPlaylists: - Session 1 - Session 2 - Session 3 - All videos\n\n\n\n\n\n\nBST\nEST\nPT\n\n\n\n\n\n\n\n\n\n\n\n08:30\n03:30\n00:30\nVirtual coffee\n\n\n09:00\n04:00\n01:00\nLive welcome from StanCon UK Committee\n\n\n09:04\n04:04\n01:04\n6 x 1 minute pitches\n\n\n09:10\n04:10\n01:10\nPlenary: Hierarchical Models for Covid -\n\n\n\n\n\nidentifying effects of lockdown and an R package,\n\n\n\n\n\nSeth Flaxman, Imperial College, London\n\n\n\n\n\nvideo\n\n\n09:50\n04:50\n01:50\nDiscussion\n\n\n10:00\n05:00\n02:00\n6 x 1 minute pitches, followed by 9 minute discussions:\n\n\n10:06\n05:06\n02:06\nHierarchical Bayes and logistic growth: Predicting diagnosed\n\n\n\n\n\nCOVID19 cases and the corresponding burden on health care\n\n\n\n\n\nsystems, Brynjólfur Gauti Jónsson, Birgir Hrafnkelsson,\n\n\n\n\n\nJón Magnús Jónsson, Jóhanna Jakobsdóttir,\n\n\n\n\n\nBergdís Björg Sigurjónsdóttir,Þórarinn Jónmundsson,\n\n\n\n\n\nSigríður Haraldsdóttir, Unnur A. Valdimarsdóttir,\n\n\n\n\n\nThor Aspelund, Directorate of Health (Iceland),\n\n\n\n\n\nCenter of Public Health Sciences (University of Iceland),\n\n\n\n\n\nDepartment of Mathematics (University of Iceland),\n\n\n\n\n\nIcelandic Heart Association, National University Hospital of Iceland\n\n\n\n\n\nvideo\n\n\n10:15\n05:15\n02:15\nProcess fault detection using Stan, Jerzy Baranowski,\n\n\n\n\n\nWaldemar Bauer, Rafał Mularczyk, Bartłomiej Gondek,\n\n\n\n\n\nAGH University of Science & Technology, Kraków, Poland\n\n\n\n\n\nvideo\n\n\n10:24\n05:24\n02:24\nFailure prediction in hierarchical equipment system:\n\n\n\n\n\nspline fitting naval ship failure, Hyunji Moon, Jinwoo Choi,\n\n\n\n\n\nHyeonseop Lee, Seoul National University\n\n\n\n\n\nvideo\n\n\n10:33\n05:33\n02:33\nbaggr: a new meta-analysis package using Stan,\n\n\n\n\n\nWitold Wiecek and Rachael Meager, London School of Economics\n\n\n\n\n\nvideo\n\n\n10:42\n05:42\n02:42\nTowards an Interface for Streaming Stan,\n\n\n\n\n\nSimon Maskell and Alessandro Varsi, University of Liverpool\n\n\n\n\n\nvideo\n\n\n10:51\n05:51\n02:51\nEnforcing stationarity through the prior in vector autoregressions,\n\n\n\n\n\nSarah Heaps, Newcastle University\n\n\n\n\n\nvideo\n\n\n11:00\n06:00\n03:00\nVirtual coffee and poster session\n\n\n13:30\n08:30\n05:30\nEnd of session\n\n\n\n\n\n\n\n\n\nBST\nEST\nPT\n\n\n\n\n\n16:30\n11:30\n08:30\nVirtual coffee\n\n\n17:00\n12:00\n09:00\nLive welcome from StanCon UK + US Committee\n\n\n17:10\n12:10\n09:10\nCode of Conduct working group, Steve Bronder and Lauren Kennedy\n\n\n\n\n\nvideo\n\n\n17:14\n12:14\n09:14\nStan Governing Body update, Jonah Gabry\n\n\n\n\n\nvideo\n\n\n17:20\n12:20\n09:20\n6 x 1 minute pitches\n\n\n17:32\n12:32\n09:32\nDeveloper talk 1: posteriordb: a database with data, models and\n\n\n\n\n\nposteriors, Aki Vehtari, Måns Magnusson, Aalto University\n\n\n\n\n\nvideo\n\n\n17:39\n12:39\n09:39\nDeveloper talk 2: Scaling Stan’s performance with reduce_sum in practice,\n\n\n\n\n\nSebastian Weber, Ben Bales, Steve Bronder, Mitzi Morris,\n\n\n\n\n\nRok Češnovar, Imperial College of London COVID-19 team,\n\n\n\n\n\nNovartis Pharma AG, Basel, Switzerland, Columbia University,\n\n\n\n\n\nUniversity of Ljubljana\n\n\n\n\n\nvideo\n\n\n17:46\n12:46\n09:46\nDeveloper talk 3: The State of GPU Computation Support for Stan,\n\n\n\n\n\nSteve Bronder, Erik Štrumbelj, Rok Češnovar, Davor Sluga,\n\n\n\n\n\nJure Demšar, Tadej Ciglarič, Sean Talts, Columbia University,\n\n\n\n\n\nUniversity of Ljubljana\n\n\n\n\n\nvideo\n\n\n17:53\n12:53\n09:53\nDeveloper talk 4: Approximate Bayesian inference for latent Gaussian\n\n\n\n\n\nmodels in Stan, Charles Margossian, Aki Vehtari, Daniel Simpson,\n\n\n\n\n\nRaj Agrawal, Columbia University, Department of Statistics;\n\n\n\n\n\nAalto University, Department of Computer Science;\n\n\n\n\n\nUniversity of Toronto, Department of Statistical Sciences;\n\n\n\n\n\nMassachusetts Institute of Technology, CSAIL\n\n\n\n\n\nvideo, code and notebook\n\n\n18:00\n13:00\n10:00\nPlenary: Multi-Source Information Modeling, Curation, and\n\n\n\n\n\nFusion Enabling Transdisciplinary Decision-Making: A Case for Space!\n\n\n\n\n\nMoriba Jah, Oden Institute for Computational Engineering & Sciences\n\n\n\n\n\nvideo\n\n\n18:50\n13:50\n10:50\nDiscussion\n\n\n19:00\n14:00\n11:00\n6 x 1 minute pitches, followed by 9 minute discussions\n\n\n19:06\n14:06\n11:06\nUsing Stan for Spatial Smoothing of Regression Parameters,\n\n\n\n\n\nWade Brorsen, Bart Niyizibi, Davood Pourvina, Eunchun Park,\n\n\n\n\n\nOklahoma State University\n\n\n\n\n\nvideo\n\n\n19:15\n14:15\n11:15\nMultivariate Synthetic Control to Estimate Missing Data in\n\n\n\n\n\nHierarchically Correlated Series, Sean Pinkney, Tyler Morrison,\n\n\n\n\n\nGaurav Shahane, Comscore, Inc.\n\n\n\n\n\nvideo\n\n\n19:24\n14:24\n11:24\nBayesian Causal Effect Estimation with Stan: Parametric and\n\n\n\n\n\nNonparametric Approaches, Arman Oganisian, Jason A. Roy,\n\n\n\n\n\nUniversity of Pennsylvania\n\n\n\n\n\nvideo\n\n\n19:33\n14:33\n11:33\nBuilding effective uncertainty visualizations with Stan/brms,\n\n\n\n\n\ntidybayes, and ggdist, Matthew Kay, University of Michigan\n\n\n\n\n\nvideo\n\n\n19:42\n14:42\n11:42\nAPI for iterative interrogation of stanfit objects in R,\n\n\n\n\n\nJonathan Sidi, Associate Director of Modeling and Simulation at\n\n\n\n\n\nSage Therapeutics\n\n\n\n\n\nvideo\n\n\n19:51\n14:51\n11:51\nBayesian Inference without Probability Density Functions,\n\n\n\n\n\nBen Goodrich, Columbia University\n\n\n\n\n\nvideo\n\n\n20:00\n15:00\n12:00\nVirtual coffee and poster session\n\n\n21:30\n16:30\n13:30\nEnd of session\n\n\n\n\n\n\n\n\n\nBST\nEST\nPT\n\n\n\n\n\n00:30\n19:30\n16:30\nVirtual coffee\n\n\n01:00\n20:00\n17:00\nLive welcome from StanCon Committee\n\n\n01:04\n20:04\n17:04\n6 x 1 minute pitches\n\n\n01:10\n20:10\n17:10\nPlenary: STAN and US Politics, David Shor\n\n\n\n\n\nvideo\n\n\n01:50\n20:50\n17:50\nDiscussion\n\n\n02:00\n21:00\n18:00\n6 x 1 minute pitches, followed by 9 minute discussions:\n\n\n02:06\n21:06\n18:06\nSpatial models for plant neighborhood dynamics in Stan,\n\n\n\n\n\nCristina Barber, Andrii Zaiats, Cara Applestein, Trevor Caughlin,\n\n\n\n\n\nBoise State University\n\n\n\n\n\nvideo\n\n\n02:15\n21:15\n18:15\n​Stress-testing the Dawid–Skene model: Analysis of repeated\n\n\n\n\n\ncategorical ratings, Damjan Vukcevic, Jeffrey Pullin, Lyle Gurrin,\n\n\n\n\n\nUniversity of Melbourne\n\n\n\n\n\nvideo\n\n\n02:24\n21:24\n18:24\nArviZ, InferenceData, and NetCDF: A unified file format for Bayesians,\n\n\n\n\n\nRavin Kumar, Oriol Abril-Pla, Osvaldo Martin, Piyush Gautam,\n\n\n\n\n\nAri Hartikainen , Alexandre Andorra, ArviZ (all)\n\n\n\n\n\nvideo (English)\n\n\n\n\n\nvideo (Catalan)\n\n\n\n\n\nvideo (French)\n\n\n\n\n\nvideo (Finnish)\n\n\n​02:33\n21:33\n18:33\nOrbit: Probabilistic Forecast with Exponential Smoothing,\n\n\n\n\n\nEdwin Ng, Zhishi Wang, Steve Yang, Uber\n\n\n\n\n\nvideo\n\n\n02:42\n21:42\n18:42\nEffortless frequentist covariances of posterior expectations in Stan,\n\n\n\n\n\nRyan Giordano, Tamara Brodercik, MIT\n\n\n\n\n\nvideo\n\n\n02:51\n21:51\n18:51\nVirtual coffee and poster session\n\n\n05:30\n00:30\n21:30\nEnd of session"
  },
  {
    "objectID": "learn-stan/stancon/StanCon2020-program.html#sponsors",
    "href": "learn-stan/stancon/StanCon2020-program.html#sponsors",
    "title": "StanCon 2020. A 24h Global Event.",
    "section": "",
    "text": "We can’t do this without the support of generous sponsors!\nOrganizers:\n\nSusana Marquez. The Rockefeller Foundation.\nDaniel Lee. Generable.\nKelli Cassidy. The University of Liverpool.\nSimon Maskell. The University of Liverpool."
  },
  {
    "objectID": "learn-stan/tutorials.html",
    "href": "learn-stan/tutorials.html",
    "title": "Stan",
    "section": "",
    "text": "Getting Started with Bayesian Statistics using Stan and Python  Bob Carpenter\nFundamentals of Stan  Charles Margossian StanCon 2023 tutorial, includes slides, models, and a Google colab notebook for R users."
  },
  {
    "objectID": "learn-stan/tutorials.html#tutorials",
    "href": "learn-stan/tutorials.html#tutorials",
    "title": "Stan",
    "section": "",
    "text": "Getting Started with Bayesian Statistics using Stan and Python  Bob Carpenter\nFundamentals of Stan  Charles Margossian StanCon 2023 tutorial, includes slides, models, and a Google colab notebook for R users."
  },
  {
    "objectID": "learn-stan/tutorials.html#videos",
    "href": "learn-stan/tutorials.html#videos",
    "title": "Stan",
    "section": "Videos",
    "text": "Videos\nThe official Stan YouTube channel is: https://www.youtube.com/channel/UCwgN5srGpBH4M-Zc2cAluOA\n\nIntroductory Videos\n\nStan Tutorials   Maggie Lieu. A series of 8 videos that will get you started using Stan in R and Python.\n\n\n\nContributed Videos\n\nStatistical Rethinking Winter 2019 Lecture 15   Richard McElreath.\nHow to write your first Stan program   Ben Lambert.\nBayesian Modeling with R and Stan   Sean Raleigh for the R Users Group, Salt Lake City, UT.\nbrms: Bayesian Multilevel Models using Stan   Paul-Christian Bürkner."
  },
  {
    "objectID": "learn-stan/tutorials.html#podcasts",
    "href": "learn-stan/tutorials.html#podcasts",
    "title": "Stan",
    "section": "Podcasts",
    "text": "Podcasts\n\nLearning Bayesian Statistics   Alexandre Andorra. A fortnightly podcast on Bayesian inference - the methods, the projects, and the people who make it possible!\nBayesian Statistics   John Krohn and Rob Trangucci. An intro to Bayesian statistics - its history, tools you can use, plus a discussion of the uses of a PhD in statistics."
  },
  {
    "objectID": "learn-stan/publications.html",
    "href": "learn-stan/publications.html",
    "title": "Published Articles and Books",
    "section": "",
    "text": "Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. Stan: A probabilistic programming language. Journal of Statistical Software 76(1).\nAndrew Gelman, Daniel Lee, and Jiqiang Guo. 2015. Stan: A probabilistic programming language for Bayesian inference and optimization. Journal of Education and Behavioral Statistics. 40(5):530–543.\nAlp Kucukelbir, Rajesh Ranganath, Andrew Gelman and David M. Blei. 2015. Automatic variational inference in Stan\nLu Zhang, Bob Carpenter, Andrew Gelman, Aki Vehtari. 2022. Pathfinder: Parallel quasi-Newton variational inference Journal of Machine Learning Research. 23(306):1−49, 2022.\nMichael Betancourt. 2017. A Conceptual Introduction to Hamiltonian Monte Carlo. arXiv:1701.02434.\nMatthew D. Hoffman, Andrew Gelman. 2014. The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research. 15(47):1593−1623.\nCole C. Monnahan, James T. Thorson, and Trevor A. Branch. 2017. Faster estimation of Bayesian models in ecology using Hamiltonian Monte Carlo Methods in Ecology and Evolution. 8(3):339-348.\nRadford Neal. 2011. MCMC Using Hamiltonian Dynamics. In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng. Chapman-Hall/CRC.\nBob Carpenter, Matthew D. Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael J. Betancourt. 2015. The Stan Math Library: Reverse-Mode Automatic Differentiation in C++.   arXiv 1509.07164."
  },
  {
    "objectID": "learn-stan/publications.html#stan-language-and-algorithms",
    "href": "learn-stan/publications.html#stan-language-and-algorithms",
    "title": "Published Articles and Books",
    "section": "",
    "text": "Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. 2017. Stan: A probabilistic programming language. Journal of Statistical Software 76(1).\nAndrew Gelman, Daniel Lee, and Jiqiang Guo. 2015. Stan: A probabilistic programming language for Bayesian inference and optimization. Journal of Education and Behavioral Statistics. 40(5):530–543.\nAlp Kucukelbir, Rajesh Ranganath, Andrew Gelman and David M. Blei. 2015. Automatic variational inference in Stan\nLu Zhang, Bob Carpenter, Andrew Gelman, Aki Vehtari. 2022. Pathfinder: Parallel quasi-Newton variational inference Journal of Machine Learning Research. 23(306):1−49, 2022.\nMichael Betancourt. 2017. A Conceptual Introduction to Hamiltonian Monte Carlo. arXiv:1701.02434.\nMatthew D. Hoffman, Andrew Gelman. 2014. The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research. 15(47):1593−1623.\nCole C. Monnahan, James T. Thorson, and Trevor A. Branch. 2017. Faster estimation of Bayesian models in ecology using Hamiltonian Monte Carlo Methods in Ecology and Evolution. 8(3):339-348.\nRadford Neal. 2011. MCMC Using Hamiltonian Dynamics. In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng. Chapman-Hall/CRC.\nBob Carpenter, Matthew D. Hoffman, Marcus Brubaker, Daniel Lee, Peter Li, and Michael J. Betancourt. 2015. The Stan Math Library: Reverse-Mode Automatic Differentiation in C++.   arXiv 1509.07164."
  },
  {
    "objectID": "learn-stan/publications.html#bayesian-workflow",
    "href": "learn-stan/publications.html#bayesian-workflow",
    "title": "Published Articles and Books",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\nAndrew Gelman, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul-Christian Bürkner, Martin Modrák. 2020. Bayesian Workflow\nAki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, Paul-Christian Bürkner, 2021. Rank-Normalization, Folding, and Localization: An Improved R-hat for Assessing Convergence of MCMC (with Discussion) Bayesian Analysis 16(2).\nAki Vehtari, Andrew Gelman, and Jonah Gabry. 2016. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. doi:10.1007/s11222-016-9696-4.\nJonah Gabry, Daniel Simpson, Aki Vehtari, Michael Betancourt, Andrew Gelman. 2019 Visualization in Bayesian workflow. Journal of the Royal Statistical Society Series A: Statistics in Society 182(2):389–402.\nRobert L. Grant, Daniel C. Furr, Bob Carpenter, and Andrew Gelman. 2016. Fitting Bayesian item response models in Stata and Stan. arXiv 1601.03443."
  },
  {
    "objectID": "learn-stan/publications.html#books-using-stan",
    "href": "learn-stan/publications.html#books-using-stan",
    "title": "Published Articles and Books",
    "section": "Books using Stan",
    "text": "Books using Stan\n\nGelman, A., Hill, J., and Vehtari A. 2020. Regression and Other Stories\nGelman, A. and Vehtari A. 2024. Active Statistics\nKorner-Nievergelt, F., Roth, T., Von Felten, S., Guélat, J., Almasi, B. and Korner-Nievergelt, P. 2024. Bayesian Data Analysis in Ecology Using Linear Models with R and Stan. Online.\nSuzuki, J. 2023. WAIC and WBIC with R Stan: 100 Exercises for Building Logic. Springer.\nMatsuura, K.. 2022. Bayesian Statistical Modeling with Stan, R, and Python. Springer.\nJohnson, A.A., M. Q. Ott, M. Dogucu. 2021. Bayes Rules! An Introduction to Applied Bayesian Modeling CRC Press.\nHolmes, E. E., M. D. Scheuerell, and E. J. Ward. 2019. Applied Time Series Analysis for Fisheries and Environmental Sciences. NOAA Fisheries, Northwest Fisheries Science Center.\nHilbe, J. M., R.S. de Souza, and E. E. O. Ishida. 2017. Bayesian Models for Astrophysical Data Using R, JAGS, Python and Stan. Cambridge University Press.\nMatsuura, K.. 2016. Bayesian Statistical Modeling Using Stan and R. Wonderful R Series, Volume 2. Kyoritsu Shuppan Co., Ltd. [in Japanese]\nMcElreath, R. 2016. Statistical Rethinking: A Bayesian Course with R and Stan. Chapman-Hall/CRC.\nKorner-Nievergelt, F., Roth, T., Von Felten, S., Guélat, J., Almasi, B. and Korner-Nievergelt, P. 2015. Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan. Academic Press.\nKruschke, J. 2014. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan. Academic Press.\nGelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A. and Rubin, D.B. 2013. Bayesian Data Analysis, Third Edition. Chapman-Hall/CRC."
  },
  {
    "objectID": "learn-stan/field-guides.html",
    "href": "learn-stan/field-guides.html",
    "title": "Domain-Specific Resources",
    "section": "",
    "text": "These guides provide applied researchers with curated lists of tutorials, case studies, and resources which contain Stan implementations of the most commonly used models in their domain. If you would like to contribute one for your field, see this Discourse post: Fostering Stan user communities through domain-specific resource pages\n\n\n\nTopics: Item Response Theory, Cognitive Diagnosis Models, Multilevel Modeling\nMaintainer: Sofia Rabe-Hesketh\n\n\n\n\n\nTopics: Ecological times series models, population models, predator-prey dynamics, species site-occupancy models.\nMaintainer: Vianey Leos Barajas\n\n\n\n\n\nTopics: Disease transmission (SIR models), disease mapping (ICAR, BYM models), survival analysis, longitudinal data.\nMaintainer: Léo Grinsztajn\n\n\n\n\n\nCognitive Science and Neuroscience papers which use Stan, brms, or RStanArm.\nMaintainer: Bruno Nicenboim\n\n\n\n\n\nCase studies using brms which address clinical questions for drug development.\nEditor: Sebastian Weber and colleagues."
  },
  {
    "objectID": "learn-stan/field-guides.html#field-guides",
    "href": "learn-stan/field-guides.html#field-guides",
    "title": "Domain-Specific Resources",
    "section": "",
    "text": "These guides provide applied researchers with curated lists of tutorials, case studies, and resources which contain Stan implementations of the most commonly used models in their domain. If you would like to contribute one for your field, see this Discourse post: Fostering Stan user communities through domain-specific resource pages\n\n\n\nTopics: Item Response Theory, Cognitive Diagnosis Models, Multilevel Modeling\nMaintainer: Sofia Rabe-Hesketh\n\n\n\n\n\nTopics: Ecological times series models, population models, predator-prey dynamics, species site-occupancy models.\nMaintainer: Vianey Leos Barajas\n\n\n\n\n\nTopics: Disease transmission (SIR models), disease mapping (ICAR, BYM models), survival analysis, longitudinal data.\nMaintainer: Léo Grinsztajn\n\n\n\n\n\nCognitive Science and Neuroscience papers which use Stan, brms, or RStanArm.\nMaintainer: Bruno Nicenboim\n\n\n\n\n\nCase studies using brms which address clinical questions for drug development.\nEditor: Sebastian Weber and colleagues."
  },
  {
    "objectID": "learn-stan/field-guides.html#stanconnect-sessions",
    "href": "learn-stan/field-guides.html#stanconnect-sessions",
    "title": "Domain-Specific Resources",
    "section": "StanConnect Sessions",
    "text": "StanConnect Sessions\nStanConnect is a virtual miniseries which consists of 3-hour meetings/mini-symposia where each meeting is like “session” of an organized conference. This series was started in 2021 and reprised in 2022. Future StanConnect series will be announced on Discourse.\n\nStan Through Space and Time (2022)\nA half-day event showcasing the use of Stan in spatial statistics and for modeling time series data.\n\nDate: October 31, 2022\nGitHub repository of slides and materials: https://github.com/stan-dev/connect22-space-time\nProgram:\n\nTime and tide wait for no one: spatio-temporal modelling in river networks - Edgar Santos Fernandez\nSpatio-temporal modelling of mosquitoes vector and its environmental drivers in Hong Kong - Stan Yip\nRobust non-Gaussian models and how to fit them in Stan - Rafael Cabral\nA space-time extension of the Poisson auto-regression to model Covid-19 cases at the England local authorities level - Pierfrancesco Alaimo Di Loro\nFitting spatio-temporal geostatistical models in Stan using the bmstdr R package - Sujit Sahu\ntipsae: Tools for Handling Indices and Proportions in Small Area Estimation - Silvia De Nicolo\ngeostan: An R package for Bayesian spatial analysis - Connor Donegan\nStructure induced by a multiple membership transformation on the conditional autoregressive model - Marco Gramatica\nA Bayesian hierarchical model for disease mapping that accounts for scaling and heavy-tailed latent effects - Victoire Michal\nBayesian latent spatial autoregressive growth modeling - Zachary Roman\n\nAbstracts\nOrganizer: James Hogg\n\n\n\nCognitive Science and Neuroscience (2021)\n\nDate: November 19, 2021\nRecorded Talks: YouTube\nProgram:\n\nResolving the multiple testing issue in neuroimaging through Bayesian multilevel modeling - Gang Chen\n\nImplementation of the Diffusion Decision Model with Across-Trial Variability in the Drift Rate - Kendal Foster and Henrik Singmann\n\nIt’s complicated: Some observations on the nuanced constraints of the multivariate normal in high dimensions - Mike Lawrence\n\nUsing computational modeling parameters to measure working memory processes - Jan Göttmann\n\n\nAbstracts\nOrganizer: Bruno Nicenboim\n\n\n\nBiostats (2021)\n\nDate: October 19, 2021\nRecorded Talks: YouTube\nGitHub repository of slides and materials: https://github.com/maxbiostat/StanConnect2021_Biostatistics\nProgram:\n\nCoding in Stan: the BYM2 model for disconnected graphs  Mitzi Morris\nNormalized power prior models in Stan  Ethan Alt\nSummarising enzyme information from online databases using Stan and Arviz,   Teddy Groves\nAutomated kinetic modelling in Stan and its application to the methionine cycle  Nicholas Cowie\nUsing Hidden Markov Models as a complement/alternative to survival modelsMartin Modrák\n\nOrganizer: Luiz Max Carvalho, PhD - Getúlio Vargas Foundation\n\n\n\nEcology, models for biological survey data (2021)\nA 2-day event showcasing applications of Stan for ecological analyses\n\nDay 1: September 30, 2021\nDay 1 Recorded Talks: YouTube\nProgram Day 1:\n\nUsing Stan to build better community - Ara Winter\nUsing Stan to diagnose and fit high-dimensional multispecies abundance models - Harold Eyster\n250000 parameters: the story of an occupancy model for Colombia’s birdlife in Stan - Jacob Socolar\nMLOps in a Bayesian workflow: tracking experiments with MLFlow - Maxwell Joseph\n\nDay 2: October 4, 2021\nProgram Day 2:\nDay 2 Recorded Talks: YouTube\n\nEvolution of habitat use and coexistence under intraguild predation: a principled Bayesian approach - Josh Goldberg\nDrift algae controls the consumption of kelp: using data from in-situ subtidal experimentation and ordinary differential equations to mechanistically model the sea urchin behavioral switch - Zach Randell\nLinking the SPDE method with Stan - Joaquin Cavieres\nUsing Stan to characterize large-scale morphological change in North American birds - Casey Youngflesh\nFitting hidden Markov models to ecological time series data in Stan - Vianey Leos Barajas\n\nOrganizer: Jacob B. Socolar, Cornell Lab of Ornithology\nPanelist Day 1: Vianey Leos Barajas: Assistant Professor, University of Toronto, Department of Statistical Sciences"
  },
  {
    "objectID": "learn-stan/case-studies.html",
    "href": "learn-stan/case-studies.html",
    "title": "Stan Case Studies",
    "section": "",
    "text": "The case studies on this page are intended to reflect best practices in Bayesian methodology and Stan programming. We aim to keep them current with the latest version of the Stan language, but there may be times when case studies need updating to reflect the latest Stan features and syntax."
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-10-2023",
    "href": "learn-stan/case-studies.html#volume-10-2023",
    "title": "Stan Case Studies",
    "section": "Volume 10 (2023)",
    "text": "Volume 10 (2023)\n\nInstrumental Variables Analysis of Randomized Experiments with One-Sided Noncompliance\nIn this document, we demonstrate how to implement Bayesian inference for causal effects in randomized experiments with one-sided noncompliance using Stan. Specifically, we aim to replicate the analysis presented in Imbens and Rubin (1997). We present Stan models with and without the exclusion restriction assumption, showcasing a significant advantage of the Bayesian model-based approach.\nView HTML (link opens in new tab)\n\nAuthors\n\nJoonHo Lee, Avi Feller, Sophia Rabe-Hesketh\n\nKeywords\n\ncausal inference, instrumental variables analysis, one-sided compliance, principal stratification\n\nSource Repository\n\nexample-models/education/causal_iv_one-sided (GitHub)\n\nDependencies\n\ntidyverse, rstan, bayesplot, patchwork\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nBayesian Structural Equation Modeling using blavaan\nIn this case study, we fit Bayesian structural equation models (SEM) using Hamiltonian Monte Carlo sampling in Stan-powered R package blavaan and illustrate how to use confirmtory factor analysis and latent growth curve modeling as SEM’s special cases. We also compared the estimates from blavaan with its frequentist counterpart using lavaan.\nView HTML (Link opens in new tab)\n\nAuthors\n\nFeng Ji, Xingyao Xiao, Aybolek Amanmyradova, Sophia Rabe-Hesketh\n\nKeywords\n\nStructural Equation Modeling (SEM), Lavant Variable Modeling, Latent Growth Curve Models, Confirmatory Factor Analysis (CFA), Growth Curve Modeling, Bayesian Model Evaluation\n\nSource Repository\n\nexample-models/education/sem (GitHub)\n\nDependencies\n\nblavaan, lavaan, rstan, MASS, mvtnorm, tidyverse, semPlot, magrittr, lavaan.survey\n\nLicense\n\nBSD (3 clause), CC-BY"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-9-2022",
    "href": "learn-stan/case-studies.html#volume-9-2022",
    "title": "Stan Case Studies",
    "section": "Volume 9 (2022)",
    "text": "Volume 9 (2022)\n\nMultilevel regression modeling with CmdStanPy and plotnine\nThis notebook is a short introduction to multilevel regression modeling using the CmdStanPyinterface and plotnine, a Python implementation of a grammar of graphics based on ggplot2.\nView(HTML)\n\nAuthors\n\nMitzi Morris\n\nKeywords\n\nPython, CmdStanPy, plotnine, hierarchical/multilevel modeling, linear regression, posterior predictive checks, radon\n\nSource Repository\n\nexample-models/jupyter/radon (GitHub)\n\nDependencies\n\ncmdstanpy, numpy, pandas, matplotlib, plotnine, jupyter\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nHoloML in Stan: Low-photon Image Reconstruction\nIn this case study, we perform image reconstruction in Stan by implementing the HoloML phase retrieval model and then solving the inverse problem with optimization. This case study requires Stan 2.30 or greater in order to use the Fourier transform functions added in that version.\nView(HTML)\n\nAuthors\n\nBrian Ward, Bob Carpenter, and David Barmherzig\n\nKeywords\n\nimage reconstruction, phase retrieval, Fourier transforms, deconvolution\n\nSource Repository\n\nWardBrian/holoml-in-stan (GitHub)\n\nDependencies\n\ncmdstanpy, numpy, matplotlib, scipy, jupyter\n\nLicense\n\nBSD (3 clause), CC-BY"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-8-2021",
    "href": "learn-stan/case-studies.html#volume-8-2021",
    "title": "Stan Case Studies",
    "section": "Volume 8 (2021)",
    "text": "Volume 8 (2021)\n\nBayesian Latent Class Models and Handling of Label Switching\nIn this case study, we fit the Bayesian latent class model using Hamiltonian Monte Carlo sampling and Variational Bayes in Stan and illustrate the issue of label switching and its treatment with simulated and empirical data.\nView(HTML)\n\nAuthors\n\nFeng Ji, Aybolek Amanmyradova, Sophia Rabe-Hesketh\n\nKeywords\n\nlatent class models, label-switching, post-hoc relabeling, variational Bayes\n\nSource Repository\n\nexample-models/education/latent_class (GitHub)\n\nDependencies\n\nlabel.switching, rstan, magrittr, knitr, poLCA\n\nLicense\n\nBSD (3 clause), CC-BY"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-7-2020",
    "href": "learn-stan/case-studies.html#volume-7-2020",
    "title": "Stan Case Studies",
    "section": "Volume 7 (2020)",
    "text": "Volume 7 (2020)\n\nBayesian model of planetary motion: exploring ideas for a modeling workflow when dealing with ordinary differential equations and multimodality\nThe Bayesian model of planetary motion is a simple but powerful example that illustrates important concepts, as well as gaps, in prescribed modeling workflows. Our focus is on Bayesian inference using Markov chains Monte Carlo for a model based on an ordinary differential equations (ODE). Our example presents unexpected multimodality, causing our inference to be unreliable and what is more, dramatically slowing down our ODE integrators. What do we do when our chains do not mix and do not forget their starting points? Reasoning about the computational statistics at hand and the physics of the modeled phenomenon, we diagnose how the modes arise and how to improve our inference. Our process for fitting the model is iterative, starting with a simplification and building the model back up, and makes extensive use of visualization.\nView(HTML)\n\nAuthors\n\nCharles Margossian and Andrew Gelman\n\nKeywords\n\nordinary differential equations, multimodality, classical mechanics\n\nSource Repository\n\nexample-models/knitr/planetary_motion (GitHub)\n\nDependencies\n\nCmdStanR, posterior, ggplot2, dplyr, plyr, tidyr, boot, latex2exp\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nHMM Interface Example\nCmdstan 2.24 introduces a new interface for working with Hidden Markov Models (HMMs). This is an example of how to use that interface.\nView(HTML)\n\nAuthors\n\nBen Bales\n\nKeywords\n\nHidden Markov Models, HMMs, cmdstanr, Stan programming\n\nSource Repository\n\nexample-models/knitr/hmm-example (GitHub)\n\nDependencies\n\nCmdStanR, tidyverse, ggplot2, posterior\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nSpatial models for plant neighborhood dynamics in Stan\nIn this case study, we demonstrate how Stan’s segment function can speed computation on sparse matrices of pairwise neighbors in plant-plant interaction models. In addition, we present solutions to common problems of fitting neighborhood models with hierarchical effects, including a comparison of centered vs. non-centered parameterizations.\nView(HTML)\n\nAuthors\n\nCristina Barber, Andrii Zaiats, Cara Applestein and T.Trevor Caughlin\n\nKeywords\n\nplants, neighbor interactions, sparse matrix, segment function\n\nSource Repository\n\nCristinabarber/Neighbor_Interactions (GitHub)\n\nR Package Dependencies\n\nrstan\n\nLicense\n\nBSD (3 clause), CC BY NC\n\n\n\n\nUpgrading to the new ODE interface\nCmdstan 2.24 introduces a new ODE interface intended to make it easier to specify the ODE system function. This document should serve as an overview of the interface changes as well as a tutorial for converting code written with the old ODE interface.\nView(HTML)\n\nAuthors\n\nBen Bales, Sebastian Weber\n\nKeywords\n\nordinary differential equations, cmdstanr, Stan programming\n\nSource Repository\n\nexample-models/knitr/convert-odes (GitHub)\n\nDependencies\n\nCmdStanR\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nBayesian Workflow for disease transmission modeling in Stan\nThis tutorial shows how to build, fit, and criticize disease transmission models in Stan, and should be useful to researchers interested in modeling the COVID-19 outbreak and doing Bayesian inference. Bayesian modeling provides a principled way to quantify uncertainty and incorporate prior knowledge into the model. What is more, Stan’s main inference engine, Hamiltonian Monte Carlo sampling, is amiable to diagnostics, which means we can verify whether our inference is reliable. Stan is an expressive probabilistic programing language that abstracts the inference and allows users to focus on the modeling. The resulting code is readable and easily extensible, which makes the modeler’s work more transparent and flexible. In this tutorial, we demonstrate with a simple Susceptible-Infected-Recovered (SIR) model how to formulate, fit, and diagnose a compartmental model in Stan. We also introduce more advanced topics which can help practitioners fit sophisticated models; notably, how to use simulations to probe our model and our priors, and computational techniques to scale ODE-based models.\nView(HTML)\n\nAuthors\n\nLeo Grinsztajn, Elizaveta Semenova, Charles C. Margossian, and Julien Riou\n\nKeywords\n\nDisease transmission, Compartment models, Ordinary Differential Equations, Bayesian Workflow\n\nSource Repository\n\ncharlesm93/disease_transmission_workflow (GitHub)\n\nDependencies\n\nRStan\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nReduce Sum Example: parallelization of a single chain across multiple cores\nStan 2.23 introduced reduce_sum, a new way to parallelize the execution of a single Stan chain across multiple cores. This introduction copies directly from Richard McElreath’s Multithreading and Map-Reduce in Stan 2.18.0: A Minimal Example\nView(HTML)\n\nAuthor\n\nBen Bales\n\nKeywords\n\nwithin-chain parallel computation, cmdstanr, Stan programming\n\nSource Repository\n\nexample-models/knitr/reduce-sum (GitHub)\n\nDependencies\n\nCmdStanR\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nStan Notebooks in the Cloud\nThis report shows you how to author a Jupyter Notebook for your Stan model and data so that anyone with a modern web browser and a Google account can run your analysis with Google Colaboratory free cloud servers. It shows you how to quickly set up a Stan installation in the cloud and introduces two lightweight interfaces: CmdStanR and CmdStanPy.\nView(HTML)\n\nAuthor\n\nMitzi Morris\n\nKeywords\n\nJupyter, Google Colab, teaching Stan, online classroom, cloud computing\n\nSource Repository\n\nexample-models/knitr/cloud-compute-2020 (GitHub)\n\nDependencies\n\ninternet connection, Google account\n\nLicense\n\nBSD (3 clause), CC-BY"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-6-2019",
    "href": "learn-stan/case-studies.html#volume-6-2019",
    "title": "Stan Case Studies",
    "section": "Volume 6 (2019)",
    "text": "Volume 6 (2019)\n\nModel-based Inference for Causal Effects in Completely Randomized Experiments\nIn this document, we discuss the implementation of Bayesian model-based inference for causal effects in Stan. We start by providing an introduction to the Bayesian inferential framework by analyzing a simulated dataset generated under unconfounded treatment assignment. Then we analyze an example dataset obtained from a completely randomized experiment focusing on the specification of the joint distribution of the potential outcomes.\nView(HTML)\n\nAuthor\n\nJoonHo Lee, Avi Feller and Sophia Rabe-Hesketh\n\nKeywords\n\ncausal inference, completely randomized experiments\n\nSource Repository\n\nexample-models/education/causal_rct\n\nR Package Dependencies\n\nrstan, rstanarm, bayesplot, tidyverse, gridExtra, Matching\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nTagging Basketball Events with HMM in Stan\nThis case study shows how we can apply Bayesian inference to Hidden Markov Models (HMMs) using Stan to extract useful information from basketball player tracking data. Specifically we show how to tag drive events and how to determine defensive assignment. Before diving into basketball data we show how to fit an HMM in Stan using a simple example. This should help build some intuition for those who are unfamiliar with HMMs and will also show how to specify an HMM using Stan.\nView(HTML)\n\nAuthor\n\nImad Ali\n\nKeywords\n\nhidden markov models, sports\n\nSource Repository\n\nimadmali/bball-hmm (GitHub)\n\nR Package Dependencies\n\nrstan, bayesplot, dplyr\n\nLicense\n\nBSD (3 clause), CC-BY-NC\n\n\n\n\nModel building and expansion for golf putting\nIn this case study, we use Stan to build a series of models to estimate the probability of a successful putt using data from professional golfers. We fit and check the fit of a series of models, demonstrating the benefits of modeling based on substantive (rather than purely statistical) principles. We successfully fit to a small dataset and then have to expand the model to fit a new, larger dataset. We use weakly informative priors and a model-misfit error term to enable the fit.\nView(HTML)\n\nAuthor\n\nAndrew Gelman\n\nKeywords\n\nnonlinear regression, sports\n\nSource Repository\n\nexample-models/knitr/golf (GitHub)\n\nR Package Dependencies\n\nrstan\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nA Dyadic Item Response Theory Model: Stan Case Study\nIn this case study, we use Stan to fit the Dyadic Item Response Theory (dIRT) model proposed by (Gin et al. 2019) to measure interactions between pairs of individuals when the responses to items represent the actions/behaviors/perceptions of an individual (called the ‘actor’) made within the context of a dyad formed with another individual (called the ‘partner’). The dIRT model is fit using Stan (version 2.18.1) in R via the rstan package.\nView(HTML)\n\nAuthor\n\nNicholas Sim, Brian Gin, Anders Skrondal and Sophia Rabe-Hesketh\n\nKeywords\n\nitem response theory, social relations model, dyadic data\n\nSource Repository\n\nexample-models/education/dyadic_irt_model (GitHub)\n\nR Package Dependencies\n\nrstan, tidyverse\n\nLicense\n\nBSD (3 clause), CC-BY"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-5-2018",
    "href": "learn-stan/case-studies.html#volume-5-2018",
    "title": "Stan Case Studies",
    "section": "Volume 5 (2018)",
    "text": "Volume 5 (2018)\n\nMultilevel Linear Models using Rstanarm\nIn this tutorial, we illustrate how to fit a multilevel linear model within a full Bayesian framework using rstanarm. This tutorial is aimed primarily at educational researchers who have used lme4 in R to fit models to their data and who may be interested in learning how to fit Bayesian multilevel models. However, for readers who have not used lme4 before, we briefly review the use of the package for fitting multilevel models.\nView(HTML)\n\nAuthor\n\nJoonHo Lee, Nicholas Sim, Feng Ji, and Sophia Rabe-Hesketh\n\nKeywords\n\neducation, rstanarm, multilevel models, linear mixed models, hierarchical linear models\n\nSource Repository\n\nexample-models/education/tutorial_rstanarm (GitHub)\n\nR Package Dependencies\n\nrstanarm, mlmRev, ggplot2, lme4\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nPredator-Prey Population Dynamics: the Lotka-Volterra model in Stan\nLotka (1925) and Volterra (1926) formulated parametric differential equations that characterize the oscillating populations of predators and prey. A statistical model to account for measurement error and unexplained variation uses the deterministic solutions to the Lotka-Volterra equations as expected population sizes. Stan is used to encode the statistical model and perform full Bayesian inference to solve the inverse problem of inferring parameters from noisy data. The model is fit to Canadian lynx and snowshoe hare populations between 1900 and 1920, based on the number of pelts collected annually by the Hudson’s Bay Company. Posterior predictive checks for replicated data show the model fits this data well. Full Bayesian inference may be used to estimate future (or past) populations.\nView(HTML)\n\nAuthor\n\nBob Carpenter\n\nKeywords\n\npopulation dynamics, Lotka-Volterra equations, differential equations, posterior predictive checks\n\nSource Repository\n\nstan-dev/example-models/knitr/lotka-volterra (GitHub)\n\nR Package Dependencies\n\nrstan, &gt;ggplot2, gridExtra, knitr, reshape, tufte\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nNearest neighbor Gaussian process (NNGP) models in Stan\nNearest neighbor Gaussian process (NNGP) based models is a family of highly scalable Gaussian processes based models. In brief, NNGP extends the Vecchia’s approximation (Vecchia 1988) to a process using conditional independence given information from neighboring locations. This case study shows how to express and fit these models in Stan.\nView(HTML)\n\nAuthor\n\nLu Zhang\n\nKeywords\n\nGaussian process, nearest neighbor Gaussian process, spatial models, latent process, regression\n\nSource Repository\n\nLuZhangstat/NNGP_STAN (GitHub)\n\nR Package Dependencies\n\nrstan\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-4-2017",
    "href": "learn-stan/case-studies.html#volume-4-2017",
    "title": "Stan Case Studies",
    "section": "Volume 4 (2017)",
    "text": "Volume 4 (2017)\n\nExtreme value analysis and user defined probability functions in Stan\nThis notebook demonstrates how to implement user defined probability functions in Stan language. As an example I use the generalized Pareto distribution (GPD) to model geomagnetic storm data from the World Data Center for Geomagnetism.\nView(HTML)\n\nAuthor\n\nAki Vehtari\n\nKeywords\n\nextreme value analysis, generalized Pareto distribution, user defined probability functions\n\nSource Repository\n\navehtari/BDA_R_demos/demos_rstan/gpareto_functions (GitHub)\n\nR Package Dependencies\n\nrstan, bayesplot, loo, ggplot2, tidyr, dplyr, extraDistr, gridExtra\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nModelling Loss Curves in Insurance with RStan\nLoss curves are a standard actuarial technique for helping insurance companies assess the amount of reserve capital they need to keep on hand to cover claims from a line of business. Claims made and reported for a given accounting period are tracked separately over time. This enables the use of historical patterns of claim development to predict expected total claims for newer policies.\nWe model the growth of the losses in each accounting period as an increasing function of time, and use the model to estimate the parameters which determine the shape and form of this growth. We also use the sampler to estimate the values of the “ultimate loss ratio”, i.e. the ratio of the total claims on an accounting period to the total premium received to write those policies. We treat each accounting period as a cohort.\nView(HTML)\n\nAuthor\n\nMick Cooney\n\nKeywords\n\nactuarial science, loss curves, insurance, ultimate loss ratio, hierarchical model\n\nSource Repository\n\nkaybenleroll/stancasestudy_losscurves (GitHub)\n\nR Package Dependencies\n\nrstan, bayesplot, tidyverse, scales, cowplot\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nSplines in Stan\nIn this document, we discuss the implementation of splines in Stan. We start by providing a brief introduction to splines and then explain how they can be implemented in Stan. We also discuss a novel prior that alleviates some of the practical challenges of spline models.\nView(HTML)\n\nAuthor\n\nMilad Kharratzadeh\n\nKeywords\n\nB-splines, piecewise regression, knots, priors\n\nSource Repository\n\nmilkha/Splines_in_Stan (GitHub)\n\nR Package Dependencies\n\nrstan, splines\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nSpatial Models in Stan: Intrinsic Auto-Regressive Models for Areal Data\nThis case study shows how to efficiently encode and compute an Intrinsic Conditional Auto-Regressive (ICAR) model in Stan. When data has a neighborhood structure, ICAR models provide spatial smoothing by averaging measurements of directly adjoining regions. The Besag, York, and Mollié (BYM) model is a Poisson GLM which includes both an ICAR component and an ordinary random-effects component for non-spatial heterogeneity. We compare two variants of the BYM model and fit two datasets taken from epidemiological studies over 56 and 700 regions, respectively.\nView(HTML)\n\nAuthor\n\nMitzi Morris\n\nKeywords\n\nspatial modeling, CAR, ICAR, INLA, OpenBUGS, hierarchical models\n\nSource Repository\n\nstan-dev/example-models (GitHub)\n\nR Package Dependencies\n\ncmdstanr, ggplot2, broom, reshape2, dplyr, maptools, spdep, R-INLA, R2OpenBugs\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nThe QR Decomposition for Regression Models\nThis case study reviews the QR decomposition, a technique for decorrelating covariates and, consequently, the resulting posterior distribution in regression models.\nView(HTML)\n\nAuthor\n\nMichael Betancourt\n\nKeywords\n\nMarkov chain Monte Carlo, regression, RStan\n\nSource Repository\n\nbetanalpha/knitr_case_studies/qr_regression (GitHub)\n\nR Package Dependencies\n\nrstan, knitr.\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nRobust RStan Workflow\nThis case study demonstrates the recommended RStan workflow for ensuring robust inferences with the default dynamic Hamiltonian Monte Carlo algorithm.\nView(HTML)\n\nAuthor\n\nMichael Betancourt\n\nKeywords\n\nMarkov chain Monte Carlo, Hamiltonian Monte Carlo, divergences, RStan\n\nSource Repository\n\nbetanalpha/knitr_case_studies/rstan_workflow (GitHub)\n\nR Package Dependencies\n\nrstan, knitr.\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nRobust PyStan Workflow\nThis case study demonstrates the recommended PyStan workflow for ensuring robust inferences with the default dynamic Hamiltonian Monte Carlo algorithm.\nView(HTML)\n\nAuthor\n\nMichael Betancourt\n\nKeywords\n\nMarkov chain Monte Carlo, Hamiltonian Monte Carlo, divergences, PyStan\n\nSource Repository\n\nbetanalpha/jupyter_case_studies/pystan_workflow (GitHub)\n\nPython Package Dependencies\n\nrstan, pystan, pickle, numpy, md5.\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nTypical Sets and the Curse of Dimensionality\nThis case study illustrates the so-called “curse of dimensionality” using simple examples based on simulation to show that all points are far away in high dimensions and that the mode is an atypical draw from a multivariate normal. The information-theoretic concept of typical set is illustrated with both discrete and continuous cases, which show that probability mass is a product of volume and density (or count and mass in the discrete case). It also illustrates Monte Carlo methods and relates distance to the log density of the normal distribution and the chi-squared distribution.\nView R version (HTML)\n\nAuthors\n\nBob Carpenter\n\nKeywords\n\nprobability mass, typical sets, concentration of measure, Monte Carlo methods\n\nSource Repository (R)\n\nstan-dev/example-models/knitr/curse-dims (GitHub)\n\nR Package Dependencies\n\nggplot2\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\nView Python version (HTML)\n\nAuthor (Python translation)\n\nAravind S (Python translation)\n\nSource Repository (Python)\n\nAravinds-ds/Stan-Code/python notebooks/curse_dims (GitHub)\n\nPython Package Dependencies\n\nnumpy, scipy, pandas, matplotlib, collections, sys\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nDiagnosing Biased Inference with Divergences\nThis case study discusses the subtleties of accurate Markov chain Monte Carlo estimation and how divergences can be used to identify biased estimation in practice.\nView(HTML)\n\nAuthor\n\nMichael Betancourt\n\nKeywords\n\nMarkov chain Monte Carlo, Hamiltonian Monte Carlo, divergences, RStan\n\nSource Repository\n\nbetanalpha/knitr_case_studies/divergences_and_bias (GitHub)\n\nR Package Dependencies\n\nrstan, knitr.\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nIdentifying Bayesian Mixture Models\nThis case study discusses the common pathologies of Bayesian mixture models as well as some strategies for identifying and overcoming them.\nView(HTML)\n\nAuthor\n\nMichael Betancourt\n\nKeywords\n\nMarkov chain Monte Carlo, Hamiltonian Monte Carlo, mixture models, multimodal models, RStan\n\nSource Repository\n\nbetanalpha/knitr_case_studies/identifying_mixture_models (GitHub)\n\nR Package Dependencies\n\nrstan, knitr.\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0\n\n\n\n\nHow the Shape of a Weakly Informative Prior Affects Inferences\nThis case study reviews the basics of weakly-informative priors and how the choice of a specific shape of such a prior affects the resulting posterior distribution.\nView(HTML)\n\nAuthor\n\nMichael Betancourt\n\nKeywords\n\nMarkov chain Monte Carlo, Hamiltonian Monte Carlo, priors, weakly-informative priors, RStan\n\nSource Repository\n\nbetanalpha/knitr_case_studies/weakly_informative_shapes (GitHub)\n\nR Package Dependencies\n\nrstan, knitr.\n\nLicense\n\nCode: BSD (3 clause), Text: CC BY-NC 4.0"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-3-2016",
    "href": "learn-stan/case-studies.html#volume-3-2016",
    "title": "Stan Case Studies",
    "section": "Volume 3 (2016)",
    "text": "Volume 3 (2016)\n\nExact Sparse CAR Models in Stan\nThis document details sparse exact conditional autoregressive (CAR) models in Stan as an extension of previous work on approximate sparse CAR models in Stan. Sparse representations seem to give order of magnitude efficiency gains, scaling better for large spatial data sets.\nView(HTML)\n\nAuthor\n\nMax Joseph\n\nKeywords\n\nconditional autoregressive (CAR), independent autoregressive (IAR), sparsity, spatial random effects, maps\n\nSource Repository\n\nmbjoseph/CARstan(GitHub)\n\nR Package Dependencies\n\nrstan, dplyr, ggmcmc, knitr, maptools, rgeos, spdep.\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nA Primer on Bayesian Multilevel Modeling using PyStan\nThis case study replicates the analysis of home radon levels using hierarchical models of Lin, Gelman, Price, and Kurtz (1999). It illustrates how to generalize linear regressions to hierarchical models with group-level predictors and how to compare predictive inferences and evaluate model fits. Along the way it shows how to get data into Stan using pandas, how to sample using PyStan, and how to visualize the results using Seaborn.\nView(HTML)\n\nAuthor\n\nChris Fonnesbeck\n\nKeywords\n\nhierarchical/multilevel modeling, linear regression, model comparison, predictive inference, radon\n\nSource Repository\n\nfonnesbeck/stan_workshop_2016 (GitHub)\n\nPython Package Dependencies\n\npystan, numpy, pandas, matplotlib, seaborn\n\nLicense\n\nApache 2.0 (code), CC-BY 3 (text)\n\n\n\n\nThe Impact of Reparameterization on Point Estimates\nWhen changing variables, a Jacobian adjustment needs to be provided to account for the rate of change of the transform. Applying the adjustment ensures that inferences that are based on expectations over the posterior are invariant under reparameterizations. In contrast, the posterior mode changes as a result of the reparameterization. In this note, we use Stan to code a repeated binary trial model parameterized by chance of success, along with its reparameterization in terms of log odds in order to demonstrate the effect of the Jacobian adjustment on the Bayesian posterior and the posterior mode. We contrast the posterior mode to the maximum likelihood estimate, which, like the Bayesian estimates, is invariant under reparameterization. Along the way, we derive the logistic distribution by transforming a uniformly distributed variable.\nView(HTML)\n\nAuthor\n\nBob Carpenter\n\nKeywords\n\nMLE, Bayesian posterior, reparameterization, Jacobian, binomial\n\nSource Repository\n\nexample-models/knitr/mle-params (GitHub)\n\nR Package Dependencies\n\nrstan\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nHierarchical Two-Parameter Logistic Item Response Model\nThis case study documents a Stan model for the two-parameter logistic model (2PL) with hierarchical priors. A brief simulation indicates that the Stan model successfully recovers the generating parameters. An example using a grade 12 science assessment is provided.\nView(HTML)\n\nAuthor\n\nDaniel C. Furr\n\nKeywords\n\neducation, item response theory, two-parameter logistic model, hierarchical priors\n\nSource Repository\n\nexample-models/education/hierarchical_2pl (GitHub)\n\nR Package Dependencies\n\nrstan, ggplot2, mirt\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nRating Scale and Generalized Rating Scale Models with Latent Regression\nThis case study documents a Stan model for the rating scale model (RSM) and the generalized rating scale model (GRSM) with latent regression. The latent regression portion of the models may be restricted to an intercept only, yielding a standard RSM or GRSM. A brief simulation indicates that the Stan models successfully recover the generating parameters. An example using a survey of public perceptions of science and technology is provided.\nView(HTML)\n\nAuthors\n\nDaniel C. Furr\n\nKeywords\n\neducation, item response theory, rating scale model, generalized rating scale model\n\nSource Repository\n\nexample-models/education/rsm_and_grsm (GitHub)\n\nR Package Dependencies\n\nrstan, edstan, ggplot2, ltm\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nPartial Credit and Generalized Partial Credit Models with Latent Regression\nThis case study documents a Stan model for the partial credit model (PCM) and the generalized partial credit model (GPCM) with latent regression. The latent regression portion of the models may be restricted to an intercept only, yielding a standard PCM or GPCM. A brief simulation indicates that the Stan models successfully recover the generating parameters. An example using the TIMSS 2011 mathematics assessment is provided\nView(HTML)\n\nAuthors\n\nDaniel C. Furr\n\nKeywords\n\neducation, item response theory, partial credit model, generalized partial credit model\n\nSource Repository\n\nexample-models/education/pcm_and_gpcm (GitHub)\n\nR Package Dependencies\n\nrstan, edstan, ggplot2, TAM\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nRasch and Two-Parameter Logistic Item Response Models with Latent Regression\nThis case study documents Stan models for the Rasch and two-parameter logistic models with latent regression. The latent regression portion of the models may be restricted to an intercept only, yielding standard versions of the models. Simulations indicate that the two models successfully recover generating parameters. An example using a grade 12 science assessment is provided.\nView(HTML)\n\nAuthors\n\nDaniel C. Furr\n\nKeywords\n\neducation, item response theory, rasch model, two-parameter logistic model\n\nSource Repository\n\nexample-models/education/rasch_and_2pl.html (GitHub)\n\nR Package Dependencies\n\nrstan, edstan, ggplot2, TAM\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nTwo-Parameter Logistic Item Response Model\nThis tutorial introduces the R package edstan for estimating two-parameter logistic item response models using Stan without knowing the Stan language. Subsequently, the tutorial explains how the model can be expressed in the Stan language and fit using the rstan package. Specification of prior distributions and assessment of convergence are discussed. Using the Stan language directly has the advantage that it becomes quite easy to extend the model, and this is demonstrated by adding a latent regression and differential item functioning to the model. Posterior predictive model checking is also demonstrated.\nView(HTML)\n\nAuthor\n\nDaniel C. Furr, Seung Yeon Lee, Joon-Ho Lee, and Sophia Rabe-Hesketh\n\nKeywords\n\neducation, item response theory, two-parameter logistic model\n\nSource Repository\n\nexample-models/education/tutorial_twopl (GitHub)\n\nR Package Dependencies\n\nrstan, reshape2, ggplot2, gridExtra, devtools, edstan\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nCognitive Diagnosis Model: DINA model with independent attributes\nThis case study documents a Stan model for the DINA model with independent attributes. A Simulation indicates that the Stan model successfully recovers the generating parameters and predicts respondents’ attribute mastery. A Stan model with no structure of the attributes is also discussed and applied to the simulated data. An example using a subset of the fraction subtraction data is provided.\nView(HTML)\n\nAuthor\n\nSeung Yeon Lee\n\nKeywords\n\neducation, cognitive diagnosis model, diagnostic classification model, attribute mastery, DINA\n\nSource Repository\n\nexample-models/education/dina_independent (GitHub)\n\nR Package Dependencies\n\nrstan, ggplot2, CDM\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\n\nPooling with Hierarchical Models for Repeated Binary Trials\nThis note illustrates the effects on posterior inference of pooling data (aka sharing strength) across items for repeated binary trial data. It provides Stan models and R code to fit and check predictive models for three situations: (a) complete pooling, which assumes each item is the same, (b) no pooling, which assumes the items are unrelated, and (c) partial pooling, where the similarity among the items is estimated. We consider two hierarchical models to estimate the partial pooling, one with a beta prior on chance of success and another with a normal prior on the log odds of success. The note explains with working examples how to (i) fit models in RStan and plot the results in R using ggplot2, (ii) estimate event probabilities, (iii) evaluate posterior predictive densities to evaluate model predictions on held-out data, (iv) rank items by chance of success, (v) perform multiple comparisons in several settings, (vi) replicate new data for posterior p-values, and (vii) perform graphical posterior predictive checks.\nView(HTML)\n\nAuthor\n\nBob Carpenter\n\nKeywords\n\nbinary trials, pooling, hierarchical models, baseball, epidemiology, prediction, posterior predictive checks\n\nSource Repository\n\nexample-models/knitr/pool-binary-trials (GitHub)\n\nR Package Dependencies\n\nrstan, ggplot2, rmarkdown\n\nLicense\n\nBSD (3 clause), CC-BY\n\n\n\nRStanARM version\nThere is also a version of this case study in which all models are fit using the RStanARM interface. Many of the visualizations are also created using RStanARM’s plotting functions.\nView RStanARM version(HTML)\n\nAuthor\n\nBob Carpenter, Jonah Gabry, Ben Goodrich"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-2-2015",
    "href": "learn-stan/case-studies.html#volume-2-2015",
    "title": "Stan Case Studies",
    "section": "Volume 2 (2015)",
    "text": "Volume 2 (2015)\n\nMultiple Species-Site Occupancy Model\nThis case study replicates the analysis and output graphs of Dorazio et al. (2006) noisy-measurement occupancy model for multiple species abundance of butterflies. Going beyond the paper, the supercommunity assumptions are tested to show they are invariant to sizing, and posterior predictive checks are provided.\nView(HTML)\n\nAuthor\n\nBob Carpenter\n\nKeywords\n\necology, occupancy, species abundance, supercommunity, posterior predictive check\n\nSource Repository\n\nexample-models/knitr/dorazio-royle-occupancy (GitHub)\n\nLicense\n\nBSD (3 clause), CC-BY\n\nR Package Dependencies\n\nrstan, ggplot2, rmarkdown"
  },
  {
    "objectID": "learn-stan/case-studies.html#volume-1-2014",
    "href": "learn-stan/case-studies.html#volume-1-2014",
    "title": "Stan Case Studies",
    "section": "Volume 1 (2014)",
    "text": "Volume 1 (2014)\n\nSoil Carbon Modeling with RStan\nThis case study provides ordinary differential equation-based compartment models of soil carbon flux, with experimental data fitted with unknown initial compartment balance and noisy CO2 measurements. Results form Sierra and Müller’s (2014) soilR package are replicated.\nView(HTML)\n\nAuthor\n\nBob Carpenter\n\nKeywords\n\nbiogeochemistry, compartment ODE, soil carbon respiration, incubation experiment\n\nSource Repository\n\nsoil-metamodel/stan/soil-knit (GitHub)\n\nLicense\n\nBSD (3 clause), CC-BY\n\nR Package Dependencies\n\nrstan, ggplot2, rmarkdown"
  },
  {
    "objectID": "learn-stan/case-studies.html#contributing-case-studies",
    "href": "learn-stan/case-studies.html#contributing-case-studies",
    "title": "Stan Case Studies",
    "section": "Contributing Case Studies",
    "text": "Contributing Case Studies\nTo contribute a case study, please contact us through the Stan Forums. We require\n\na documented, reproducible example with narrative documentation (e.g., knitr or Jupyter with software/compiler versions noted and seeds fixed) and\nan open-source code license (preferably BSD or GPL for code, Creative Commons for text); authors retain all copyright."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "About the Stan Project",
    "section": "",
    "text": "Stan is freedom-respecting, open-source software (new BSD core, some interfaces GPLv3).  Stan is associated with NumFOCUS, a 501(c)(3) nonprofit supporting open code and reproducible science, through which you can help support Stan."
  },
  {
    "objectID": "about/index.html#open-code-reproducible-science",
    "href": "about/index.html#open-code-reproducible-science",
    "title": "About the Stan Project",
    "section": "",
    "text": "Stan is freedom-respecting, open-source software (new BSD core, some interfaces GPLv3).  Stan is associated with NumFOCUS, a 501(c)(3) nonprofit supporting open code and reproducible science, through which you can help support Stan."
  },
  {
    "objectID": "about/index.html#licensing",
    "href": "about/index.html#licensing",
    "title": "About the Stan Project",
    "section": "Licensing",
    "text": "Licensing\n\nComputer code: BSD 3-clause license,\nText content: CC-BY ND 4.0 license\n\n\nCopyright and trademark\n\nCopyright 2011–2024, Stan Development Team and their assignees.\nThe Stan name and logo are registered trademarks of NumFOCUS under the direction of the Stan Governing Body.\n\nBoth the Stan name and logo may be freely used when referencing Stan, for example, in blog posts, lecture notes, or open source software packages, but they may not be used in the branding of commercial entities, such as paid software packages or paid courses, without the express license of the Stan Governing Body. If you have any questions or would like to inquire about licensing the Stan name and/or logo for a commercial venture then please contact NumFOCUS at mailto:admin@numfocus.org.\nThe Stan name and Stan logo may not be used in ways that suggest the usage is endorsed by the Stan project without written permission from the Stan Governing Body.\nWhen using the Stan logo please use either the Stan Logo png or the Stan Logo svg."
  },
  {
    "objectID": "about/index.html#stan-development-team",
    "href": "about/index.html#stan-development-team",
    "title": "About the Stan Project",
    "section": "Stan Development Team",
    "text": "Stan Development Team\nThe project is a team effort comprised of full time and volunteer developers from around the world.\n\nDevelopment Team Members and Collaborators"
  },
  {
    "objectID": "about/index.html#stan-community-forums",
    "href": "about/index.html#stan-community-forums",
    "title": "About the Stan Project",
    "section": "Stan Community Forums",
    "text": "Stan Community Forums\n\nStan forums - message board for questions, discussion, and announcements related to Stan for both users and developers.\nStan slack - developer discussions\nGitHub issues - report bugs or suggest new features or enhancements."
  },
  {
    "objectID": "about/index.html#how-to-report-bugs",
    "href": "about/index.html#how-to-report-bugs",
    "title": "About the Stan Project",
    "section": "How to Report Bugs",
    "text": "How to Report Bugs\nThe key to a successful bug report is to provide as much context as possible, ideally in the form of a small reproducible example. Useful information includes:\n\nA description of what you were trying to do\nThe specific call to Stan that caused the bug\nThe complete error message. If the error message includes references a specific program line, then please include the Stan program.\nStan version information, ideally including the core Stan version, and version of the Stan interface.\nYour compute environment: operating system and C++ compiler."
  },
  {
    "objectID": "about/index.html#how-to-contribute-to-stan",
    "href": "about/index.html#how-to-contribute-to-stan",
    "title": "About the Stan Project",
    "section": "How to Contribute to Stan",
    "text": "How to Contribute to Stan\nWe welcome new Stan contributors!\n\nCode - If you’d like to contribute code, please consult the Developer Process Wiki or check out the GitHub issues on the Stan repositories for the tool that you’d like to contribute to and look for issues tagged “good first issue” or “help wanted”. The GitHub issues as well as the Stan Forums and Stan slack channel are all good places to discuss what you want to do beforehand.\nWrite - If you’d like to contribute case studies, tutorials and similar materials, please make a post on the Stan forums publicity category, and tag the @Stan_Development_Team.\nHost events - If you would like to organize a workshop or hackathon, please reach out to the community at large, via the Stan forums, or contact the Stan Governing Body at board@mc-stan.org."
  },
  {
    "objectID": "about/index.html#how-to-cite-stan",
    "href": "about/index.html#how-to-cite-stan",
    "title": "About the Stan Project",
    "section": "How to Cite Stan",
    "text": "How to Cite Stan\nWe appreciate citations for the Stan software because it lets us find out what people have been doing with Stan and motivate further grant funding.\nWe appreciate citations for the Stan software because it lets us find out what people have been doing with Stan and motivate further grant funding. When citing Stan we recommend citing both Stan itself as well as the particular interface used.\nTo cite Stan itself you can cite the Stan Reference Manual or Stan User’s Guide taking the year and version from the latest release on the Stan Project’s Github releases page: https://github.com/stan-dev/stan/releases.\n\nStan Development Team. YEAR. Stan Reference Manual, VERSION. https://mc-stan.org\nStan Development Team. YEAR. Stan User’s Guide, VERSION. https://mc-stan.org\n\nTo cite the interface or post-processing package you used we recommend using the citation conventions for the language (R, Python, Julia, etc.) associated with the interface or package. For example, to cite RStan use the standard conventions for citing R packages."
  },
  {
    "objectID": "about/index.html#help-fund-stan",
    "href": "about/index.html#help-fund-stan",
    "title": "About the Stan Project",
    "section": "Help Fund Stan",
    "text": "Help Fund Stan\nStan operates through NumFOCUS, a U.S. 501(c)(3) nonprofit organization that serves open-source software projects including NumPy, Julia, Jupyter, ScikitLearn, and many more.\n\nContribute to Stan via NumFOCUS    (Salsa Labs payment processing)\nSponsor Stan via GitHub"
  },
  {
    "objectID": "about/index.html#shop-for-stan-swag",
    "href": "about/index.html#shop-for-stan-swag",
    "title": "About the Stan Project",
    "section": "Shop for Stan Swag",
    "text": "Shop for Stan Swag\nIf you’re looking to infer in style, stop by one of our shops for Stan t-shirts and mugs.\n\nStan Shop [US]   (Spreadshirt US)\nStan Shop [UK]   (Spreadshirt UK)"
  },
  {
    "objectID": "about/index.html#acknowledgements",
    "href": "about/index.html#acknowledgements",
    "title": "About the Stan Project",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nStan has grown from a small research project started in 2011 at Columbia University to a global community of developers, researchers, and users. The Stan project owes its success to the contributions from hundreds of developers, researchers, active users, and funders. Individual contributions to the software and documentation can be tracked through GitHub.\nWe are grateful to all the users who have taken the time to file bug reports and feature requests via the GitHub and the Stan forums; this feedback has greatly improved Stan’s usability and reliability.\nStan has been funded through grants for Stan and its developers, through in-kind donations in the form of companies contributing developer time to Stan and individuals contributing their own time to Stan, and through donations to the open-source scientific software non-profit NumFOCUS. Stan development has been funded by grants from the U.S. Department of Energy, the U.S. Department of Education, the U.S. National Science Foundation, the U.S. Office of Naval Research, the U.S. National Institutes of Health, the Alfred P. Sloan Foundation, and the Chan Zuckerberg Foundation, and by gifts from Novartis, Google, and Facebook. Hosting and support for Stan’s continuous integration testing is generously provided by the Simons Foundation."
  },
  {
    "objectID": "learn-stan/stancon/StanConnect2022_space_time.html",
    "href": "learn-stan/stancon/StanConnect2022_space_time.html",
    "title": "StanConnect 2022: Stan Through Space and Time",
    "section": "",
    "text": "StanConnect 2022: Stan Through Space and Time\nSpeakers and topics\nEdgar Santos Fernandez Time and tide wait for no one: spatio-temporal modelling in river networks Abstract:\nSpatio-temporal models are widely used in many research areas including ecology and conservation. The recent proliferation of the use of in-situ sensors in streams and rivers supports space-time water quality modelling and monitoring in near real-time. In this presentation, we introduce a new family of Bayesian spatio-temporal models for river networks, in which spatial dependence is established based on stream distance and flow connectivity, and temporal autocorrelation is incorporated using vector autoregression approaches. We have developed several variations of these models within a Bayesian framework which have led to the creation of an R package (SSNbayes). Our results show that the proposed models perform well in terms of out-of-sample performance measures.\nBio:\nDr. Santos-Fernandez works on the development of statistical methodology across many domains such as ecology and conservation, citizen science, risk assessment and sports analytics. This includes advancing sampling techniques, spatio-temporal modelling, multivariate statistics, and anomaly detection.\nHe is currently working on spatio-temporal applications in river networks.\nMore details and recent publications can be found here: https://www.researchgate.net/profile/Edgar-Santos-Fernandez\nStan Yip Spatio-temporal modelling of mosquitoes vector and its environmental drivers in Hong Kong Abstract:\nIn this talk, we present an application of a spatio-temporal beta regression model in modelling mosquito vectors implemented in Stan language. The mosquito abundance indices, namely ovitrap and gravitrap indices are captured through a beta distribution model with support from zero to one. A hurdle model extension to this framework is also discussed.\nBio:\nDr Yip is a researcher in various areas in statistical applications primarily environmental sciences and climatology. Prior to his role in the Hong Kong Polytechnic University, he has spent a few years in multiple R&D roles in industry before returning to academia. He was a research scientist in National Centre for Atmospheric Science, a research associate in University of Exeter, a junior member of Isaac Newton Institute for Mathematical Sciences in Cambridge and a visiting scholar in Duke University.\nRafael Cabral Robust non-Gaussian models and how to fit them in Stan Abstract:\nTraditionally the excitation noise of spatial and temporal models is Gaussian. However, real-world data may not be Gaussian in nature, and it is well known that outliers can adversely affect the inferences and predictions made from a Gaussian model. In this talk, I will present a generic and robust class of non-Gaussian models that leads to more robust estimates and better predictions. If you already have a Gaussian model implemented in Stan you will only need to change one line of code!\nBio:\nMy name is Rafael and I am PhD candidate at KAUST, Saudi Arabia, being supervised by Profs. Haavard Rue, and David Bolin. My PhD research revolves around building more flexible, robust, and computationally efficient modeling frameworks for spatial and temporal data. I’ve worked with Gaussian and non-Gaussian processes, model criticism and robustness, and approximate inference with Stan, INLA, and variational inference.\nPierfrancesco Alaimo Di Loro A space-time extension of the Poisson auto-regression to model Covid-19 cases at the England local authorities level Abstract:\nThe incidence of an infectious disease is one of the main indicators to describe the evolution of an epidemic process in a population. Understanding its pattern is key to addressing public health policies and verifying their effectiveness.\nHere, we propose a space-time extension of the Poisson auto-regression to model the local incidences collected over different areas. We set up a generalized linear framework to link the auto-regressive coefficient and the baseline rate to observed covariates and space-time CAR-AR Leroux random effects.\nThe estimation is carried out in a Bayesian Framework through STAN. The fit of such a complex model requires adopting efficient strategies to speed up the likelihood evaluation and reach convergence in due time.\nWe model the number of weekly COVID-19 cases recorded in 313 English districts during the second and third waves of the COVID-19 pandemic. We consider two alternative sets of observed covariates: the level of local restriction currently in place; the value of various Google mobility indices. We first verify the convergence of the estimation mechanism and the ability of the model to recover the true parameters in an extensive simulation study. Then, we fit the model and simpler versions of it on the observed data. The full model outplays all others according to multiple metrics. It allows quantifying the relative importance of previous lags and evaluating the relative importance of the hidden cases across space and time.\nBio:\nI am a Junior Assistant Professor at the Dpt. GEPLI of LUMSA University and collaborator with the S3RI institute of the University of Southampton. My research interests concern the study of spatial and spatio-temporal phenomena, with a particular focus on the Bayesian Hierarchical Modeling of large geo-referenced data.\nSujit Sahu Fitting spatio-temporal geostatistical models in Stan using the bmstdr R package. Abstract:\nIn this talk I present the recently published R package bmstdr that is able to fit several Bayesian spatial and spatio-temporal models. Point referenced data are modeled using Gaussian processes and Gaussian error distributions. Two model fitting engines: Bspatial for spatial only point referenced data and Bsptime for spatio-temporal data are included in the package. Both of these engines admit “Stan” as one of the package options among other possibilities such as spBayes, spTimer, spTDyn and INLA. A third model fitting function, Bmoving_sptime, is provided for fitting irregularly observed spatio-temporal data possibly from a set of moving sensors.\nThe user of bmstdr is afforded the flexibility to name particular rows of their input data frame for validation purposes.\nThe package allows quick comparison of models using both model choice criteria, such as DIC and WAIC, and K-fold cross-validation without much programming effort. Familiar linear model fit exploration tools and diagnostic plots are included through the S3 methods such as summary, residuals and plot implemented for the three bmstdr functions. Our illustrations show that compared to some other packages Stan fitted spatio-temporal models validate better, and also perform better according to some model choice criteria such as the WAIC.\nBio:\nSujit Sahu is a Professor of Statistics at the University of Southampton. He is the author of the book Bayesian modeling of spatio-temporal data with R published by Chapman and Hall/CRC Press.\nPackage site: https://cran.r-project.org/web/packages/bmstdr/vignettes/bmstdr-vig_bookdown.html\nSilvia De Nicolo tipsae: Tools for Handling Indices and Proportions in Small Area Estimation Abstract:\nThe tipsae package implements a set of small area estimation tools for mapping proportions and indicators defined on the unit interval. It provides for small area models defined at area level, including the classical Beta regression, Zero and/or One Inflated Beta and Flexible Beta ones. The models, developed within a Bayesian framework, are estimated through Stan language, allowing fast estimation and customized parallel computing. To account for possible dependency structure in the data, we enable the inclusion of spatial and/or temporal random effects in the linear predictor by means of Intrinsic Conditional Auto-Regressive and Random Walk priors. The additional features of the tipsae package, such as diagnostics, visualization and exporting functions as well as variance smoothing and benchmarking functions, improve the user experience through the entire process of estimation, validation and outcome presentation. A Shiny application with a user-friendly interface further eases the implementation of Bayesian models for small area analysis.\nBio:\nPost-Doctoral Fellow at the University of Bologna, her main research interests concern Bayesian hierarchical models, small area estimation and their application to inequality and poverty measurement.\nConnor Donegan geostan: An R package for Bayesian spatial analysis Abstract:\nThis presentation will introduce geostan, an R package that provides access to pre-built Stan models and other functions for analyzing spatial data. The project aims to support and facilitate a full spatial analysis workflow, from exploratory analysis to modeling and model evaluation. A unique feature of the package is its spatial measurement error models, which enable researchers to incorporate data quality information from error-laden, survey-based covariates. In addition to providing access to a variety of pre-built models (GLMs, CAR, BYM, SAR, ESF), geostan provides tools for building custom, computationally efficient spatial models in Stan. A geostan workflow will be illustrated through an analysis of small-area colorectal cancer incidence in Texas metropolitan areas.\nBio:\nConnor Donegan is a doctoral candidate in Geospatial Information Sciences at The University of Texas at Dallas, and a research assistant in the Peter O’Donnell Jr. School of Public Health at UT Southwestern Medical Center. He studies health geography, spatial statistics, and epistemology.\nMarco Gramatica Structure induced by a multiple membership transformation on the conditional autoregressive model Abstract:\nThe usual context for disease mapping is to model data aggregated at the areal level. In some contexts, however, (e.g. residential histories, general practitioner catchment areas) the data are not recorded on a particular spatial framework, but it is possible to specify spatial random effects, or covariate effects, at the areal level, by using a multiple membership principle (MM). In fact, both Petrof (2020) and Gramatica (2021) use a weighted average of conditional autoregressive (CAR) spatial random effects to embed spatial information for a spatially-misaligned outcome and estimate relative risk for both frameworks (areas and memberships). In this talk we investigate the application of the MM principle to the CAR prior in terms of its parameterisation, properness and identifiability. We carry out simulations involving different numbers of memberships as compared to number of areas and assess impact of this on estimating of CAR parameters and relative risks. Results show that overall posterior samples are well calibrated for both frameworks across all simulation scenarios. Finally, we present the results of an application of the MM modelling strategy to diabetes prevalence data in South London.\nBio:\nMarco Gramatica has just completed his PhD with a thesis on Bayesian modelling of spatially misaligned data. His research focused on the use of CAR priors and Multiple Membership to jointly model data recorded on different spatial frameworks. He is now a Postdoctoral Research Assistant at Queen Mary University of London.\nVictoire Michal A Bayesian hierarchical model for disease mapping that accounts for scaling and heavy-tailed latent effects Abstract:\nIn disease mapping, we estimate the relative risk of a disease across different areas within a region of interest. The number of cases in an area is often modelled through a Poisson distribution with mean given by the product between an offset and the logarithm of the relative risk of the disease. The Besag, York and Mollié model, commonly used to account for potential overdispersion and a spatial correlation structure among the counts, does not accommodate outliers. An area may be one of two types of outliers: it may be an outlier in the usual sense, exhibiting an extreme disease risk, or it may be a spatial outlier. Spatial outliers refer to risks that are outliers with respect to their neighbors. We build on the Bayesian hierarchical model proposed by Riebler et al. (2016) and assume a scale mixture structure wherein the variance of the latent process changes across areas and allows for outlier identification. We compare our approach with that proposed by Congdon (2017), in an analysis of cases of Zika during the 2015-2016 epidemic in Rio de Janeiro. This is joint work with Laís Picinini Freitas (Université de Montréal) and Alexandra M. Schmidt (McGill University).\nBio:\nVictoire is a PhD student in the Biostatistics graduate program at McGill University. She studies Spatial Statistics and Disease Mapping to model the cases of Zika in Rio de Janeiro as well as Small Area Estimation and Record Linkage to estimate the household consumption in Ghana at a fine aggregation level. She holds a Bachelor’s degree in Mathematics and a Master’s degree in Statistics, both from the University of Montreal.\nRachary Roman Bayesian latent spatial autoregressive growth modeling. Abstract:\nStructural Equation Models (SEM) are widely used in behavioral research for measuring and testing multi-faceted constructs. The integration of spatial models to behavioral phenomena particularly in psychology has been limited. This in part may be due to the requirements for observed variables that traditional spatial approaches require. In this talk I will present a recent adaptation I developed to accommodate spatial autoregressive effects with a common latent variable approach, Latent Growth Modeling (LGM). This approach can be seen as a latent parameterization of mixed effects models with the added flexibility of the latent variable framework. I will emphasize the MCMC application written in Stan, an example applied to German Covid-19 data will also be presented.\nBio:\nDr. Zachary Roman is a postdoctoral researcher in the psychology department at the University of Zurich, he is also affiliated with the Method Center at the University of Tuebingen. Zachary completed his Ph.D. in quantitative psychology at the University of Kansas in 2019. His research interests include the applications of spatial / social network autoregressive approaches in the behavioral sciences, specifically focused on methodological developments in this area.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "install/index.html",
    "href": "install/index.html",
    "title": "Getting Started",
    "section": "",
    "text": "To analyze your data with Stan, you can either"
  },
  {
    "objectID": "install/index.html#download-and-install-stan",
    "href": "install/index.html#download-and-install-stan",
    "title": "Getting Started",
    "section": "Download and Install Stan",
    "text": "Download and Install Stan\nTo compile and run Stan models directly from within R, Python, or Julia, select your OS, programming language interface, and preferred installation method in the grid below. For other programming environments, skip to Other Programming Environments\n\n\n\n\n\n\nPrerequisites\n\n\n\nStan requires a C++17 compiler and some build utilities.. The conda option of certain packages can install these for you, otherwise these are bundled together by Rtools.\n\n\nStan requires a C++17 compiler. The conda option of certain packages can install this for you, or we recommend to install Xcode from the App Store and then run xcode-select --install.\n\n\nStan requires a C++17 compiler. The conda option of certain packages can install this for you, or on .deb based distros, sudo apt-get install build-essential will install what you need.\n\n\n\n\n\nHow to Install\n\n\n\nPlease select interface and preferred package manager.\n\n\n\nRun pip install cmdstanpy. Then, in Python, run import cmdstanpy; cmdstanpy.install_cmdstan() or follow the manual installation instructions for CmdStan.\nFor more information, see the CmdStanPy documentation.\n\n\nRun conda install -c conda-forge cmdstanpy.\nNote: this will also install CmdStan and any system prerequisites.\n\n\nRun pip install -e git+https://github.com/stan-dev/cmdstanpy@develop#egg=cmdstanpy. Then, in Python, run import cmdstanpy; cmdstanpy.install_cmdstan() or follow the manual installation instructions for CmdStan.\nFor more information, see the CmdStanPy documentation.\n\n\n\nIn R, run install.packages(\"cmdstanr\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\"))). Then run cmdstanr::install_cmdstan() or follow the manual installation instructions for CmdStan.\nFor more information, see the CmdStanR documentation\n\n\nRun conda install -c conda-forge r-cmdstanr.\nNote: this will also install CmdStan and any system prerequisites.\n\n\nIn R, run remotes::install_github(\"stan-dev/cmdstanr\").\nThen run cmdstanr::install_cmdstan() or follow the manual installation instructions for CmdStan.\nFor more information, see the CmdStanR documentation\n\n\n\nDownload a release from GitHub: https://github.com/stan-dev/cmdstan/releases.\nThen follow these instructions to build CmdStan.\n\n\nRun conda install -c conda-forge cmdstan.\nNote: this will also install CmdStan and any system prerequisites.\n\n\nRun git clone https://github.com/stan-dev/cmdstan.git --recursive\nThis will download the source code from the current development branch of CmdStan into a directory named cmdstan, along with the submodules for core Stan code and the Stan math library.\nThen follow these instructions to build CmdStan.\n\n\n\nIn R, run install.packages(\"rstan\")\nFor more information, see the RStan Getting Started wiki\n\n\nIn R, run install.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\"))).\nFor more information, see the RStan Getting Started wiki\n\n\nRun conda install -c conda-forge r-rstan.\nNote: this will also install any system prerequisites.\n\n\nremotes::install_github(“stan-dev/rstan”, ref = “develop”, subdir = “rstan/rstan”)\nFor more information, see the RStan wiki page Installing RStan from Source\n\n\n\nTo install Stan.jl e.g. in the Julia REPL: ] add Stan, then use Conda.jl or conda to install CmdStan.\nSee Stan.jl documentation for further details.\n\n\nRun https://github.com/StanJulia/Stan.jl.git, then follow instructions in the README file.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nConda simplifies the installation process by ensuring that all required libraries and tools are compatible with each other and is available for Linux, Mac, and Windows platforms.\nYou can either install miniconda, a free, minimal installer for conda or you can get the full Anaconda system which provides graphical installer wizards for MacOS and Windows users.\nJulia users can install Conda.jl.\n\n\n\nOther Programming Environments\n\n\n\n\n\n\n\n\nLanguage\nTool\nDescription\n\n\n\n\nR, Python\nGoogle Colab\nPrebuilt CmdStan binaries for Google Colab are available from the GitHub CmdStan releases page. Installing these binaries at the start of a Colab session is much faster than installing CmdStan during a Colab session.\n\n\nMathematica\nMathematicaStan\nStan interface for Mathematica. Available from its GitHub repository.\n\n\nMATLAB\nMatlabStan\nInstallation instructions available on the MatlabStan wiki.\n\n\nPython\nPyStan\nAvailable via pip. Run command: python -m pip install pystan.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs of Release 3.10.0, PyStan is no longer being actively supported."
  },
  {
    "objectID": "install/index.html#prerequisite-c17-toolchain",
    "href": "install/index.html#prerequisite-c17-toolchain",
    "title": "Getting Started",
    "section": "Prerequisite: C++17 toolchain",
    "text": "Prerequisite: C++17 toolchain\nStan models are specified using the Stan language which are then compiled to executable programs that can be run on your data to perform inference and make predictions. To use Stan from within your preferred programming environment, you need a C++ toolchain comprised of a C++17 compiler and the GNU Make utility.\n\nOn Linux, these are bundled into the meta-package build-essential. To install, run command:\nsudo apt-get install build-essential\nOn Mac, the Clang compiler and GNU Make are included with Xcode, the Apple toolset for software developers. Install Xcode from the App Store and then run command:\nxcode-select --install\n\n\nOn Windows 10, there are two ways to get a Stan-compatible C++ toolchain:\n\nUse the conda installer for CmdStan, CmdStanPy or CmdStanR or RStan, since these packages all include the required toolchain.\nGet Rtools which includes a C++17 compiler, GNU Make for windows, and a few Unix utilities.\nCmdStanR users can call the internal function cmdstanr:::install_toolchain.\nCmdStanPy provides both the function cmdstanpy::get_cxx_toolchain and command line script get_cxx_toolchain."
  },
  {
    "objectID": "install/index.html#local-cmdstan-installations-for-cmdstanpy-cmdstanr-and-stan.jl",
    "href": "install/index.html#local-cmdstan-installations-for-cmdstanpy-cmdstanr-and-stan.jl",
    "title": "Getting Started",
    "section": "Local CmdStan installations for CmdStanPy, CmdStanR, and Stan.jl",
    "text": "Local CmdStan installations for CmdStanPy, CmdStanR, and Stan.jl\nCmdStanPy, CmdStanR, and Stan.jl and require a local CmdStan installation. Both CmdStanPy and CmdStanR provide method install_cmdstan to do this from within Python or R; and CmdStanPy also provides this as a command-line function. See the online documentation:\n\nCmdStanPy: install_cmdstan function\nCmdStanR: install_cmdstan function\nStan.jl build instructions\n\nThe default installation location is in the user’s home directory and is named .cmdstan (a hidden directory). This directory contains one or more versions of CmdStan.\nBoth CmdStanPy and CmdStanR provide the following functions:\n\nrebuild_cmdstan - rebuild the specified release. On Mac, often required after an Xcode update.\ncmdstan_path - displays the path to the local install of CmdStan; path name includes version number.\nset_cmdstan_path - specify which version of CmdStan to use."
  },
  {
    "objectID": "install/index.html#troubleshooting-the-install",
    "href": "install/index.html#troubleshooting-the-install",
    "title": "Getting Started",
    "section": "Troubleshooting the Install",
    "text": "Troubleshooting the Install\nTo help troubleshoot problems that arise when trying to use Stan, we provide the following summary of the chain of events in conditioning a model on data and doing inference:\n\nCompile model\n\nStan compiler translates Stan file to C++ file\nC++ file is compiled to executable program, via GNU Make\n\nRun inference algorithm\n\nInterfaces run compiled executable program\nCompiled executable generates per-chain outputs\n\n\nIf the program contains syntax errors, these will be caught and reported by the Stan compiler (program stanc). If the Stan program is successfully translated to C++, then it should compile; error messages from the C++ compiler indicate a problem with the C++ toolchain.\nIf a model fails to run or appears to run slowly, this is a strong indication that the model is poorly specified given the data. Consult the Stan User’s Guide or search the Stan Forums on Discourse\nCommon Points of failure; how to address them.\n\nSoftware download failed.\n\nworkaround: check internet connectivity, disk space, and file permissions\n\nC++ components fail to compile\n\nWorkaround: we highly recommend installing using conda to create a clean environment for Stan and its toolchain.\n\nStan model fails to compile with error message about a ““.(PCH file)()\n\nFix: for CmdStan based systems, rebuild CmdStan.\n\n\nSee CmdStan Guide section Troubleshooting the Installation for further details."
  },
  {
    "objectID": "install/index.html#high-level-stan-interfaces",
    "href": "install/index.html#high-level-stan-interfaces",
    "title": "Getting Started",
    "section": "High-level Stan Interfaces",
    "text": "High-level Stan Interfaces\n\n\n\n\n\n\n\n\nLanguage\nTool\nDescription\n\n\n\n\nR\nbrms\nUse extended lme4-like formula syntax to specify and fit multivariate and multilevel models in Stan. (Requires CmdStanR and C++ compiler.)\n\n\nR\nRStanArm\nProvides stable, efficient Stan versions of R model-fitting packages. (Stan models are pre-compiled, no C++ compiler needed.)\n\n\nR\nRethinking\nAccompanies the book and course materials for Statistical Rethinking, 2nd Ed by Richard McElreath. (Requires CmdStanR and C++ compiler.)"
  },
  {
    "objectID": "install/index.html#introductory-notebooks-vignettes-and-tutorials",
    "href": "install/index.html#introductory-notebooks-vignettes-and-tutorials",
    "title": "Getting Started",
    "section": "Introductory Notebooks,  Vignettes,  and Tutorials",
    "text": "Introductory Notebooks,  Vignettes,  and Tutorials\n\n\n\n\n\n\n\nInterface\nTitle\n\n\n\n\nCmdStanPy\nCmdStanPy “Hello, World!”Getting Started with Bayesian Statistics using Stan and PythonMultilevel regression modeling with CmdStanPy and plotnine\n\n\nCmdStanR\nGetting Started with CmdStanRStanCon2023/Stan_tutorial.ipynb\n\n\nJulia\nStan.jl Examples\n\n\nGoogle Colab\nStan Notebooks in the Cloud\n\n\n\nFor more learning resources, see the Tutorials, Publications and Stan Case Studies pages."
  },
  {
    "objectID": "learn-stan/diagnostics-warnings.html",
    "href": "learn-stan/diagnostics-warnings.html",
    "title": "How to Diagnose and Resolve Convergence Problems",
    "section": "",
    "text": "A big advantage of Stan is that it employs a range of diagnostics to let you notice many potential problems with your model — Stan is conservative and throws warnings for anything suspicious. Here we walk through the types of warnings and hints to help you diagnose and resolve underlying modelling problems. If you fail to diagnose/resolve the problems with the model yourself or if you have trouble understanding or applying some of the hints, don’t worry, you are welcome to ask on Stan Discourse, we’ll try to help!\nFor guidance on warnings that occur when compiling the model, see Stan User’s guide on errors and warnings."
  },
  {
    "objectID": "learn-stan/diagnostics-warnings.html#when-can-warnings-be-ignored",
    "href": "learn-stan/diagnostics-warnings.html#when-can-warnings-be-ignored",
    "title": "How to Diagnose and Resolve Convergence Problems",
    "section": "When can warnings be ignored",
    "text": "When can warnings be ignored\nIn most cases the warnings actually indicate important problems with your model. This does not mean that every time you see a warning the model estimates are meaningless, but when you see warnings you shouldn’t trust your estimates without first understanding what the warnings mean.\nHowever, in early stages of a modelling workflow, we often don’t need completely reliable inference, and a roughly correct posterior can be enough to let us check if the model is sensible using posterior predictive checks. If warnings occur rarely or the diagnostics are just somewhat above the recommended threshold, it often makes sense to do some rough sanity checks before investigating the warnings in detail. This can help to avoid investing a lot of time debugging a model that would be discarded anyway due to lack of fit to data or other conceptual problems."
  },
  {
    "objectID": "learn-stan/diagnostics-warnings.html#types-of-warnings",
    "href": "learn-stan/diagnostics-warnings.html#types-of-warnings",
    "title": "How to Diagnose and Resolve Convergence Problems",
    "section": "Types of warnings",
    "text": "Types of warnings\n\nDivergent transitions after warmup\nExceptions thrown when computing target density and its gradient (Hamiltonian proposal rejected)\nR-hat\nBulk- and Tail-ESS\nMaximum treedepth\nBFMI low\n\n\nDivergent transitions after warmup\nExample:\n1: There were 15 divergent transitions after warmup.\nStan uses Hamiltonian Monte Carlo (HMC) to explore the target distribution — the posterior defined by a Stan program + data — by simulating the evolution of a Hamiltonian system. In order to approximate the exact solution of the Hamiltonian dynamics we need to choose a step size governing how far we move each time we evolve the system forward. The step size controls the resolution of the sampler.\nFor hard problems the curvature of the posterior can vary a lot and there can be features of the target distribution that are too small for this resolution. For example, divergences are likely if the log posterior density doesn’t have continuous derivative everywhere. Consequently the sampler misses those features and returns biased estimates. Fortunately, this mismatch of scales manifests as divergences which provide a practical diagnostic.\nEven a small number of divergences after warmup cannot be safely ignored if completely reliable inference is desired. But if you get only few divergences and you get good Rhat and ESS values, the resulting posterior is often good enough to move forward. There are also cases when a small number divergences without any pattern in their locations can be verified to be unimportant, but this cannot be safely assumed without a careful investigation of the model.\nFurther reading on divergences:\n\nIdentity Crisis - a discussion of the causes of divergences, diagnosis and treatment.\nTaming divergences in Stan models a less mathematical but hopefully more accessible intuition on what divergent transitions are.\nA Conceptual Introduction to Hamiltonian Monte Carlo\n\n\n\nExceptions thrown when Hamiltonian proposal rejected\nExamples:\nException thrown at line 24: normal_log: Scale parameter is 0, but must be positive! \n-----\nRejecting initial value:\nGradient evaluated at the initial value is not finite.\nStan can’t start sampling from this initial value.\nThe first warning indicates that the standard deviation (scale parameter) of the normal distribution (at line 24 in the Stan program) is 0, but it must be positive for Stan to compute the value of density function.\nThe second message indicates that the gradient of the target (as computed by Stan’s automatic differention) is infinite, indicating numerical problems somewhere in the model but unfortunately without clear information about where exactly.\nThere are many other types of these warnings. A message like this does not necessarily indicate that something is wrong with your model, as it’s possible that even in a correctly specified and generally well-behaved model a numerical inaccuracy is occasionally seen, especially during warmup (for example, scale parameter with zero lower bound can still become numerically indistinguishable from 0). If such warnings occur only before the first iteration it is a sign that initialization is problematic. In this case changing the initial values can help but is not necessary if there are no further warnings. Warnings after the first few iterations may indicate bugs or numerical issues in the model and if exceptions occur many times, investigation is definitely warranted.\nFurther reading on numerical accuracy of floating point format used in the computation: - Lecture notes by Geyer (2020) “Stat 3701 Lecture Notes: Computer Arithmetic” provide code examples in R illustrating the most common issues in floating-point arithmetic which can cause also unwanted rounding to the boundary of the constraint.\n\n\nR-hat\nR-hat (sometimes also Rhat) convergence diagnostic compares the between- and within-chain estimates for model parameters and other univariate quantities of interest. If chains have not mixed well (so that between- and within-chain estimates don’t agree), R-hat is larger than 1. We recommend running at least four chains by default and in general only fully trust the sample if R-hat is less than 1.01. In early workflow, R-hat below 1.1 is often sufficient.\nStan reports R-hat as the maximum of rank normalized split-R-hat and rank normalized folded-split-R-hat, which works for thick tailed distributions and is sensitive also to differences in scale.\nHigh R-hat means that the chains have not mixed well and so there is not a good reason to think of them as them being fully representative of the posterior.\nWhen there are divergent transitions in the model, high R-hat is often just another symptom of the problematic geometry that caused the divergences. If there are “maximum treedepth” warnings alongside high R-hat, it usually reflects a problematic geometry of the posterior that is hard to traverse efficiently and is often a side effect of setting very high adapt_delta in an attempt to resolve divergences. High R-hat without other warnings is commonly associated with posteriors that have multiple well-separated modes for the offending parameters, but can also arise in unimodal posteriors with difficult geometry, in case of high correlations, or in near improper posteriors.\nR-hat and ESSs (see below) are useful quick summaries, but for the final results, it is useful to check also Monte Carlo standard error for the quantities of interest and compare that to the domain knowledge of the required accuracy, and if it is low, then run longer chains to get more samples from the posterior.\nFurther reading on R-hat:\n\nRank-normalization, folding, and localization: An improved \\(\\hat{R}\\) for assessing convergence of MCMC\nDocumentation for the rhat() function in the posterior package.\n\n\n\nBulk and Tail ESS\nRoughly speaking, the effective sample size (ESS) of a quantity of interest captures how many independent draws contain the same amount of information as the dependent sample obtained by the MCMC algorithm. The higher the ESS the better. Stan uses R-hat adjustment to use the within- and between-chain information in computing the ESS. For example, in case of multimodal distributions with well-separated modes, this leads to an ESS estimate that is close to the number of distinct modes that are found.\nBulk-ESS refers to the effective sample size based on the rank normalized draws. This does not directly compute the ESS relevant for computing the mean of the parameter, but instead computes a quantity that is well defined even if the chains do not have finite mean or variance. Overall bulk-ESS estimates the sampling efficiency for location summaries such as mean and median. Often smaller ESS would be sufficient for the desired estimation accuracy, but the estimation of ESS and convergence diagnostics themselves require higher ESS. For final results, we recommend requiring that the bulk-ESS is greater than 100 times the number of chains. For example, when running four chains, this corresponds to having a rank-normalized effective sample size of at least 400. In early workflow, ESS &gt; 20 is often sufficient.\nTail-ESS computes the minimum of the effective sample sizes (ESS) of the 5% and 95% quantiles. Tail-ESS can help diagnose problems due to different scales of the chains and slow mixing in the tails. If one or more chains has no draws below the 5% or above 95% quantiles computed from the all draws, there will be no direct way to assess that those chains are not completely failing, and thus NA (not available) can be reported for tail-ESS. &gt; [name=Paul] We recently changed this policy in posterior because constant within chain was too conservative and turned out to return NA to oft\nIn most cases, low bulk-ESS and tail-ESS is accompanied by large R-hat, and all recommendations for large Rhat are often useful in dealing with low ESS.\nR-hat and ESS are useful quick summaries, but for final results, it can useful to check also Monte Carlo standard error for the quantities of interest and compare that to the domain knowledge of the required accuracy, and if needed then sample from the posterior.\nFurther reading on ESS:\n\nRank-normalization, folding, and localization: An improved \\(\\hat{R}\\) for assessing convergence of MCMC\nComparison of MCMC effective sample size estimators.\nDocumentation for ess_bulk() and ess_tail() in the posterior package.\n\n\n\nMaximum treedepth\nWarnings about hitting the maximum treedepth are not as serious as other warnings. While divergent transitions, high R-hat and low ESS are a validity concern, hitting the maximum treedepth is an efficiency concern. Configuring the No-U-Turn-Sampler (the variant of HMC used by Stan) involves putting a cap on the number of simulation steps it evaluates during each iteration (for details on this see the Hamiltonian Monte Carlo Sampling chapter in the Stan manual). This is controlled through a maximum tree depth parameter max_treedepth where the maximum number of steps is 2^max_treedepth. When the maximum allowed tree depth is reached it indicates that NUTS might be terminating prematurely to avoid excessively long execution time.\nIf this is the only warning you are getting and your ESS and R-hat diagnostics are good, it is likely safe to ignore this warning, but finding the root cause could result in a more efficient model.\nWe do not generally recommend increasing max treedepth. In practice, the max treedepth limit being reached can be a sign of model misspecification, and to increase max treedepth can then result in just taking longer to fit a model that you don’t want to be fitting.\n\n\nBFMI low\nExample:\nThe E-BFMI 0.2786 is below the nominal threshold of 0.3 which suggests that\nHMC may have trouble exploring the target distribution.\nYou may see a warning that says some number of chains had an estimated BFMI that was too low. This implies that the adaptation phase of the Markov chains did not turn out well or the posterior has thick tails that were not well explored in the simulation. As with other warnings, if the BFMI value is just slightly below the threshold, the posterior is likely good for sanity checks but probably still not good for final inferences.\nA brief technical description of the diagnostic can be found in section 6.1 of A Conceptual Introduction to Hamiltonian Monte Carlo and a thorough account is given in https://arxiv.org/abs/1604.00695."
  },
  {
    "objectID": "learn-stan/diagnostics-warnings.html#diagnosing-and-resolving-problems",
    "href": "learn-stan/diagnostics-warnings.html#diagnosing-and-resolving-problems",
    "title": "How to Diagnose and Resolve Convergence Problems",
    "section": "Diagnosing and resolving problems",
    "text": "Diagnosing and resolving problems\nDiagnosing problems is best thought of as a part of larger workflow of model building, testing and critique/evaluation. Building blocks of such workflow are provided in our Bayesian Workflow article.\nSome approaches that can be helpful for most types of warnings follow. This is not, and can’t be, a definitive guide — each case is problematic in its own way and there is no single approach that would always work.\nIf you are using Stan via a package or other higher-level interface such as rstanarm or brms, your options are more limited, but the general advice below still applies. Additionally you may want to consult the package documentation or the underlying Stan code to get a good understanding on what is happening under the hood.\n\nReduce your model. Find the smallest / least complex model and a (preferably simulated) dataset that shows problems. Only add more complexity after you resolve or at least understand all the issues with the small model. If your model has multiple components (for example, a linear predictor for parameters in an ODE model), build and test small models where each of the components is separate (for example, a separate linear model and separate ODE model with constant parameters). For higher-level interfaces, this usually means removing predictor terms.\n\nThe small model implementation workflow vignette from the SBC package shows an example of building a larger model from simpler components step-by-step.\n\nReduce your data (if it is big). Diagnosing a model that takes a long time to fit and has many parameters is difficult, so working with a suitable subset of the data can speed the process up. This is however a double-edged sword as some problems can be caused by having not enough data, notably fitting varying slopes/intercepts for less than 3 categories can be problematic if priors are weak.\n\nYou can often reduce the model and data at the same time - e.g. if your linear model has multiple predictors, you can filter the data to only contain rows with a specific value of a categorical predictor (or narrow range of a continuous predictor) and then remove the predictor from the model.\n\nUse stronger priors. Often a computation is slow to converge because it is drifting all over parameter space. A reasonable prior can control things by reducing the range of operation. Including a reasonable prior will not always solve problems of computation, but often it gets rid of the worst problems.\nVisualisations: in R some very useful plots are provided by the bayesplot package (e.g. mcmc_parcoord, mcmc_pairs, mcmc_rank_overlay) and the shinystan package (which provides an interactive GUI). In Python and Julia the ArviZ and ArviZ.jl packages provide similar functionality. The most commonly useful visualisation is the pairs plot where well behaved models commonly show a set of “Gaussian blobs” while any other pattern can indicate problems. Further reading:\n\nbayesplot vignette on diagnostics\nArviZ example gallery\nCase study - diagnosing a multilevel model\nThe small model implementation workflow vignette from the SBC package shows (among other things) how several problems manifest in a pairs plot.\nGabry et al. 2019 - Visualization in Bayesian workflow\n\nMake sure all parameters in your model are well informed by the model. Typical problematic situations are when large changes in parameters can result in almost the same posterior density (for example complete separation in a logistic regression) and multimodality (multiple local maxima of the posterior distributions). Further reading:\n\nCase study - mixture models\nIdentifying non-identifiability - some informal intuition of the concept and examples of problematic models and how to spot them.\nUnderdetermined linear regression discusses problems arising when the data cannot inform all parameters.\nhttps://discourse.mc-stan.org/t/interpretation-of-cor-term-from-multivariate-animal-models/16703/26 has an example where a varying intercept at individual-level is not informed by the data.\n\nIf computation is very slow in the warmup stage, this can sometimes be fixed by choosing more reasonable initial values rather than using Stan’s defaults, which can be so widely dispersed that for models with complex geometries, chains can get stuck in unimportant regions of parameter space.\nSpecifically for divergences/max treedepth:\n\nCheck that the log posterior density and its derivatives are everywhere continuous. Numerical instabilities and inaccuracies can also cause the log density or its derivatives to behave discontinuously in practice, even if they are continuous in theory.\nIf the model seems “almost OK” or it is very fast to fit, you might be able to resolve the warnings by playing a bit with adapt_delta and stepsize; see here for an example. Increasing adapt_delta in particular has become common as the go-to first thing people try, and while there are cases where it can be useful, it may also be possible to obtain more efficient sampling with the default adapt_delta and modified model, prior or parameterization. Increasing adapt_delta/max_treedepth will likely increase the sampling time per iteration. You are more likely to achieve both better sampling performance and a more robust model (not to mention understanding thereof) by pursing the above options and leaving adjustment of adapt_delta as a last-resort. Increasing adapt_delta beyond 0.99 and max_treedepth beyond 12 is seldom useful. For the purpose of diagnosis, it is sometimes helpful to have more divergences, so reverting to default settings for diagnosis can be useful. Increasing adapt_delta/max_treedepth is rarely useful to resolve Rhat/ESS/BFMI issues.\n\nCreate a simulated dataset with known true values of all parameters. If the errors disappear on simulated data and the posterior mostly covers the “true” parameters from the simulation, your model may be a bad fit for the actual observed data. If you are using a higher-level interface, writing code to simulate data is a great way to test whether you understand what the model is actually doing. Further reading:\n\nFalling (In Love With Principled Modeling) has examples of using Stan to simulate data.\nhttps://discourse.mc-stan.org/t/failure-to-recover-simulated-group-means-in-cross-classified-lmm-with-monotonic-predictor/12978 shows an R simulation for a complex model and how it helped diagnose a bug.\nbrms can simulate datasets using the sample_prior = \"only\" argument (see the docs for more details).\n\nMove parameters to the data block and set them to their true values (from simulated data). Then return them one by one to parameters block. Which parameter introduces the problems?\nUsing a simulated dataset, introduce tight priors centered at true parameter values (known from the simulation). How tight need the priors to be to let the model fit? Useful for identifying multimodality.\nCheck your code. Problems are almost as likely a result of a programming error as they are a truly statistical issue. This is most relevant for exceptions and rejections.\n\nAre all the declared parameters used in the model?\nDo all parameters have informative priors?\nDo your array indices and for loops match?\nDo you have correct hard bounds (e.g. standard deviation parameters have &lt;lower=0&gt;, probabilities have &lt;lower=0, upper=1&gt;). Don’t use hard bounds to express soft prior knowledge as the effects to the shape of the posterior are not trivial.\nCheck for other errors/warnings in the Stan output (for example Location parameter is inf, but must be finite!) and investigate their sources, as they usually hint at coding errors. In rstan, those warnings might not get displayed unless you run with chains = 1.\n\nSpecifically for exceptions/rejections: Help the sampler avoid bounds of the parameter space and numerical inaccuracies.\n\nUse priors that place less weight on values near the boundary (if that’s consistent with domain expertise).\nUse numerically stable expressions. This includes always performing calculations on the log-scale when possible and making use of Stan’s special composed functions. For example, log_sum_exp(x) is more robust numerically than log(sum(exp(x)). Other examples like this include log1m(x) instead of log(1-x), log1p_exp(x) instead of log(1 + exp(x)), and many others. See the Composed Functions subsection of the the Stan functions reference for a complete listing.\nUse the print() function in your Stan model to display intermediate values in a computation to see where an invalid value originates.\n\nIntroduce more informative priors. If your posterior includes values that imply some clearly absurd predictions (e.g. 10m tall humans, almost zero between-subject differences in survival, …), removing those by a stricter prior might be useful.\n\nWhen working on the logarithmic scale (e.g. logistic/Poisson regression) even seemingly narrow priors like normal(0, 1); can be actually quite wide (this makes an odds ratio/multiplicative effect of exp(2) or roughly 7.4 still a-priori plausible - is that consistent with your domain expertise?).\nSome (slightly outdated) advice on priors is at https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\nHalf-normal or half-student distribution is usually preferable to half-cauchy for sd parameters in varying intercept/effect models. Forum discussion, Gelman 2006\nIf you have additional knowledge that would let you defensibly constrain your priors use it. Identity Crisis has some discussion of when this can help.\nSimulating data from the prior (a.k.a prior predictive check) is a good way to check if the priors and their interaction are roughly reasonable. Gabry et al. 2019 has an example.\n\nCompare your priors to the posterior distribution. If the model is sampling heavily in the very tails of your priors or on the boundaries of parameter constraints, this is a bad sign, indicating that your priors might be substantially influencing the sampling. Here, setting wider priors can sometimes help.\nReparametrize your model. An ideal posterior is close to independent normal distributions in each parameter as those result in constant curvature. Any deviations from this ideal — sharp corners, edges, cusps, banana shapes, tight correlations or other irregularities signal potential targets for reparametrization. In default setting, Stan will automatically rescale each parameter by a constant factor during adaptation, so the absolute scale of the posterior is usually not very important. The main part of reparametrization is to change the actual parameters and compute your parameters of interest in the transformed parameters block. Further reading:\n\nStan user’s guide chapter on reparametrization.\nCase study - diagnosing a multilevel model discusses non-centered parametrization which is frequently useful.\nThe case study on hierarchical models by Mike Betancourt goes into more detail on the non-centered parametrization, Betancourt & Girolami 2015 addresses the same topic.\nNon-centered parametrization for the exponential distribution\nStan users guide chapter on QR reparametrization for linear models\nIdentifying non-identifiability - a sigmoid model shows an example of where the parameters are not well informed by data, while https://discourse.mc-stan.org/t/difficulties-with-logistic-population-growth-model/8286/3 show a potential reparametrization.\nReparametrizing the Sigmoid Model of Gene Regulation shows problems and solutions in an ODE model.\n\nMultiple parametrizations of a sum-to-zero constraint.\n\nSpecifically for low BFMI warnings:\n\nLook at the pairs plot to see which primitive parameters are correlated with the energy__ margin.\nThe primitive parameters that are correlated with the energy__ margin in the pairs plot are a good place to start thinking about reparameterizations. There should be be a negative relationship between lp__ and energy__ in the pairs plot, but this is not a concern because lp__ is the logarithm of the posterior kernel rather than a primitive parameter.\n\nSpecifically for Rhat, ESS, low BFMI warnings: You might try setting a higher number of warmup or sampling iterations. Increasing the number of iterations is rarely helpful for resolving divergences/max treedepth warnings.\n\nLook at change in bulk-ESS and tail-ESS when the number of iterations increase. If R-hat is less than 1.01 and ESS grows linearly with the number of iterations and eventually exceeds the recommended limit, the mixing is sufficient but MCMC has high autocorrelation requiring a large number of iterations.\n\nRun the optimizing mode (penalized maximum likelihood) instead of sampling (NUTS) to check if the resulting values look at least roughly reasonable, if not try to find out why.\nRun Stan in diagnose (test_grad) mode to test the gradient computations. This can sometimes detect numerical instabilities in your model."
  },
  {
    "objectID": "learn-stan/diagnostics-warnings.html#i-got-no-warnings-when-fitting-the-same-model-in-jagswinbugs",
    "href": "learn-stan/diagnostics-warnings.html#i-got-no-warnings-when-fitting-the-same-model-in-jagswinbugs",
    "title": "How to Diagnose and Resolve Convergence Problems",
    "section": "I got no warnings when fitting the same model in JAGS/WinBUGS/…",
    "text": "I got no warnings when fitting the same model in JAGS/WinBUGS/…\nYou might not be used to seeing so many warnings from other software you use, but that does not mean that Stan has more problems than that other software. The Stan Development Team places a high priority on notifying users about any issue that could potentially be important and Stan will always tell you about problems it encounters instead of hiding them from you.\nA huge advantage of the algorithms used by Stan is that they permit certain unique diagnostics that are unavailable when using other algorithms. This can lead to more warnings from Stan, but this is a feature rather than a drawback. Sometimes when people reimplement a model from some other software in Stan, they get unexpected warnings, and further investigation reveals that the problem is not Stan being too sensitive, but that the original model in fact did not compute the correct posterior."
  },
  {
    "objectID": "learn-stan/diagnostics-warnings.html#getting-help",
    "href": "learn-stan/diagnostics-warnings.html#getting-help",
    "title": "How to Diagnose and Resolve Convergence Problems",
    "section": "Getting help",
    "text": "Getting help\nTe best place to get help from Stan developers and users if you have difficulties fitting a model is to visit the Stan Forums.\nIn order to both reduce the amount of help you need and allow us to give the best help when you do need it, it is recommended to: * Put Stan programs in a stand-alone file with a .stan extension. Even though some Stan interfaces allow specifying the model as a string, the line numbers in warning and error messages are only meaningful if you use a separate file. * Maintain reproducibility by saving the model and initial values in files and the RStan (other other Stan interface) commands in scripts. * Use version control such as git on your files and scripts so that you have a history of the changes you’ve made. * Start simple! Build your model in stages, and check for good fits at each stage, only adding complexity if there are no red flags. If you start by writing a complicated model it can be more difficult to figure out where things are going wrong."
  },
  {
    "objectID": "learn-stan/outdated.html",
    "href": "learn-stan/outdated.html",
    "title": "The resource you are looking for is out of date!",
    "section": "",
    "text": "You’ve been directed here because you tried to navigate to a url for a learning resource that no longer exists. As Stan evolves, some resources are removed from the website.\nYou can look for more recent replacements under the “Learning Resources” drop down above, or in the documentation.\nIf you really need to find the old resource, you can try looking in the GitHub repository history for this site.\n\n\n\n Back to top"
  },
  {
    "objectID": "learn-stan/stancon-talks.html",
    "href": "learn-stan/stancon-talks.html",
    "title": "StanCon Talks",
    "section": "",
    "text": "Location & Date: Oxford University, England – September 9-13, 2024\nProgram\nRecorded Talks: YouTube playlist\nTutorials (not recorded)\n\nIntroduction to Stan – Richard McElreath\nModel selection – Aki Vehtari, Noa Kallioinen, and Teemu Säilynoja\nBayesian hierarchical models in Stan – Sean Pinkney\nBayesian optmization with Stan – Anna Riha and Elizaveta Semenova\nBiodiversity modeling and forecasting – Will Pearse\nInfectious disease modeling in Stan – Juliette Unwin\n\nOrganizers: Charles Margossian, Will Pearse,"
  },
  {
    "objectID": "learn-stan/stancon-talks.html#stancon-2024-oxford",
    "href": "learn-stan/stancon-talks.html#stancon-2024-oxford",
    "title": "StanCon Talks",
    "section": "",
    "text": "Location & Date: Oxford University, England – September 9-13, 2024\nProgram\nRecorded Talks: YouTube playlist\nTutorials (not recorded)\n\nIntroduction to Stan – Richard McElreath\nModel selection – Aki Vehtari, Noa Kallioinen, and Teemu Säilynoja\nBayesian hierarchical models in Stan – Sean Pinkney\nBayesian optmization with Stan – Anna Riha and Elizaveta Semenova\nBiodiversity modeling and forecasting – Will Pearse\nInfectious disease modeling in Stan – Juliette Unwin\n\nOrganizers: Charles Margossian, Will Pearse,"
  },
  {
    "objectID": "learn-stan/stancon-talks.html#stancon-2023-st.-louis",
    "href": "learn-stan/stancon-talks.html#stancon-2023-st.-louis",
    "title": "StanCon Talks",
    "section": "StanCon 2023 St. Louis",
    "text": "StanCon 2023 St. Louis\n\nLocation & Date: Washington University, USA – June 20-23, 2023\nProgram\nGitHub repository of slides and course materials from StanCon 2023: https://github.com/stan-dev/stancon2023\nTutorials\n\nFundamentals of Stan – Charles Margossian\nBayesian workflow illustrated using BRMS – Mitzi Morris\nHierarchical models in Stan: varieties, optimizations & nuances – Mike Lawrence\nBuilding a GPT in Stan – Daniel Lee\nCognitive diagnostic models in R and Stan – Jake Thompson\nAdvances of model assessment, selection, and inference after model selection – Andrew Johnson\n\nOrganizers: Charles Margossian, Debashis Mondal, Yi Zhang, Eric Ward"
  },
  {
    "objectID": "learn-stan/stancon-talks.html#stancon-2020-global",
    "href": "learn-stan/stancon-talks.html#stancon-2020-global",
    "title": "StanCon Talks",
    "section": "StanCon 2020 Global",
    "text": "StanCon 2020 Global\n\nLocation & Date: Online, 24 hour event, August 13, 2020\nProgram\nRecorded StanCon 2020 Videos on YouTube\n\nSession 1\nSession 2\nSession 3\nAll videos\n\nOrganizers: Susana Marquez, Daniel Lee, Kelli Cassidy, Simon Maskell"
  },
  {
    "objectID": "learn-stan/stancon-talks.html#stancon-2019-cambridge",
    "href": "learn-stan/stancon-talks.html#stancon-2019-cambridge",
    "title": "StanCon Talks",
    "section": "StanCon 2019 Cambridge",
    "text": "StanCon 2019 Cambridge\n\nLocation & Date: University of Cambridge – August 20-23, 2019\nProgram\nRecorded Talks: YouTube playlist\nTutorials (not recorded)\n\nPopulation and ODE-based models using Stan and Torsten – Charles Margossian & Yi Zhang\nHierarchical Modeling with Stan – Ben Goodrich\nModel assessment and selection – Aki Vehtari\nA Dive into Stan’s C++ Model Concept – Daniel Lee\nIntroduction to Stan for Programmers – Jonathan Auerbach & Breck Baldwin\nBasics of Bayesian inference and Stan – Jonah Gabry & Lauren Kennedy\n\nOrganizers: University of Cambridge"
  },
  {
    "objectID": "learn-stan/stancon-talks.html#stancon-2018-helsinki",
    "href": "learn-stan/stancon-talks.html#stancon-2018-helsinki",
    "title": "StanCon Talks",
    "section": "StanCon 2018 Helsinki",
    "text": "StanCon 2018 Helsinki\n\nGitHub Repo for StanCon 2018 Helsinki\nRecorded Talks: YouTube playlist\nTutorials:\n\nBasics of Bayesian inference and Stan – Jonah Gabry & Lauren Kennedy\nHierarchical models – Ben Goodrich\nStan C++ development: adding a new function to Stan – Bob Carpenter, Sean Talts & Mitzi Morris\nOrdinary differential equation (ODE) models in Stan – Daniel Lee\nProductization of Stan – Eric Novik\nModel assessment and selection – Aki Vehtari\n\nLocation & Date: Aalto University, Helsinki, Finland – August 29-31, 2018\nOrganizers: Aalto University"
  },
  {
    "objectID": "learn-stan/stancon-talks.html#stancon-2018-asilomar",
    "href": "learn-stan/stancon-talks.html#stancon-2018-asilomar",
    "title": "StanCon Talks",
    "section": "StanCon 2018 Asilomar",
    "text": "StanCon 2018 Asilomar\n\nGitHub Repo for StanCon 2018 Asilomar\nRecorded Talks: YouTube playlist\nTutorials (not recorded)\n\nIntroduction to Stan – Jonah Sol Gabry, Mitzi Morris & Sean Talts\nExecutive decision making the Bayesian way – Jonathan Auerback & Eric Novik\nAdvanced Hierarchical Models in Stan – Ben Goodrich\nModel assessment, selection and inference after model selection – Aki Vehtari\nHow to develop for Stan at the C++ level – Charles Margossian\nA Dive into Stan’s C++ Model Concept – Daniel Lee\n\nLocation & Date: Asilomar Conference Center, Pacific Grove, California, USA – Jan 10-12, 2018\nOrganizers: Columbia University"
  },
  {
    "objectID": "learn-stan/stancon-talks.html#stancon-2017-new-york-city",
    "href": "learn-stan/stancon-talks.html#stancon-2017-new-york-city",
    "title": "StanCon Talks",
    "section": "StanCon 2017 New York City",
    "text": "StanCon 2017 New York City\n\nProgram\nRecorded Talks: YouTube playlist\nGithub Repo with Contributed Talks\nLocation & Date: Columbia University, New York, USA - Jan 17, 2017\nOrganizers: Columbia University"
  },
  {
    "objectID": "learn-stan/stancon/StanCon2019-program.html",
    "href": "learn-stan/stancon/StanCon2019-program.html",
    "title": "StanCon 2019 Schedule",
    "section": "",
    "text": "8:00am-9:00am Registration\n9:00am-11:30am Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: Stan for Programmers.\nTrack 3: Hierarchical Modeling with Stan.\n\n11:30am-12:30pm Open Developers Meeting (loo, projpred, bayesplot, discourse)\n12:30pm-2:00pm Provided Lunch\n2:00pm-4:30pm Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: A Dive into Stan’s C++ Model Concept.\nTrack 3: Population and ODE-based models using Stan and Torsten.\n\n4:30pm-5:30pm Open Developers Meeting (posteriordb = reference model and posterior database, bayesbenchr = framework for benchmarking inference algorithms)\n\n\n\n\n\n8:00am-9:00am Registration\n9:00am-11:30am Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: Stan for Programmers.\nTrack 3: Hierarchical Modeling with Stan.\n\n11:30am-12:30pm Open Developers Meeting, (bayesflow for Bayesian workflow, parallelization, optimization, KINSOL solver)\n12:30pm-2:00pm Provided Lunch\n2:00pm-4:30pm Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: Model assessment and selection.\nTrack 3: Population and ODE-based models using Stan and Torsten.\n\n4:30pm-5:30pm Open Developers Meeting, (sparse matrices, Laplace for GLVMs)\n\n\n\n\n\n8:00am-9:00am Registration\n9:00am-10:00am Submitted Talks\n\nPrior choice in logit models of discrete choice. Jim Savage.  Abstract  Video\nApproximate leave-future-out cross-validation for Bayesian time series models. Paul Bürkner, Jonah Gabry, Aki Vehtari.  Abstract  Video\nThe Currency of Place and the Short-Term Rental Market. Mikael Brunila.  Abstract  Video\n\n10:00am-10:40am Break\n10:40am-11:40am Submitted Talks\n\nModelling enzyme kinetics with Stan. Teddy Groves. DTU BIOSUSTAIN Quantitative Modelling of Cell Metabolism Team  Abstract  Video\nThe emergence of HIV resistance to antiretroviral therapy in southern Africa: a mechanistic meta-analysis of survey data. Julien Riou, Matthias Egger, Christian Althaus. Institute of Social and Preventive Medicine, University of Bern, Switzerland  Abstract  Video\nHandling missing data, censored values and measurement error in machine learning models using multiple imputation for early stage drug discovery. Rowan Swiers. AstraZeneca  Abstract  Video\n\n11:40am-12:00pm Sponsor Talks and Birds of Feather\n12:00pm-1:00pm Provided Lunch\n1:00pm-2:00pm Stan Community Meeting\n2:00pm-3:00pm Submitted Talks\n\nFast Forward Like a Lambo (skrrt skrrt). Daniel Lee. Generable  Abstract  Video\nProfit-Maximizing A/B Tests. Elea McDonnell Feit, Ron Berman. Drexel University, The Wharton School  Abstract  Video\nWhen seasonality meets Bayesian: Decomposing seasonalities in Stan. Hyunji Moon, SNU, Hyeonseop Lee, PUBG.  Abstract  Video\n\n3:00pm-3:40pm Break\n3:40pm-4:20pm Submitted Talks\n\nChronikis: a Bayesian time-series modeling language. Kevin S. Van Horn. Adobe Inc.  Abstract, Docker link  Video\nEstimating the prevalence of HIV infection in England using Bayesian evidence synthesis. Anne Presanis, Christopher Jackson (presenting author), Daniela De Angelis (MRC Biostatistics Unit, University of Cambridge); Peter Kirwan, Alison Brown, Ada Miltz, Ross Harris, Cuong Chau, Stephanie Migchelsen, Hamish Mohammed, Katy Davison, Sara Croxford, Sarika Desai, Kathy Lowndes, Valerie Delpech, Noel Gill (Public Health England).  Abstract  Video\n\n4:20pm-5:10pm David Spiegelhalter Communicating Uncertainty about Facts, Numbers and Science  Video\n5:10pm-6:30pm Networking at the Pub and pickup football (soccer) match\n6:30pm Dinner at King’s College\n\n\n\n\n\n8:00am-9:00am Registration\n9:00am-10:00am Submitted Talks\n\nExtending Stan’s Automatic Differentiation (AD) capabilities using dco/c++. Philip Maybank. Numerical Algorithms Group (NAG)  Abstract  Video\nThe State of GPU Computation Support for Stan. Rok Češnovar (University of Ljubljana - UL), Steve Bronder (Capital One), Davor Sluga (UL), Jure Demšar (UL), Tadej Ciglarič (UL), Sean Talts (Columbia University), Erik Štrumbelj (UL).  Abstract  Video\nModeling cocoa bean fermentation processes. Mauricio Moreno-Zambrano, Sergio Grimbs, Matthias S. Ullrich, and Marc-Thorsten Hütt. Department of Life Sciences & Chemistry, Jacobs University Bremen  Abstract  Video\n\n10:00am-10:40am Break\n10:40am-12:00pm Submitted Talks\n\nBayesian analyses of time-to-event data using the rstanarm R package. Eren M. Elçi, Sam Brilleman. Public Health and Preventive Medicine, Monash University  Abstract  Video\nA Decision-Theoretic Journey From Early Clinical Data to Late Stage Efficacy using Hierarchical Joint Models. Krzysztof Sakrejda, Eric Novik. Generable  Abstract  Video\nStacking for multimodal posterior distributions. Yuling Yao, Aki Vehtari, and Andrew Gelman.  Abstract  Video\nBayesian leave-one-out cross-validation for large data. Måns Magnusson (Aalto), Michael Riis Andersen (Danish Technical University), Johan Jonasson (Chalmers Technical University), Aki Vehtari (Aalto).  Abstract  Video\n\n12:00pm-1:00pm Provided Lunch\n1:00pm-2:00pm Open Developers Meeting, (stanc optimization)\n2:00pm-3:00pm Submitted Talks\n\nSimulation of Statistic Mechanical Systems using Stan. Forrest Eli Hurley. North Carolina State University  Abstract  Video\nStructured priors for survey estimates in the presence of non-representative data. Yuxiang Gao (University of Toronto), Lauren Kennedy (Columbia University), Daniel Simpson (University of Toronto).  Abstract  Video\nPrediction and causal inference for time-to-event outcomes truncated by death. Leah Comment.  Abstract  Video\n\n3:00pm-3:40pm Break\n3:40pm-4:00pm Submitted Talk\n\nGetting the Lead out–Does New York City’s childhood lead testing make statistical sense? Jonathan Auerbach, Breck Baldwin. Columbia University  Abstract  Video\n\n4:00pm-4:50pm Lauren Kennedy Out of Sample Prediction and the Quest for Generalization  Video"
  },
  {
    "objectID": "learn-stan/stancon/StanCon2019-program.html#tuesday-august-20-tutorials",
    "href": "learn-stan/stancon/StanCon2019-program.html#tuesday-august-20-tutorials",
    "title": "StanCon 2019 Schedule",
    "section": "",
    "text": "8:00am-9:00am Registration\n9:00am-11:30am Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: Stan for Programmers.\nTrack 3: Hierarchical Modeling with Stan.\n\n11:30am-12:30pm Open Developers Meeting (loo, projpred, bayesplot, discourse)\n12:30pm-2:00pm Provided Lunch\n2:00pm-4:30pm Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: A Dive into Stan’s C++ Model Concept.\nTrack 3: Population and ODE-based models using Stan and Torsten.\n\n4:30pm-5:30pm Open Developers Meeting (posteriordb = reference model and posterior database, bayesbenchr = framework for benchmarking inference algorithms)"
  },
  {
    "objectID": "learn-stan/stancon/StanCon2019-program.html#wednesday-august-21-tutorials",
    "href": "learn-stan/stancon/StanCon2019-program.html#wednesday-august-21-tutorials",
    "title": "StanCon 2019 Schedule",
    "section": "",
    "text": "8:00am-9:00am Registration\n9:00am-11:30am Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: Stan for Programmers.\nTrack 3: Hierarchical Modeling with Stan.\n\n11:30am-12:30pm Open Developers Meeting, (bayesflow for Bayesian workflow, parallelization, optimization, KINSOL solver)\n12:30pm-2:00pm Provided Lunch\n2:00pm-4:30pm Tutorials with Break\n\nTrack 1: Basics of Bayesian inference and Stan.\nTrack 2: Model assessment and selection.\nTrack 3: Population and ODE-based models using Stan and Torsten.\n\n4:30pm-5:30pm Open Developers Meeting, (sparse matrices, Laplace for GLVMs)"
  },
  {
    "objectID": "learn-stan/stancon/StanCon2019-program.html#thursday-august-22-conference",
    "href": "learn-stan/stancon/StanCon2019-program.html#thursday-august-22-conference",
    "title": "StanCon 2019 Schedule",
    "section": "",
    "text": "8:00am-9:00am Registration\n9:00am-10:00am Submitted Talks\n\nPrior choice in logit models of discrete choice. Jim Savage.  Abstract  Video\nApproximate leave-future-out cross-validation for Bayesian time series models. Paul Bürkner, Jonah Gabry, Aki Vehtari.  Abstract  Video\nThe Currency of Place and the Short-Term Rental Market. Mikael Brunila.  Abstract  Video\n\n10:00am-10:40am Break\n10:40am-11:40am Submitted Talks\n\nModelling enzyme kinetics with Stan. Teddy Groves. DTU BIOSUSTAIN Quantitative Modelling of Cell Metabolism Team  Abstract  Video\nThe emergence of HIV resistance to antiretroviral therapy in southern Africa: a mechanistic meta-analysis of survey data. Julien Riou, Matthias Egger, Christian Althaus. Institute of Social and Preventive Medicine, University of Bern, Switzerland  Abstract  Video\nHandling missing data, censored values and measurement error in machine learning models using multiple imputation for early stage drug discovery. Rowan Swiers. AstraZeneca  Abstract  Video\n\n11:40am-12:00pm Sponsor Talks and Birds of Feather\n12:00pm-1:00pm Provided Lunch\n1:00pm-2:00pm Stan Community Meeting\n2:00pm-3:00pm Submitted Talks\n\nFast Forward Like a Lambo (skrrt skrrt). Daniel Lee. Generable  Abstract  Video\nProfit-Maximizing A/B Tests. Elea McDonnell Feit, Ron Berman. Drexel University, The Wharton School  Abstract  Video\nWhen seasonality meets Bayesian: Decomposing seasonalities in Stan. Hyunji Moon, SNU, Hyeonseop Lee, PUBG.  Abstract  Video\n\n3:00pm-3:40pm Break\n3:40pm-4:20pm Submitted Talks\n\nChronikis: a Bayesian time-series modeling language. Kevin S. Van Horn. Adobe Inc.  Abstract, Docker link  Video\nEstimating the prevalence of HIV infection in England using Bayesian evidence synthesis. Anne Presanis, Christopher Jackson (presenting author), Daniela De Angelis (MRC Biostatistics Unit, University of Cambridge); Peter Kirwan, Alison Brown, Ada Miltz, Ross Harris, Cuong Chau, Stephanie Migchelsen, Hamish Mohammed, Katy Davison, Sara Croxford, Sarika Desai, Kathy Lowndes, Valerie Delpech, Noel Gill (Public Health England).  Abstract  Video\n\n4:20pm-5:10pm David Spiegelhalter Communicating Uncertainty about Facts, Numbers and Science  Video\n5:10pm-6:30pm Networking at the Pub and pickup football (soccer) match\n6:30pm Dinner at King’s College"
  },
  {
    "objectID": "learn-stan/stancon/StanCon2019-program.html#friday-august-23",
    "href": "learn-stan/stancon/StanCon2019-program.html#friday-august-23",
    "title": "StanCon 2019 Schedule",
    "section": "",
    "text": "8:00am-9:00am Registration\n9:00am-10:00am Submitted Talks\n\nExtending Stan’s Automatic Differentiation (AD) capabilities using dco/c++. Philip Maybank. Numerical Algorithms Group (NAG)  Abstract  Video\nThe State of GPU Computation Support for Stan. Rok Češnovar (University of Ljubljana - UL), Steve Bronder (Capital One), Davor Sluga (UL), Jure Demšar (UL), Tadej Ciglarič (UL), Sean Talts (Columbia University), Erik Štrumbelj (UL).  Abstract  Video\nModeling cocoa bean fermentation processes. Mauricio Moreno-Zambrano, Sergio Grimbs, Matthias S. Ullrich, and Marc-Thorsten Hütt. Department of Life Sciences & Chemistry, Jacobs University Bremen  Abstract  Video\n\n10:00am-10:40am Break\n10:40am-12:00pm Submitted Talks\n\nBayesian analyses of time-to-event data using the rstanarm R package. Eren M. Elçi, Sam Brilleman. Public Health and Preventive Medicine, Monash University  Abstract  Video\nA Decision-Theoretic Journey From Early Clinical Data to Late Stage Efficacy using Hierarchical Joint Models. Krzysztof Sakrejda, Eric Novik. Generable  Abstract  Video\nStacking for multimodal posterior distributions. Yuling Yao, Aki Vehtari, and Andrew Gelman.  Abstract  Video\nBayesian leave-one-out cross-validation for large data. Måns Magnusson (Aalto), Michael Riis Andersen (Danish Technical University), Johan Jonasson (Chalmers Technical University), Aki Vehtari (Aalto).  Abstract  Video\n\n12:00pm-1:00pm Provided Lunch\n1:00pm-2:00pm Open Developers Meeting, (stanc optimization)\n2:00pm-3:00pm Submitted Talks\n\nSimulation of Statistic Mechanical Systems using Stan. Forrest Eli Hurley. North Carolina State University  Abstract  Video\nStructured priors for survey estimates in the presence of non-representative data. Yuxiang Gao (University of Toronto), Lauren Kennedy (Columbia University), Daniel Simpson (University of Toronto).  Abstract  Video\nPrediction and causal inference for time-to-event outcomes truncated by death. Leah Comment.  Abstract  Video\n\n3:00pm-3:40pm Break\n3:40pm-4:00pm Submitted Talk\n\nGetting the Lead out–Does New York City’s childhood lead testing make statistical sense? Jonathan Auerbach, Breck Baldwin. Columbia University  Abstract  Video\n\n4:00pm-4:50pm Lauren Kennedy Out of Sample Prediction and the Quest for Generalization  Video"
  },
  {
    "objectID": "learn-stan/stancon/stancon2019-abstracts.html",
    "href": "learn-stan/stancon/stancon2019-abstracts.html",
    "title": "StanCon 2019 Programme",
    "section": "",
    "text": "Pharmaceuticals/Medicine\n\nComputing prediction and tolerance intervals for a mixture of normal distributions. Jean-francois Michiels, Timothy Mutsvari, Oussama Errazi. Pharmalex. Abstract\nParallel numerical ODE solution in Torsten for population models. Yi Zhang, William R. Gillespie. Metrum LLC Abstract\nMulti-channel Gaussian Processes as flexible alternatives to linear models: perspectives and challenges to scaling up Bayesian inference to genomic-scale data. Caetano Souto-Maior, Susan T. Harbison. Laboratory of Systems Genetics, National Heart Lung and Blood Institute, NIH. Abstract\nEstimating the prevalence of HIV infection in England using Bayesian evidence synthesis. Anne Presanis, Christopher Jackson [presenting author], Daniela De Angelis (MRC Biostatistics Unit, University of Cambridge); Peter Kirwan, Alison Brown, Ada Miltz, Ross Harris, Cuong Chau, Stephanie Migchelsen, Hamish Mohammed, Katy Davison, Sara Croxford, Sarika Desai, Kathy Lowndes, Valerie Delpech, Noel Gill (Public Health England).  Abstract Video\nA Decision-Theoretic Journey From Early Clinical Data to Late Stage Efficacy using Hierarchical Joint Models.. Krzysztof Sakrejda, Eric Novik.  Generable  Abstract Video\nBayesian analyses of time-to-event data using the rstanarm R package. Eren M. Elçi, Sam Brilleman. Public Health and Preventive Medicine, Monash University. Abstract Video\nModelling enzyme kinetics with Stan. Teddy Groves. DTU BIOSUSTAIN Quantitative Modelling of Cell Metabolism Team Abstract Video\nThe emergence of HIV resistance to antiretroviral therapy in southern Africa: a mechanistic meta-analysis of survey data. Julien Riou, Matthias Egger, Christian Althaus. Institute of Social and Preventive Medicine, University of Bern, Switzerland Abstract Video\nHandling missing data, censored values and measurement error in machine learning models using multiple imputation for early stage drug discovery. Rowan Swiers. AstraZeneca Abstract Video\nA Bayesian multi-layered model to predict mechanisms, types, and severity of drug-induced liver injury. Elizaveta Semenova, Dominic Williams, Stanley E Lazic. Data Science and Quantitative Biology group, AstraZeneca, Cambridge UK Abstract\n\nModeling\n\nGaussian process modeling and covariate selection for longitudinal data. Juho Timonen, Aki Vehtari, Harri Lähdesmäki. Aalto University Abstract\nEstimating the effect of age and league on scoring rate in professional soccer. Benjamin Torvaney. Wefarm Abstract\nHierarchical models for gamma-ray burst populations. J. Michael Burgess.  MPE  Abstract\nModeling cocoa bean fermentation processes. Mauricio Moreno-Zambrano, Sergio Grimbs, Matthias S. Ullrich and Marc-Thorsten Hütt.  Department of Life Sciences & Chemistry, Jacobs University Bremen Abstract Video\nApproximate leave-future-out cross-validation for Bayesian time series models. Paul Bürkner, Jonah Gabry, Aki Vehtari.  Abstract Video\nWhen seasonality meets Bayesian: Decomposing seasonalities in Stan. Hyunji Moon, SNU, Hyeonseop Lee, PUBG.  Abstract Video\nPrediction and causal inference for time-to-event outcomes truncated by death. . Leah Comment.  Abstract Video\nFast Forward Like a Lambo (skrrt skrrt). Daniel Lee. Generable Abstract Video\nThe Currency of Place and the Short-Term Rental Market. Mikael Brunila.  Abstract Video\nProfit-Maximizing A/B Tests. Elea McDonnell Feit, Ron Berman. Drexel Univeristy, The Wharton School Abstract Video\nStructured priors for survey estimates in the presence of non-representative data. Yuxiang Gao (University of Toronto), Lauren Kennedy (Columbia University), Daniel Simpson (University of Toronto).  Abstract Video\nChronikis: a Bayesian time-series modeling language. Kevin S. Van Horn. Adobe Inc. Abstract Video\nA long-short term event memory state-space model for multi-party elections. Marcus Groß. INWT Statistics GmbH Abstract\nSimulation of Statistic Mechanical Systems using Stan. Forrest Eli Hurley. North Carolina State University Abstract Video\nRegularized Hierarchical Models for Remotely Sensed Forest Inventories. Nathan E. Rutenbeck SilviaTerra Abstract \nGetting the Lead out–Does New York City’s childhood lead testing make statistical sense?. Jonathan Auerbach, Breck Baldwin. Columbia Univeristy Abstract Video \n\nInference\n\nMaking Stan Faster using Sequential Monte Carlo samplers. Simon Maskell (University of Liverpool), Alessandro Varsi (University of Liverpool), Peter Green (University of Liverpool), Paul Horridge (University of Liverpool), Alejandro Diaz (University of Liverpool), Lee Devlin (University of Liverpool), Rob Moore (University of Liverpool), Katerina Chatzopoulou (University of Liverpool), Jinglai Li (University of Liverpool), Maria Sudell (University of Liverpool), Luke Mason (STFC), Robin Pinning (STFC), Jack Taylor (STFC), Vassil Alexandrov (STFC), Ed Pyzer-Knapp (IBM) .  Abstract\nOne weird trick: Non-parametric Bayesian updating by kernels. Robert Grant. BayesCamp Abstract\nSemiparametric Modeling of the Mean,Variance and Scale Parameters in Skew Normal Regression Models: A Bayesian Perspective. Héctor Zarate.  Abstract\nPrior choice in logit models of discrete choice. Jim Savage. Schmidt Futures Abstract Video\nStacking for multimodal posterior distributions. Yuling Yao, Aki Vehtari and Andrew Gelman.  Abstract Video\nBayesian leave-one-out cross-validation for large data. Måns Magnusson, Aalto, Michael Riis Andersen, Danish Technical University, Johan Jonasson, Chalmers Technical University, Aki Vehtari, Aalto.  Abstract Video\n\nCore Stan\n\nThe State of GPU Computation Support for Stan. Rok Češnovar (University of Ljubljana - UL), Steve Bronder (Capital One), Davor Sluga (UL), Jure Demšar (UL), Tadej Ciglarič (UL), Sean Talts (Columbia University), Erik Štrumbelj (UL).  Abstract Video\nExtending Stan’s Automatic Differentiation (AD) capabilities using dco/c++. Philip Maybank. Numerical Algorithms Group (NAG) Abstract Video\n\n\n\n\n\n Regularized Hierarchical Models for Remotely Sensed Forest Inventories. Nathan E. Rutenbeck SilviaTerra \nManagement and conservation of the world’s forests is critical for maintaining global timber supply, as well as for the ecosystem services forestlands provide. Forest biometrics remains a field focused on traditional methods in sampling and regression, despite the fact that these methods are ill equipped to utilize the profusion of remote sensing data now available. When remote sensing data is used, it is often deployed within simple population-level regression models that simultaneously leave out information regarding known forest structure and sample design, and are prone to overfitting of effects at the population level. Using Stan, we show that for the prediction of forest basal area (a key inventory attribute) incorporating known structural attributes (forest stands) and sample design information into a hierarchical modeling framework along with remote sensing data can yield beneficial results in terms of reducing overfitting and improving predictive performance when compared to more conventional methods. We fit and compared four candidate models, examining their performance with respect to one another and to the conventional frequentist inferences that are so widely used for operational forest inventory. The four models we examined are 1) a hierarchical model incorporating forest stand and sample design effects; 2) a population-level remote sensing principal components model; 3) the hierarchical model with the addition of remote sensing principal component effects at the population level; and 4) the hierarchical and remote sensing model with the addition of regularizing horseshoe priors on remote sensing effects. The hierarchical model without remote sensing effects showed the expected shrinkage of stand-level mean basal area predictions toward the global mean. The addition of remote sensing effects showed overall reductions in prediction error in comparison to the sample design model. Incorporating regularizing priors on the remote sensing principal components effects retained signal from the remote sensing data but showed further shrinkage of predictions of stand-level mean basal area toward sample means. Our results suggest that the penalized hierarchical model can be used in developing operational forest inventories that balance information from known forest structural heterogeneity, the sample design, and remote sensing data.\n A Bayesian multi-layered model to predict mechanisms, types, and severity of drug-induced liver injury. Elizaveta Semenova, Dominic Williams, Stanley E Lazic. Data Science and Quantitative Biology group, AstraZeneca, Cambridge UK \nAbstract: Drug-induced liver injury (DILI) is a major cause of attrition in drug development and a common reason for withdrawing a drug from the market. Predicting clinical liver toxicity is difficult, but preclinical in vitro assays and physical/chemical properties of drugs can be used as predictors. We developed a multi-layered Bayesian model where we use assay results to predict the mechanism(s) of toxicity, use the mechanisms to predict the type of liver injury, and then combine the type of injury with the clinical dose of a drug to predict the severity of injury. The model therefore has a layered structure, enabling uncertainty to propagate through the layers. Based only on assay and physchem data, along with the clinical dose, the model enables safety pharmacologists to predict the severity, type, and mechanism of liver toxicity with good accuracy.\n\n \n Getting the Lead out–Does New York City’s childhood lead testing make statistical sense?. Jonathan Auerbach, Breck Baldwin. Columbia Univeristy .  Video\nAbstract: The US has dramatically reduced blood lead levels in children over the past 30 years and that effort continues. New York City (NYC) was an early adopter of lead reduction policies and that effort continues with laws that require all children be tested and with mandatory interventions for those tested blood levels (tbll) greater than 5mg/dL. But there is a statistically interesting story around how current blood level limits are set, the performance of common tests and how to apply common Bayes rule reasoning to publicly available data.\nThe data we have: We have high quality blood lead level (bll) tests applied nation wide (NHANES) for 5,000 children, we have NYC supplied data that provides counts for all children’s tested blood lead level, the number greater than 5mg/dL, 10mg/dL and 15/dL and claims of blood tests that widely vary from sources like FDA applications for blood testing equipment, actual studies of test performance and government testing standards.\nThe data we want: New York city recently dropped the threshold for intervention from 10mg/dL to 5mg/dL. It is an open question what the false positive rate is for these test thresholds with some research suggesting that it is as high as 70%. On the other extreme is an FDA applications for the LeadCare Plus testing device claim a standard deviation of .5 at the 5mg/dL which suggests a very low false positive rate…but that depends on the distribution of actual blls in the NYC population.\nHow we got the data we wanted: This is a simple application of Bayes rule: p(bll &gt; 5|t &gt;5) = p(tbll &gt; 5| bll&gt;5) p(bll&gt;5)/p(tbll&gt;5) where we don’t know p(bll&gt;5) for NYC. NYC refused to release non-quantized data for tbll under FIOA requests, which if we had, would allow a fairly straightforward determination of false positive rates from tbll test evaluations. But we do have data for the US as a whole in non-quantized form.\nThe paper describes a process of model refinement staring with naive approaches and incrementally modifying our models to better suite NYC data. The final approach, subject to change as we do more work, is to fit national NHANES data with an exponential distribution, assume that similar distributions apply to NYC and recover a believable false positive rate across a range of reported blood test performance. Along the way we show an interesting simple use of the ‘integrate_ode_rk45’ function in Stan and demonstrate Bayesian workflow.\n\n Simulation of Statistic Mechanical Systems using Stan. Forrest Eli Hurley. North Carolina State University  Video\nAbstract: Bayesian statistics is closely coupled with physics. The metropolis algorithm (1953) was developed by scientists working at Los Alamos as a method for thermodynamic simulation of molecular dynamics. Not until the work of W. K. Hastings (1970) was the method generalized to arbitrary probability distributions. Hamiltonian Monte Carlo is even more deeply rooted in physics than the Metropolis-Hastings algorithm. The simulation of states with velocities, energies, and a Hamiltonian describes nothing other than a physical system. It matches a canonical ensemble in that there is not a fixed energy between steps, only an overall fixed temperature. The temperature is usually only implicit, but some tempering methods simulate chains at higher temperatures to smooth the probability distributions. The Ising Model, a proxy for magnetization, is a prevalent introductory model in the study of statistical mechanics. It consists of an N-dimensional grid of spin up or down particles. The energy varies depending on the alignment of spins between nearest neighbors. At low temperatures spins tend to align on a macroscopic scale; at high temperatures they become evenly distributed. We simulate the XY Model, similar to the Ising Model but allowing spins to be represented by unit vectors in two dimensions, using Stan. We create chains at several temperatures to identify the locations of phase transitions in macroscopic properties. Our work shows the applicability of Stan for computation in continuous statistical mechanical problems.\n\n A long-short term event memory state-space model for multi-party elections. Marcus Groß. INWT Statistics GmbH \nAbstract: State-space models are a popular choice in modelling voting intentions and election results by using poll data. The presented multivariate state-space model attempts to go beyond random-walk or Kalman-filter approaches (with comparable performance to simple weighted survey averages) to the problem by introducing a long-short term event memory effect. This effect serves as reasonable explanation to the observation that the voter’s share partially tends to reverse to the party’s long-term trend after larger short term movements. Any event influencing the voter’s share of a party is presumed to have a convex shaped effect decomposable into a short term effect due to e.g. media spreading and a smaller long term effect remaining despite overlay effects of new events and forgetting. This effect is modelled by a mixture of a random walk and two contrasting autoregressive processes. By also taking advantage of the widely observed effect that government parties tend to fall in voter’s share, whereas the opposite effect is observed for opposition parties, mid- and long-term predictions of election outcomes can be considerably be improved. The Stan-model is fitted and evaluated on poll data from seven pollsters for the German national elections (“Bundestagswahl”) from 1994 to 2017, where low double digits (out-of-sample) improvements in prediction performance can be seen between 3- and 18-months prior elections. By taking into account the pollsters house effects, their poll errors and even more importantly their correlations in poll errors, an appropriate and realistic estimation error can be propagated.\n\n Bayesian leave-one-out cross-validation for large data. Måns Magnusson, Aalto, Michael Riis Andersen, Danish Technical University, Johan Jonasson, Chalmers Technical University, Aki Vehtari, Aalto.  Video\nAbstract: Model inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.\n\n Stacking for multimodal posterior distributions. Yuling Yao, Aki Vehtari and Andrew Gelman.  Video \nAbstract: When working with multimodal posterior distributions, MCMC algorithms can have difficulty moving between modes, and default variational or mode-based approximate inferences can understate posterior uncertainty. And, even if the most important modes can be found, it is difficult to evaluate their relative weights in the posterior, which requires computing the integral of the posterior in the neighborhood of each mode. Here we propose an alternative approach, using parallel runs of MCMC, variational, or mode- based inferences to hit as many modes as possible, and then using Bayesian stacking to weight the set of simulations at each mode. Bayesian stacking is a method for constructing a weighted average of distributions so as to minimize cross-validated prediction errors. The result from stacking is not necessarily equivalent, even asymptotically, to fully Bayesian inference, but it serves many of the same goals. We discuss in the context of several theoretical and applied examples.\n\n Chronikis: a Bayesian time-series modeling language. Kevin S. Van Horn. Adobe Inc.  Video\nAbstract: Chronikis (http://chronikis.org) is an open-source language for Bayesian time-series models that compiles to Stan and R. It currently focuses on linear state-space models, with plans to incrementally expand the class of supported models over time. The goal for Chronikis is to allow one to quickly and reliably create and apply a variety of models to a time series, doing a full Bayesian analysis on each.\nThus the Chronikis language itself focuses on concise and clear model specification, and as far as possible the task of creating efficient estimation and forecasting code is left to the compiler. These twin goals are facilitated by making the Chronikis language fully declarative: the body of a Chronikis program is just an expression whose ““value”” is a probability distribution over time series.\nThe compiler applies a series of semantics-preserving transformations to the body of a Chronikis program, eventually arriving at a form that it can straightforwardly translate to Stan. Along the way it infers types and shapes for all variables except the parameters of main(), reparameterizes in some cases to use non-centered parameterization, assigns each variable to the appropriate Stan block, and infers bounds for variables assigned to the parameters block.\nFor the sake of clarity, Chronikis supports operations for constructing complex models from simpler components. For example, here is a Chronikis program for a random-walk model with observation noise:\ndef main(s_rw, s_obs: real{0.0,}, mu0: real, sigma0: real{0.0,}) = sigma_rw ~ half_cauchy(s_rw); sigma_obs ~ half_cauchy(s_obs);\naccum(wn(sigma_rw)) + constp(mu0, sigma0) + wn(sigma_obs)\nNotes on the above:\n\nThe main() parameters s_rw, s_obs, mu0, and sigma0 are prior parameters.\nsigma_rw^2 and sigma_obs^2 are the random-walk and observation-error variances.\nwn(s) is a white noise process with variance s^2.\nconstp(m,s) is a distribution over constant time series, with a Normal(m,s) distribution for the constant value.\naccum is an operator on time-series distributions; accum(D) is a time-series distribution whose draws are cumulative sums of a time series drawn from D.\nSum (+) is another operator on time-series distributions; D1 + D2 is a time-series distribution whose draws are the element-wise sum of independent draws from D1 and D2.\n\nChronikis also has some innovative support for (quasi-)periodic time-series model components. The period can be arbitrarily large, and need not even be an integer. One can allow the periodic pattern to slowly change over time. There is a smoothness parameter, and this bounds the size of the latent state required, regardless of how large the period may be. Chronikis accomplishes all this by constructing a linear state-space model that approximates the zero-mean Gaussian process defined by variant of MacKay’s periodic kernel, modified to ensure that the realizations of the process are themselves zero-centered.\n\n Structured priors for survey estimates in the presence of non-representative data. Yuxiang Gao (University of Toronto), Lauren Kennedy (Columbia University), Daniel Simpson (University of Toronto).  Video \nAbstract:A central theme in the field of survey statistics is estimating population-level quantities through data coming from potentially non-representative samples of the population. Multilevel Regression and Poststratification (MRP), a model-based approach, is gaining traction against the traditional weighted approach for survey estimates. MRP uses partial pooling through random effects, thus shrinking model estimates to an overall mean and reducing potential overfitting. Despite MRP’s straightforward specification of prior distributions, the estimates coming from it are susceptible to bias if there is an underlying structure that the prior does not capture. This work aims to provide a new framework for specifying structured prior distributions that lead to bias reduction in MRP estimates. We use simulation studies to explore the benefit of these priors and demonstrate on US survey data.\n\n Profit-Maximizing A/B Tests. Elea McDonnell Feit, Ron Berman. Drexel University, The Wharton School  Video\nAbstract: Marketers often use A/B testing as a tactical tool to compare marketing treatments in a test stage and then deploy the better-performing treatment to the remainder of the consumer population. While these tests have traditionally been analyzed using hypothesis testing, we re-frame such tactical tests as a Bayesian decision problem with an explicit trade-off between the opportunity cost of the test (where some customers receive a sub-optimal treatment) and the potential losses associated with deploying a sub-optimal treatment to the remainder of the population.\nWe derive a closed-form expression for the profit-maximizing test size and show that it is substantially smaller than that typically recommended for a hypothesis test, particularly when the response is noisy or when the total population is small. The common practice of using small holdout groups for media testing can be rationalized by asymmetric priors. The proposed test design achieves nearly the same expected regret as the flexible, yet harder-to-implement multi-armed bandit.\nAdopting a Bayesian approach to experimental design requires informative priors. We show how priors can be estimated from data on past A/B test, using Stan to fit a hierarchical meta model. An R notebook will be provided which shows the complete process from meta-analysis of past experiments to determining the profit-maximizing sample size for the new A/B test.\nA full paper is available at https://arxiv.org/abs/1811.00457.\n\n The Currency of Place and the Short-Term Rental Market. Mikael Brunila.  Video \nAbstract: Airbnb and short-term rentals are raising rents in cities through the use of new technologies and by catering to culturally savvy populations. As a phenomenon of the attention economy, Airbnb is a platform where meaning becomes priced, as efficient and attractive communication is awarded by more bookings. In this paper, we look at how this capitalization of meaning can be understood by modelling the use of neighbourhood names. Using Natural Language Processing techniques and Bayesian hierarchical logit models with Intrinsic Auto-Regressive priors, we explore how listings draw upon the value placed on well-known neighbourhoods to promote themselves. Our findings separate different spatial effects as well as neighbourhood and listing level patterns that help us explain how neighbourhood names are deployed to promote short-term rentals on Airbnb.\n\n Fast Forward Like a Lambo (skrrt skrrt). Daniel Lee. Generable Video\nAbstract: Exploring simple, automatic within-chain parallelization. For any (well-behaved) statistical model written in the Stan language, the Stan Math library (Math) provides the gradient of the log joint probability distribution function specified. It currently provides the gradient with reverse-mode automatic differentiation. Math also provides forward-mode automatic differentiation, which isn’t as well tested as reverse-mode, but is available none-the less. Reverse-mode automatic differentiation scales well as it can tackle an arbitrary number of parameters with one sweep. However, this can’t be parallelized easily. Forward-mode requires N sweeps to evaluate N directional derivatives, but each of these sweeps can be done in parallel. With the adoption of C++14 capable compilers, we’re now able to use threading as an easy paradigm to coordinate within-chain parallelization. We’ll show some of the performance considerations and some preliminary results.\n\n Prediction and causal inference for time-to-event outcomes truncated by death. Leah Comment.  Video\nAbstract: Predicting customer behaviour is crucial for making decisions such as the cost of acquisition or planning for production or service capacity. In the model being presented individual purchase data of a fashion retailer is utilized to describe and predict their behaviour using Bayeasian multi-layered architecture to allow for heterogeneity and latent variables, such as customer state of activity.\n\n–&gt;\n When seasonality meets Bayesian: Decomposing seasonalities in Stan. Hyunji Moon, SNU, Hyeonseop Lee, PUBG.   Video\nAbstract: Multiple seasonalities play a key role in time series forecasting, especially for business time series where seasonal effects are often dramatic. Previous approaches including Fourier decomposition, exponential smoothing, and Seasonal ARIMA do not reflect distinct characteristics of each period in seasonal patterns such as unique behavior of specific day of the week in business data. We propose a multi-dimensional hierarchical model. Intermediate parameters for each seasonal period are first estimated, then mixture of intermediate parameters are then taken, resulting in the model which successfully reflects interactions between multiple seasonalities. Although this process leads to the reduction of data available for each parameter, a robust estimation can be obtained through a hierarchical Bayesian model. Consideration of not only the characteristics of each seasonal periods but also the interactions between characteristics from multiple seasonalities becomes possible through this model. Our new model is implemented in Stan and considerable improvements in prediction accuracy compared to previous models are achieved. Previous models include Fourier decomposition which Prophet uses to model seasonalities. Comparison has been performed on real-world dataset from a nation-scale logistic network.\n\n Approximate leave-future-out cross-validation for Bayesian time series models. Paul Bürkner, Jonah Gabry, Aki Vehtari.  Video\nAbstract: One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model’s future predictive accuracy, for instance, for the purpose of model comparison or selection. As exact cross-validation for Bayesian models is often computationally expensive, approximate cross-validation methods have been developed; most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate as the information from future observations is available to influence predictions of the past. To tackle the prediction task properly and account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation. We provide examples using Bayesian time-series models fitted with Stan.\n\n Handling missing data, censored values and measurement error in machine learning models using multiple imputation for early stage drug discovery. Rowan Swiers. AstraZeneca  Video\nAbstract: Multiple imputation is a technique for handling missing data, censored values and measurement error. Currently it is underused in the machine learning field due to lack of familiarity and experience with the technique, whilst other missing data solutions such as full Bayesian models can be hard to set up. However, randomization-based evaluations of Bayesianly derived repeated imputations can provide approximately valid inference of the posterior distributions and allow use of techniques which rely upon complete data such as SVMs and random Forest models.\nThis paper, using simulated data sets inspired by AstraZeneca drug data, shows how multiple imputation techniques can improve the analysis of data with missing values or with uncertainty. We pay close attention to the prediction of Bayesian posterior coverage due its importance in industrial applications. Comparisons are made to other commonly used methods of handling missing data such as single uniform imputation and data removal. Furthermore, we review several standard multiple imputation models and compare them on our simulated data sets. We provide recommendations on when to use each technique and where extra care is needed based upon data distributions. Finally, using simulated data, we give examples of how correct use of multiple imputation can affect investment decisions in the early stages of drug discovery.\nAnalysis was performed using both Python and Stan and is provided in a Jupyter notebook.\n\n The emergence of HIV resistance to antiretroviral therapy in southern Africa: a mechanistic meta-analysis of survey data. Julien Riou, Matthias Egger, Christian Althaus. Institute of Social and Preventive Medicine, University of Bern, Switzerland Video\nAbstract: Large-scale campaigns providing access to antiretroviral therapy (ART) to people living with HIV in southern Africa have been ongoing since the early 2000s. The success of these campaigns is now threatened by the emergence of HIV drug resistance, most of all resistance to non-nucleoside reverse-transcriptase inhibitors (NNRTI), a class of drugs constitutive of ART. Systematic reviews of cross-sectional surveys have provided insights into the temporal trends of NNRTI resistance among HIV-infected individuals. However, these simple temporal trends fail to account for the local dynamics of HIV transmission and treatment that create the evolutionary pressure generating resistance mutations. Such approaches limit our general understanding of the phenomena of resistance emergence in response to drug introduction and disallow any between-country comparison. Here, we propose a mechanistic approach linking the observed levels of NNRTI resistance to the underlying dynamics of HIV in each country.\nWe developed a SIR-like model consisting of a hierarchical system of ordinary differential equations in Stan. The model considered the infection of susceptible individuals with HIV, the treatment of diagnosed individuals with ART from the early 2000s, the occurrence of resistance mutations in response to the evolutionary pressure created by ART, and the transmission of mutant, resistant viruses to susceptible individuals. The model was fitted jointly to country-level data regarding different aspects of the HIV epidemic (prevalence of HIV, number of people under ART and population size in 8 countries of southern Africa from 2000 to 2016) and to measurements of NNRTI resistance in cross-sectional surveys (60 surveys from 2000 to 2016). Partial pooling was allowed by introducing a hierarchical structure by country on the parameters governing the occurrence of resistance, as well as a hierarchical structure by survey on resistance data.\nThe model could adequately reproduce the dynamics of the HIV epidemics in each country. We found substantial heterogeneity between the rates of emergence of NNRTI resistance across countries that is not explained by differences in the local dynamics of HIV transmission and treatment. Understanding the factors associated with this heterogeneity will allow public health authorities to anticipate on potential issues of drug resistance emergence related to local characteristics.\n\n Modelling enzyme kinetics with Stan.Teddy Groves. DTU BIOSUSTAIN Quantitative Modelling of Cell Metabolism Team Video\nAbstract: The advent of high-throughput technologies has transformed molecular biology into a data-rich discipline. However, integrating this data into a predictive modeling framework is not trivial because many different sources of uncertainty about the molecular processes governing cell metabolism must be taken into account. In particular, reaction fluxes and steady-state reactant concentrations can at best be measured noisily, and even for the best-understood organisms pre-experimental knowledge of kinetic parameters is incomplete and imprecise.\nWe are using Stan to overcome the existing limitations in the study of cell metabolism by combining pre-experimental knowledge about kinetic parameters with experimental measurements of reactant concentrations and reaction fluxes. The presentation and accompanying notebook show a simple but instructive case.\nWe model cell metabolism as a set of differential equations describing enzyme-catalysed reactions. Expert knowledge is taken into account in the form of priors over parameters describing the enzymes’ dynamics. Measured metabolite concentrations and reaction fluxes are treated as depending on the parameters via the differential equations, with random noise representing measurement error.\nWe will discuss how our approach compares to others in the same field, how we plan to develop our project and some of the challenges we have faced so far. The biggest challenge is that a large and complicated system of ODEs must be solved every time the joint log probability density is evaluated. We demonstrate a strategy for speeding up this calculation by exploiting the assumption that the system of reactions is at steady state.\nhttps://www.biosustain.dtu.dk/research/scientific-sections/quantitative-modelling-of-cell-metabolism/staff-quantitative-modelling-of-cell-metabolism\n\n Modeling cocoa bean fermentation processes. Mauricio Moreno-Zambrano, Sergio Grimbs, Matthias S. Ullrich and Marc-Thorsten Hütt.  Department of Life Sciences & Chemistry, Jacobs University Bremen Video\nAbstract:A key step in the production of chocolate is the fermentation of cocoa beans. This importance relies on its role in the development of chocolate’s flavor and aroma. Unlike other food fermentation processes, this specific fermentation is well known because of its lack of control and multiple ways in which it is performed. Here, a quantitative model of cocoa bean fermentation is constructed on previously available data regarding microbiological and metabolites dynamics. The model is formulated as a system of coupled ordinary differential equations (ODEs) with two different types of state variables: (1) Metabolite concentrations of glucose (Glc), fructose (Fru), ethanol (EtOH), lactic acid (LA) and acetic acid (Ac), and (2) population sizes of yeast (Y), lactic acid bacteria (LAB) and acetic acid bacteria (AAB). In total, the model comprehends 25 unknown parameters that were estimated using the Markov chain Monte Carlo No-U-Turn sampler in Rstan. Thereafter, we demonstrate that the model can quantitatively describe existing fermentation series and that the estimated parameters can be used to extract and interpret differences in environmental conditions between two independent fermentation trials [1].\nReferences\n[1] Moreno-Zambrano, M., Grimbs, S., Ullrich M. S. and Hütt, M-T. (2018). A mathematical model of cocoa bean fermantation. Royal Society Open Science, 5(10), 180 964.\n\n Bayesian analyses of time-to-event data using the rstanarm R package. Eren M. Elçi, Sam Brilleman. Public Health and Preventive Medicine, Monash University.  Video\nAbstract: Time-to-event data refers to the observed time from a defined origin (e.g. diagnosis of a disease) until a terminating event of interest (e.g. death). Time-to-event data emerges in a range of industries and scientific disciplines, although it is particularly common in medical and pharmaceutical research. In these research fields, time-to-event data is commonly known as survival data reflecting the fact that death is an event endpoint often used in clinical studies. Analyses of survival data are widely used for decision making in clinical trials, drug development and regulatory approvals.\nIn this talk we introduce a flexible family of Bayesian survival models that are being integrated into the rstanarm R package through the new stan_surv modelling function. The implementation uses a familiar formula syntax for specifying covariates and censoring mechanisms, based on the widely recognised survival R package. The stan_surv modelling function accommodates standard parametric (e.g. exponential, Weibull and Gompertz) survival models under either hazard or accelerated failure time formulations. Additionally, flexible parametric (cubic spline-based) hazard models are available. These allow the time-dependent baseline hazard and time-dependent effects of covariates to both be modelled using flexible smooth functions. We demonstrate the software using an example dataset. We put particular emphasis on functionality that allows practitioners to implement survival analyses as part of a robust Bayesian workflow, including prior and posterior checks and efficient leave-one-out cross-validation.\n\n Prior choice in logit models of discrete choice. Jim Savage. Schmidt Futures Video\nAbstract: In models of discrete choice, sensible-seeming priors on part-worth coefficients can imply priors in the choice probability space that are highly implausible, putting close to 100% prior weight on a single choice dominating all others. This problem reveals itself in problems with initialization and poor fit quality. Yet choosing priors is complicated by the research design, including the dimensionality of choice attributes, and their scale and covariance. In this talk I provide intuition for how priors and choice attributes interact to create extreme prior choice probabilities, and describe a new method to define priors that implies close-to-uniform weight in the choice probability space.\n\n Semiparametric Modeling of the Mean,Variance and Scale Parameters in Skew NormalRegression Models: A Bayesian Perspective. Héctor Zarate. \nAbstract: The goal of this paper is to estimate the location, scale and shape functions in heteroscedastic semiparametric models when the response variable comes from a skew normal distribution. We rely on the connection among smoothing methods that use basis functions with penalization, mixed models and a Bayesian Markov Chain sampling simulation methodology. The novelty of our strategy lies in its potential to contribute to a simple and unified computational methodology that takes into account the factors that affect the parameters in the responses, which in turn is important for an efficient estimation and correct inference without the requirement of fully parametric models. A simulation study investigates the performance of the estimates. Finally, an application using the forecasting predictive densities, highlights the merits of our approach.\n\n Hierarchical models for gamma-ray burst populations. J. Michael Burgess.  MPE \nAbstract: Inferring the number, rate and intrinsic properties of short gamma-ray bursts has been a long studied problem in the field. As it is closely related to the number of GW events expected for neutron star mergers, the topic has begun to be discussed int he literature again. However, the utilized techniques for GRBs still rely on improper statistical modeling V/Vmax estimators and in many cases, methods are simply guessed. I will discuss the use of Bayesian hierarchal models to infer population and object level parameters of inhomogeneous-Poisson process distributed populations. Techniques to handle high-dimensional selections effects will be introduced. The methodology will then be applied to sGRB population data with the aim of understand how many of these objects there are, where they are in the Universe and what are their properties under given modeling assumptions. The methodology is general, thus extensions to other populations can be made easily.\n\n A Decision-Theoretic Journey From Early Clinical Data to Late Stage Efficacy using Hierarchical Joint Models.. Krzysztof Sakrejda, Eric Novik.  Generable  Video\nAbstract: Most statistical problems end with estimating the quantities of interest which may be unobservable parameters or in the prediction context, potentially observable data. As statisticians we sometimes forget that models are often decision-making tools and making decisions conditional on our understanding of the uncertainties in the system is the ultimate goal of the consumers of our models. In this talk, we will introduce a decision problem of advancing therapies to late-stage clinical trials from early-stage clinical data. We do this in the context of a Bayesian Joint Model.\nIn clinical studies, it is common to measure a clinical biomarker repeatedly over time (‘longitudinal data’). It is also common to measure the patient-specific time from a defined origin, e.g. diagnosis of a disease, until a clinical event of interest, such as death or disease progression (‘survival data’). Joint Modeling as it is called in the Survival literature aims to model both the longitudinal biomarker evolutions and survival endpoints simultaneously. Commonly, this is achieved by specifying a joint likelihood formulation for longitudinal and survival outcomes.\nJoint modeling approaches provide several benefits over more traditional modeling and have applications to health through (i) improving the understanding of how biomarkers influence event endpoints; (ii) the development of dynamic risk prediction models for use in personalized medicine; and in the context of clinical trials (iii) requiring fewer patients than the event model alone.\nOnce the inferences from the Joint Model are obtained, we set up a Utility function describing the risk preferences of the trial’s sponsors and take its expectation with respect to the posterior distribution. The resulting function is then maximized.\n\n The State of GPU Computation Support for Stan. Rok Češnovar (University of Ljubljana - UL), Steve Bronder (Capital One), Davor Sluga (UL), Jure Demšar (UL), Tadej Ciglarič (UL), Sean Talts (Columbia University), Erik Štrumbelj (UL).  Video\nAbstract: Our presentations details the current state of and future work on the OpenCL-based framework that allows the Stan automatic differentiation library to utilize GPUs. Our research was initially motivated by large Gaussian Process models where the computation is dominated by the Cholesky decomposition but has since developed into an extensible framework.\nThe following GPU-optimized routines for matrix algebra primitives are already available to Stan users (including reverse mode): matrix multiplication, solving triangular systems, Cholesky decomposition and some special cases. Several support functions are available in the Math library but not exposed to Stan users: matrix initialization, input validity checking, copy, pack/unpack, multiplication by scalar, and transpose. We have made progress on implementing commonly used likelihoods - 4 Generalized Linear Model likelihoods can already be used: normal (identity), Bernoulli (logit), Poisson (log) and Negative Binomial (log). And data caching is now available and substantially reduces the overhead of transferring data to the GPU.\nWe will show how problem size, model and choice of hardware impact the speedups that we can achieve with GPU computation in Stan. Finally, we will discuss directions for future work, routines to implement next, autotuning tunable GPU parameters and advanced data caching.\n\n Extending Stan’s Automatic Differentiation (AD) capabilities using dco/c++. Philip Maybank. Numerical Algorithms Group (NAG)  Video\nAbstract: Tape-based AD Libraries, such as NAG’s dco/c++ tool, keep a record of calculations that are executed by a program in order to evaluate derivatives. They are applicable to a wider range of numerical codes than tape-free AD libraries, which are typically written to compute derivatives for a specific library of functions. The Stan Math Library is a tape-free AD library. The basic idea of the work in this presentation is that dco/c++ can be used to supply derivatives to Stan. This extends the range of functions which can be used by Stan’s MCMC samplers. We illustrate this idea on a toy problem: inferring the parameters of a damped harmonic oscillator driven by white noise using Stan’s NUTS.\n\n Estimating the prevalence of HIV infection in England using Bayesian evidence synthesis. Anne Presanis, Christopher Jackson [presenting author], Daniela De Angelis (MRC Biostatistics Unit, University of Cambridge); Peter Kirwan, Alison Brown, Ada Miltz, Ross Harris, Cuong Chau, Stephanie Migchelsen, Hamish Mohammed, Katy Davison, Sara Croxford, Sarika Desai, Kathy Lowndes, Valerie Delpech, Noel Gill (Public Health England).  Video\nAbstract: We present a substantive application of Stan that has informed national health policy.\nAnnual estimation of the number of people living with HIV in England, including those who are unaware of their infection, has, for several years, been based on a Bayesian model that combines evidence from multiple sources of data. For several demographic and risk groups, the model estimates the number of people in each group, the prevalence of HIV, and the proportion of HIV infections that are diagnosed.\nIn the 2018 version of this model, implemented in Stan, the strata are defined by age, gender, sexual behaviour, injecting drug use, ethnicity and region. Changes between years are also modelled. Routinely-collected data sources include a register of diagnosed HIV infections, a register of attendances at genitourinary medicine (GUM) clinics, and the national census. These are combined with data from several surveys of health and sexual behaviour among different groups, HIV testing data from unlinked anonymous surveys of drug users, and data from HIV testing of donated blood.\nThis is an example of a ““multiparameter evidence synthesis”“, where the quantities of interest cannot be estimated directly, but can be inferred indirectly through a network of model assumptions. Potential biases due to selection, under-reporting and missing data are represented explicitly through structural assumptions and informative priors. A four-level hierarchical model is used to borrow strength between stratum-specific parameters. Stan’s model description language makes the assumptions explicit, and its inference engine provides posterior estimates efficiently.\nThe estimates from 2018 demonstrate that the UN-AIDS target of 90% of infections diagnosed by 2020 has been met in England, and the estimates continue to inform policies around HIV testing, treatment and prevention.\n\n Estimating the effect of age and league on scoring rate in professional soccer. Benjamin Torvaney. Wefarm\nAbstract: Understanding the effect of different factors on player output is critical to accurately evaluating player performance. In particular, it is useful to be able to project performance into the future, whether to assess a potential new signing, or to aid in squad management. To do this, we must account for footballing context. Intuitively, we know that scoring goals in the Norwegian Eliteserien is less impressive than scoring in the Premier League; however, this is rarely quantified.\nIf we propose a model in which a player’s expected goalscoring rate is the product of their ability, the difficulty of the competition, and a relative age effect, we can estimate the effect of each parameter from historical goalscoring tallies (accompanied by minutes played). We can extend the model to allow competition factors to vary over time, to reflect the changing dynamics of professional soccer.\nSuch a model yields promising results: high profile soccer stars have the highest model estimates; a clear age curve for goalscoring is produced; competition strengths vary over time in accordance with popular perception.\n\n Multi-channel Gaussian Processes as flexible alternatives to linear models: perspectives and challenges to scaling up Bayesian inference to genomic-scale data. Caetano Souto-Maior, Susan T. Harbison. Laboratory of Systems Genetics, National Heart Lung and Blood Institute, NIH.\nAbstract:\n\n One weird trick: Non-parametric Bayesian updating by kernels. Robert Grant.  BayesCamp\nAbstract: One of the big attractions for people adopting Bayesian methods is the promise of ““updating”” their parameter estimates and predictions as more data arrive. Yesterday’s posterior becomes today’s prior. In practice, this is not always simple, requiring at the very least a complete set of sufficient statistics, random samples from an unchanging population, and no changes of mind about the probability distribution for the priors. Sometimes, one would like to update without imposing an a priori distribution on yesterday’s posterior and without estimating lots of statistics. I discuss a kernel approach, which is easily incorporated in Stan by an additional target+= statement, looping over yesterday’s posterior draws, and uniform proposal densities. I compare this with parametric updates, and explore the potential to reduce computation by using kernels weighted by counts of posterior draws inside hypercubes of parameter space.\n\n Making Stan Faster using Sequential Monte Carlo samplers. Simon Maskell (University of Liverpool), Alessandro Varsi (University of Liverpool), Peter Green (University of Liverpool), Paul Horridge (University of Liverpool), Alejandro Diaz (University of Liverpool), Lee Devlin (University of Liverpool), Rob Moore (University of Liverpool), Katerina Chatzopoulou (University of Liverpool), Jinglai Li (University of Liverpool), Maria Sudell (University of Liverpool), Luke Mason (STFC), Robin Pinning (STFC), Jack Taylor (STFC), Vassil Alexandrov (STFC), Ed Pyzer-Knapp (IBM). \nAbstract: Stan uses the No U-Turn Sampler (NUTS), a specific instance of Markov Chain Monte Carlo (MCMC). MCMC can be slow, e.g., when dimensionality is high and it would be better if NUTS was faster. We have recently been working to improve the run-time of a solution to problems that Stan can tackle (and those that it cannot, e.g. those that would require reversible jump MCMC). Our approach has been to replace NUTS with a variant of a Sequential Monte Carlo (SMC) sampler that uses the clever ideas embodied in NUTS without coupling them to MCMC specifically. SMC samplers manipulate a population of samples, making it possible to distribute computation across each of many processors. Our work has shown that SMC samplers can be configured to exploit this parallelism (and the advances that have led to the development of, for example, the use of NUTS as a proposal distribution). This can achieve faster run-time than MCMC in terms of the number of effective samples per second (by running the SMC sampler on clusters of hundreds of cores, as are routinely used in the context of Deep Learning, for example). Furthermore, we have shown that SMC samplers can be configured to outperform MCMC by making better use of the available processing resources. This is possible because MCMC’s convergence proofs require that the single sampling chain never goes wrong while the proofs for SMC samplers only require that the samples don’t all malfunction simultaneously. Put another way, SMC samplers have an additional degree of freedom in their design and this degree of freedom can be exploited to offer improved performance relative to MCMC. This talk will explain how SMC samplers can outperform MCMC per second and per flop. We will also describe our progress to date on integrating SMC samplers into Stan: our intent is to make it possible to use all Stan files. Thus far we’re able to achieve a runtime that is over an order of magnitude faster than MCMC.\n\n Gaussian process modeling and covariate selection for longitudinal data. Juho Timonen, Aki Vehtari, Harri Lähdesmäki. Alto University\nAbstract: Longitudinal data arises when the same observational units are measured repeatedly, and is common in clinical studies. Such data is often modeled using generalized linear mixed effect models with off-the-shelf software packages. These are, however, restricted to a parametric form and cannot model non-stationary disease effects. We demonstrate our new R-package for interpretable Bayesian non-parametric modeling of longitudinal data using additive Gaussian processes. Like the R-packages and brms, our goal is to provide an interface to Stan with a simple and intuitive syntax. However, our Stan program is specifically designed for Gaussian process modeling of longitudinal data, allowing the user to specify a model that mixes group and individual-specific age effects or effects of other continuous or categorical covariates. We show how our package uses Stan to model non-stationary disease effects and uncertainty of the observed disease onsets, identify heterogeneous effects present in only a subset of study subjects, and handles general non-Gaussian likelihoods. Furthermore, we define a way of resolving the relevance of any continuous or categorical covariate by sampling only one full model with all covariates included. Our focus is on biomedical applications, where is often vital to determine which covariates affect the response variable, in order to reduce future measurement costs or have a better interpretation about the progression of a disease.\n\n Computing prediction and tolerance intervals for a mixture of normal distributions. Jean-francois Michiels, Timothy Mutsvari, Oussama Errazi. Pharmalex\nAbstract: For the submission of a Biosimilar product, Biosimilarity assessment is the first step to achieve in the “Totality of Evidence” strategy as required by Authorities (e.g. FDA). The main objective of biosimilarity is to give evidence that the test biological product is as similar as possible to the reference product. The definition of ‘similar’ remains a critical component that needs to be addressed and justified. For biologicals, it is the process and its capability that should be evaluated, i.e. the risk of producing batches outside defendable limits. Thus, the first step is to set the acceptance limits. β-expectation and (β,γ), also known as Beta-Gamma, tolerance intervals are useful metrics to demonstrate that a test product (i.e. the biosimilar) is similar to a reference product. Biosimilarity is concluded if the β-expectation of the biosimilar product is within the (β,γ) of the reference. β-expectation interval is constructed to contain a β proportion of the population on average. A (β,γ) tolerance interval on the other hand is built to contain at least a β proportion of the population with a confidence level γ. In general, the pharmaceutical company producing the biosimilar has no access to the data of the reference product. Buying boxes of the reference product from several drugstores and analysing them is nevertheless one possible strategy to acquire knowledge on the process variability. Due to that sampling strategy, the distribution of the reference product can be quite exotic and it is likely that the distribution of the reference product is a mixture of normal distributions. Fitting a mixture of 2 normal distributions on data is performed using Stan. The output are the posterior distributions of the mean and standard deviation of the 2 normal distributions and the posterior distribution of the relative proportion of the 2 distributions. We present different algorithms to derive β-expectation and (β,γ) tolerance intervals for a mixture of 2 normal distributions. Using simulations, the operating characteristics of the intervals are shown (e.g. the capability to conclude similarity when it is actually similar).\n\n Parallel numerical ODE solution in Torsten for population models. Yi Zhang, William R. Gillespie. Metrum LLC\nAbstract: Torsten is a collection of functions to facilitate analysis of pharmacometric data using Stan. To seek an alternative to the ““map_rect”” function for within-chain parallel computation in Stan, we have implemented numerical ODE solution functions for population models with functional signatures that specify schedules of events such as doses and observations in a manner consistent with NONMEM compatible.\nThe population solution function feature is designed toward multi-level parallelization using Message Passing Interface(MPI). For that we first implemented Torsten’s own ODE integrators based on CVODES library. Table 1 shows MPI performance results of such an integrator on a group of 1000 Lorenz systems.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n10986\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n5505\n\n\n2.00\n\n\n1.00\n\n\n\n\n4\n\n\n3091\n\n\n3.55\n\n\n0.89\n\n\n\n\n8\n\n\n1459\n\n\n7.53\n\n\n0.94\n\n\n\n\n16\n\n\n1355\n\n\n8.11\n\n\n0.51\n\n\n\n\n32\n\n\n739\n\n\n14.87\n\n\n0.46\n\n\n\n\n64\n\n\n424\n\n\n25.91\n\n\n0.40\n\n\n\n\n128\n\n\n382\n\n\n28.76\n\n\n0.22\n\n\n\n\n256\n\n\n284\n\n\n38.68\n\n\n0.15\n\n\n\n\n512\n\n\n293\n\n\n37.49\n\n\n0.07\n\n\n\nTable 1: MPI performance of the Lorenz model solved by Torsten’s BDF integrator. (n_population = 1000)\nThen we developed MPI-based population solvers that are specifically designed for PKPD applications, for which ODE system size \\(n\\) is typically in the scale of \\(10^0\\sim 10^2\\). We employ the latest standard(MPI-3) functionalities for latency hiding, and test the implementation on two MPI implementations (OpenMPI and MPICH). Tables 2-5 show performance results of one such function on a simple two-compartment PK model(\\(n=3\\)) and a more complex PKPD model(\\(n=8\\)), run on a METWORX workflow.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n2966\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n1544\n\n\n1.92\n\n\n0.96\n\n\n\n\n4\n\n\n866\n\n\n3.42\n\n\n0.85\n\n\n\n\n8\n\n\n887\n\n\n3.34\n\n\n0.42\n\n\n\nTable 2: Parallel performance of solving a two-compartment population model using pmx_solve_group_bdf and OpenMPI.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n45791\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n23532\n\n\n1.95\n\n\n0.97\n\n\n\n\n4\n\n\n13421\n\n\n3.41\n\n\n0.85\n\n\n\n\n8\n\n\n10394\n\n\n4.41\n\n\n0.55\n\n\n\nTable 3: Parallel performance of solving a Neutropenia population model using pmx_solve_group_bdf and OpenMPI.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n2470\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n1419\n\n\n1.74\n\n\n0.87\n\n\n\n\n4\n\n\n1170\n\n\n2.11\n\n\n0.53\n\n\n\n\n8\n\n\n860\n\n\n2.87\n\n\n0.36\n\n\n\nTable 4: Parallel performance of solving a two-compartment population model using pmx_solve_group_bdf and MPICH.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n45087\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n22976\n\n\n1.96\n\n\n0.98\n\n\n\n\n4\n\n\n14158\n\n\n3.18\n\n\n0.80\n\n\n\n\n8\n\n\n10523\n\n\n4.28\n\n\n0.54\n\n\n\nTable 5: Parallel performance of solving a Neutropenia population model using pmx_solve_group_bdf and MPICH.\nIn addtional to population-level parallelization, we are also implementing individual-level parallelization based on parallel time integration with multigrid. This will enables us to reduce the solution time of a single ODE system, and create a multi-level parallelization for ODE-based population models. The results of a preliminary implementation are shown in Table 6.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n2.8\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n1.7\n\n\n1.65\n\n\n0.82\n\n\n\n\n4\n\n\n1.2\n\n\n2.33\n\n\n0.58\n\n\n\nTable 6: Parallel performance of solving 10^4 steps of a single Neutropenia ODE system using parallel-in-time technique."
  },
  {
    "objectID": "learn-stan/stancon/stancon2019-abstracts.html#talks-by-topic",
    "href": "learn-stan/stancon/stancon2019-abstracts.html#talks-by-topic",
    "title": "StanCon 2019 Programme",
    "section": "",
    "text": "Pharmaceuticals/Medicine\n\nComputing prediction and tolerance intervals for a mixture of normal distributions. Jean-francois Michiels, Timothy Mutsvari, Oussama Errazi. Pharmalex. Abstract\nParallel numerical ODE solution in Torsten for population models. Yi Zhang, William R. Gillespie. Metrum LLC Abstract\nMulti-channel Gaussian Processes as flexible alternatives to linear models: perspectives and challenges to scaling up Bayesian inference to genomic-scale data. Caetano Souto-Maior, Susan T. Harbison. Laboratory of Systems Genetics, National Heart Lung and Blood Institute, NIH. Abstract\nEstimating the prevalence of HIV infection in England using Bayesian evidence synthesis. Anne Presanis, Christopher Jackson [presenting author], Daniela De Angelis (MRC Biostatistics Unit, University of Cambridge); Peter Kirwan, Alison Brown, Ada Miltz, Ross Harris, Cuong Chau, Stephanie Migchelsen, Hamish Mohammed, Katy Davison, Sara Croxford, Sarika Desai, Kathy Lowndes, Valerie Delpech, Noel Gill (Public Health England).  Abstract Video\nA Decision-Theoretic Journey From Early Clinical Data to Late Stage Efficacy using Hierarchical Joint Models.. Krzysztof Sakrejda, Eric Novik.  Generable  Abstract Video\nBayesian analyses of time-to-event data using the rstanarm R package. Eren M. Elçi, Sam Brilleman. Public Health and Preventive Medicine, Monash University. Abstract Video\nModelling enzyme kinetics with Stan. Teddy Groves. DTU BIOSUSTAIN Quantitative Modelling of Cell Metabolism Team Abstract Video\nThe emergence of HIV resistance to antiretroviral therapy in southern Africa: a mechanistic meta-analysis of survey data. Julien Riou, Matthias Egger, Christian Althaus. Institute of Social and Preventive Medicine, University of Bern, Switzerland Abstract Video\nHandling missing data, censored values and measurement error in machine learning models using multiple imputation for early stage drug discovery. Rowan Swiers. AstraZeneca Abstract Video\nA Bayesian multi-layered model to predict mechanisms, types, and severity of drug-induced liver injury. Elizaveta Semenova, Dominic Williams, Stanley E Lazic. Data Science and Quantitative Biology group, AstraZeneca, Cambridge UK Abstract\n\nModeling\n\nGaussian process modeling and covariate selection for longitudinal data. Juho Timonen, Aki Vehtari, Harri Lähdesmäki. Aalto University Abstract\nEstimating the effect of age and league on scoring rate in professional soccer. Benjamin Torvaney. Wefarm Abstract\nHierarchical models for gamma-ray burst populations. J. Michael Burgess.  MPE  Abstract\nModeling cocoa bean fermentation processes. Mauricio Moreno-Zambrano, Sergio Grimbs, Matthias S. Ullrich and Marc-Thorsten Hütt.  Department of Life Sciences & Chemistry, Jacobs University Bremen Abstract Video\nApproximate leave-future-out cross-validation for Bayesian time series models. Paul Bürkner, Jonah Gabry, Aki Vehtari.  Abstract Video\nWhen seasonality meets Bayesian: Decomposing seasonalities in Stan. Hyunji Moon, SNU, Hyeonseop Lee, PUBG.  Abstract Video\nPrediction and causal inference for time-to-event outcomes truncated by death. . Leah Comment.  Abstract Video\nFast Forward Like a Lambo (skrrt skrrt). Daniel Lee. Generable Abstract Video\nThe Currency of Place and the Short-Term Rental Market. Mikael Brunila.  Abstract Video\nProfit-Maximizing A/B Tests. Elea McDonnell Feit, Ron Berman. Drexel Univeristy, The Wharton School Abstract Video\nStructured priors for survey estimates in the presence of non-representative data. Yuxiang Gao (University of Toronto), Lauren Kennedy (Columbia University), Daniel Simpson (University of Toronto).  Abstract Video\nChronikis: a Bayesian time-series modeling language. Kevin S. Van Horn. Adobe Inc. Abstract Video\nA long-short term event memory state-space model for multi-party elections. Marcus Groß. INWT Statistics GmbH Abstract\nSimulation of Statistic Mechanical Systems using Stan. Forrest Eli Hurley. North Carolina State University Abstract Video\nRegularized Hierarchical Models for Remotely Sensed Forest Inventories. Nathan E. Rutenbeck SilviaTerra Abstract \nGetting the Lead out–Does New York City’s childhood lead testing make statistical sense?. Jonathan Auerbach, Breck Baldwin. Columbia Univeristy Abstract Video \n\nInference\n\nMaking Stan Faster using Sequential Monte Carlo samplers. Simon Maskell (University of Liverpool), Alessandro Varsi (University of Liverpool), Peter Green (University of Liverpool), Paul Horridge (University of Liverpool), Alejandro Diaz (University of Liverpool), Lee Devlin (University of Liverpool), Rob Moore (University of Liverpool), Katerina Chatzopoulou (University of Liverpool), Jinglai Li (University of Liverpool), Maria Sudell (University of Liverpool), Luke Mason (STFC), Robin Pinning (STFC), Jack Taylor (STFC), Vassil Alexandrov (STFC), Ed Pyzer-Knapp (IBM) .  Abstract\nOne weird trick: Non-parametric Bayesian updating by kernels. Robert Grant. BayesCamp Abstract\nSemiparametric Modeling of the Mean,Variance and Scale Parameters in Skew Normal Regression Models: A Bayesian Perspective. Héctor Zarate.  Abstract\nPrior choice in logit models of discrete choice. Jim Savage. Schmidt Futures Abstract Video\nStacking for multimodal posterior distributions. Yuling Yao, Aki Vehtari and Andrew Gelman.  Abstract Video\nBayesian leave-one-out cross-validation for large data. Måns Magnusson, Aalto, Michael Riis Andersen, Danish Technical University, Johan Jonasson, Chalmers Technical University, Aki Vehtari, Aalto.  Abstract Video\n\nCore Stan\n\nThe State of GPU Computation Support for Stan. Rok Češnovar (University of Ljubljana - UL), Steve Bronder (Capital One), Davor Sluga (UL), Jure Demšar (UL), Tadej Ciglarič (UL), Sean Talts (Columbia University), Erik Štrumbelj (UL).  Abstract Video\nExtending Stan’s Automatic Differentiation (AD) capabilities using dco/c++. Philip Maybank. Numerical Algorithms Group (NAG) Abstract Video"
  },
  {
    "objectID": "learn-stan/stancon/stancon2019-abstracts.html#abstracts",
    "href": "learn-stan/stancon/stancon2019-abstracts.html#abstracts",
    "title": "StanCon 2019 Programme",
    "section": "",
    "text": "Regularized Hierarchical Models for Remotely Sensed Forest Inventories. Nathan E. Rutenbeck SilviaTerra \nManagement and conservation of the world’s forests is critical for maintaining global timber supply, as well as for the ecosystem services forestlands provide. Forest biometrics remains a field focused on traditional methods in sampling and regression, despite the fact that these methods are ill equipped to utilize the profusion of remote sensing data now available. When remote sensing data is used, it is often deployed within simple population-level regression models that simultaneously leave out information regarding known forest structure and sample design, and are prone to overfitting of effects at the population level. Using Stan, we show that for the prediction of forest basal area (a key inventory attribute) incorporating known structural attributes (forest stands) and sample design information into a hierarchical modeling framework along with remote sensing data can yield beneficial results in terms of reducing overfitting and improving predictive performance when compared to more conventional methods. We fit and compared four candidate models, examining their performance with respect to one another and to the conventional frequentist inferences that are so widely used for operational forest inventory. The four models we examined are 1) a hierarchical model incorporating forest stand and sample design effects; 2) a population-level remote sensing principal components model; 3) the hierarchical model with the addition of remote sensing principal component effects at the population level; and 4) the hierarchical and remote sensing model with the addition of regularizing horseshoe priors on remote sensing effects. The hierarchical model without remote sensing effects showed the expected shrinkage of stand-level mean basal area predictions toward the global mean. The addition of remote sensing effects showed overall reductions in prediction error in comparison to the sample design model. Incorporating regularizing priors on the remote sensing principal components effects retained signal from the remote sensing data but showed further shrinkage of predictions of stand-level mean basal area toward sample means. Our results suggest that the penalized hierarchical model can be used in developing operational forest inventories that balance information from known forest structural heterogeneity, the sample design, and remote sensing data.\n A Bayesian multi-layered model to predict mechanisms, types, and severity of drug-induced liver injury. Elizaveta Semenova, Dominic Williams, Stanley E Lazic. Data Science and Quantitative Biology group, AstraZeneca, Cambridge UK \nAbstract: Drug-induced liver injury (DILI) is a major cause of attrition in drug development and a common reason for withdrawing a drug from the market. Predicting clinical liver toxicity is difficult, but preclinical in vitro assays and physical/chemical properties of drugs can be used as predictors. We developed a multi-layered Bayesian model where we use assay results to predict the mechanism(s) of toxicity, use the mechanisms to predict the type of liver injury, and then combine the type of injury with the clinical dose of a drug to predict the severity of injury. The model therefore has a layered structure, enabling uncertainty to propagate through the layers. Based only on assay and physchem data, along with the clinical dose, the model enables safety pharmacologists to predict the severity, type, and mechanism of liver toxicity with good accuracy.\n\n \n Getting the Lead out–Does New York City’s childhood lead testing make statistical sense?. Jonathan Auerbach, Breck Baldwin. Columbia Univeristy .  Video\nAbstract: The US has dramatically reduced blood lead levels in children over the past 30 years and that effort continues. New York City (NYC) was an early adopter of lead reduction policies and that effort continues with laws that require all children be tested and with mandatory interventions for those tested blood levels (tbll) greater than 5mg/dL. But there is a statistically interesting story around how current blood level limits are set, the performance of common tests and how to apply common Bayes rule reasoning to publicly available data.\nThe data we have: We have high quality blood lead level (bll) tests applied nation wide (NHANES) for 5,000 children, we have NYC supplied data that provides counts for all children’s tested blood lead level, the number greater than 5mg/dL, 10mg/dL and 15/dL and claims of blood tests that widely vary from sources like FDA applications for blood testing equipment, actual studies of test performance and government testing standards.\nThe data we want: New York city recently dropped the threshold for intervention from 10mg/dL to 5mg/dL. It is an open question what the false positive rate is for these test thresholds with some research suggesting that it is as high as 70%. On the other extreme is an FDA applications for the LeadCare Plus testing device claim a standard deviation of .5 at the 5mg/dL which suggests a very low false positive rate…but that depends on the distribution of actual blls in the NYC population.\nHow we got the data we wanted: This is a simple application of Bayes rule: p(bll &gt; 5|t &gt;5) = p(tbll &gt; 5| bll&gt;5) p(bll&gt;5)/p(tbll&gt;5) where we don’t know p(bll&gt;5) for NYC. NYC refused to release non-quantized data for tbll under FIOA requests, which if we had, would allow a fairly straightforward determination of false positive rates from tbll test evaluations. But we do have data for the US as a whole in non-quantized form.\nThe paper describes a process of model refinement staring with naive approaches and incrementally modifying our models to better suite NYC data. The final approach, subject to change as we do more work, is to fit national NHANES data with an exponential distribution, assume that similar distributions apply to NYC and recover a believable false positive rate across a range of reported blood test performance. Along the way we show an interesting simple use of the ‘integrate_ode_rk45’ function in Stan and demonstrate Bayesian workflow.\n\n Simulation of Statistic Mechanical Systems using Stan. Forrest Eli Hurley. North Carolina State University  Video\nAbstract: Bayesian statistics is closely coupled with physics. The metropolis algorithm (1953) was developed by scientists working at Los Alamos as a method for thermodynamic simulation of molecular dynamics. Not until the work of W. K. Hastings (1970) was the method generalized to arbitrary probability distributions. Hamiltonian Monte Carlo is even more deeply rooted in physics than the Metropolis-Hastings algorithm. The simulation of states with velocities, energies, and a Hamiltonian describes nothing other than a physical system. It matches a canonical ensemble in that there is not a fixed energy between steps, only an overall fixed temperature. The temperature is usually only implicit, but some tempering methods simulate chains at higher temperatures to smooth the probability distributions. The Ising Model, a proxy for magnetization, is a prevalent introductory model in the study of statistical mechanics. It consists of an N-dimensional grid of spin up or down particles. The energy varies depending on the alignment of spins between nearest neighbors. At low temperatures spins tend to align on a macroscopic scale; at high temperatures they become evenly distributed. We simulate the XY Model, similar to the Ising Model but allowing spins to be represented by unit vectors in two dimensions, using Stan. We create chains at several temperatures to identify the locations of phase transitions in macroscopic properties. Our work shows the applicability of Stan for computation in continuous statistical mechanical problems.\n\n A long-short term event memory state-space model for multi-party elections. Marcus Groß. INWT Statistics GmbH \nAbstract: State-space models are a popular choice in modelling voting intentions and election results by using poll data. The presented multivariate state-space model attempts to go beyond random-walk or Kalman-filter approaches (with comparable performance to simple weighted survey averages) to the problem by introducing a long-short term event memory effect. This effect serves as reasonable explanation to the observation that the voter’s share partially tends to reverse to the party’s long-term trend after larger short term movements. Any event influencing the voter’s share of a party is presumed to have a convex shaped effect decomposable into a short term effect due to e.g. media spreading and a smaller long term effect remaining despite overlay effects of new events and forgetting. This effect is modelled by a mixture of a random walk and two contrasting autoregressive processes. By also taking advantage of the widely observed effect that government parties tend to fall in voter’s share, whereas the opposite effect is observed for opposition parties, mid- and long-term predictions of election outcomes can be considerably be improved. The Stan-model is fitted and evaluated on poll data from seven pollsters for the German national elections (“Bundestagswahl”) from 1994 to 2017, where low double digits (out-of-sample) improvements in prediction performance can be seen between 3- and 18-months prior elections. By taking into account the pollsters house effects, their poll errors and even more importantly their correlations in poll errors, an appropriate and realistic estimation error can be propagated.\n\n Bayesian leave-one-out cross-validation for large data. Måns Magnusson, Aalto, Michael Riis Andersen, Danish Technical University, Johan Jonasson, Chalmers Technical University, Aki Vehtari, Aalto.  Video\nAbstract: Model inference, such as model comparison, model checking, and model selection, is an important part of model development. Leave-one-out cross-validation (LOO) is a general approach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We propose a combination of using approximate inference techniques and probability-proportional-to-size-sampling (PPS) for fast LOO model evaluation for large datasets. We provide both theoretical and empirical results showing good properties for large data.\n\n Stacking for multimodal posterior distributions. Yuling Yao, Aki Vehtari and Andrew Gelman.  Video \nAbstract: When working with multimodal posterior distributions, MCMC algorithms can have difficulty moving between modes, and default variational or mode-based approximate inferences can understate posterior uncertainty. And, even if the most important modes can be found, it is difficult to evaluate their relative weights in the posterior, which requires computing the integral of the posterior in the neighborhood of each mode. Here we propose an alternative approach, using parallel runs of MCMC, variational, or mode- based inferences to hit as many modes as possible, and then using Bayesian stacking to weight the set of simulations at each mode. Bayesian stacking is a method for constructing a weighted average of distributions so as to minimize cross-validated prediction errors. The result from stacking is not necessarily equivalent, even asymptotically, to fully Bayesian inference, but it serves many of the same goals. We discuss in the context of several theoretical and applied examples.\n\n Chronikis: a Bayesian time-series modeling language. Kevin S. Van Horn. Adobe Inc.  Video\nAbstract: Chronikis (http://chronikis.org) is an open-source language for Bayesian time-series models that compiles to Stan and R. It currently focuses on linear state-space models, with plans to incrementally expand the class of supported models over time. The goal for Chronikis is to allow one to quickly and reliably create and apply a variety of models to a time series, doing a full Bayesian analysis on each.\nThus the Chronikis language itself focuses on concise and clear model specification, and as far as possible the task of creating efficient estimation and forecasting code is left to the compiler. These twin goals are facilitated by making the Chronikis language fully declarative: the body of a Chronikis program is just an expression whose ““value”” is a probability distribution over time series.\nThe compiler applies a series of semantics-preserving transformations to the body of a Chronikis program, eventually arriving at a form that it can straightforwardly translate to Stan. Along the way it infers types and shapes for all variables except the parameters of main(), reparameterizes in some cases to use non-centered parameterization, assigns each variable to the appropriate Stan block, and infers bounds for variables assigned to the parameters block.\nFor the sake of clarity, Chronikis supports operations for constructing complex models from simpler components. For example, here is a Chronikis program for a random-walk model with observation noise:\ndef main(s_rw, s_obs: real{0.0,}, mu0: real, sigma0: real{0.0,}) = sigma_rw ~ half_cauchy(s_rw); sigma_obs ~ half_cauchy(s_obs);\naccum(wn(sigma_rw)) + constp(mu0, sigma0) + wn(sigma_obs)\nNotes on the above:\n\nThe main() parameters s_rw, s_obs, mu0, and sigma0 are prior parameters.\nsigma_rw^2 and sigma_obs^2 are the random-walk and observation-error variances.\nwn(s) is a white noise process with variance s^2.\nconstp(m,s) is a distribution over constant time series, with a Normal(m,s) distribution for the constant value.\naccum is an operator on time-series distributions; accum(D) is a time-series distribution whose draws are cumulative sums of a time series drawn from D.\nSum (+) is another operator on time-series distributions; D1 + D2 is a time-series distribution whose draws are the element-wise sum of independent draws from D1 and D2.\n\nChronikis also has some innovative support for (quasi-)periodic time-series model components. The period can be arbitrarily large, and need not even be an integer. One can allow the periodic pattern to slowly change over time. There is a smoothness parameter, and this bounds the size of the latent state required, regardless of how large the period may be. Chronikis accomplishes all this by constructing a linear state-space model that approximates the zero-mean Gaussian process defined by variant of MacKay’s periodic kernel, modified to ensure that the realizations of the process are themselves zero-centered.\n\n Structured priors for survey estimates in the presence of non-representative data. Yuxiang Gao (University of Toronto), Lauren Kennedy (Columbia University), Daniel Simpson (University of Toronto).  Video \nAbstract:A central theme in the field of survey statistics is estimating population-level quantities through data coming from potentially non-representative samples of the population. Multilevel Regression and Poststratification (MRP), a model-based approach, is gaining traction against the traditional weighted approach for survey estimates. MRP uses partial pooling through random effects, thus shrinking model estimates to an overall mean and reducing potential overfitting. Despite MRP’s straightforward specification of prior distributions, the estimates coming from it are susceptible to bias if there is an underlying structure that the prior does not capture. This work aims to provide a new framework for specifying structured prior distributions that lead to bias reduction in MRP estimates. We use simulation studies to explore the benefit of these priors and demonstrate on US survey data.\n\n Profit-Maximizing A/B Tests. Elea McDonnell Feit, Ron Berman. Drexel University, The Wharton School  Video\nAbstract: Marketers often use A/B testing as a tactical tool to compare marketing treatments in a test stage and then deploy the better-performing treatment to the remainder of the consumer population. While these tests have traditionally been analyzed using hypothesis testing, we re-frame such tactical tests as a Bayesian decision problem with an explicit trade-off between the opportunity cost of the test (where some customers receive a sub-optimal treatment) and the potential losses associated with deploying a sub-optimal treatment to the remainder of the population.\nWe derive a closed-form expression for the profit-maximizing test size and show that it is substantially smaller than that typically recommended for a hypothesis test, particularly when the response is noisy or when the total population is small. The common practice of using small holdout groups for media testing can be rationalized by asymmetric priors. The proposed test design achieves nearly the same expected regret as the flexible, yet harder-to-implement multi-armed bandit.\nAdopting a Bayesian approach to experimental design requires informative priors. We show how priors can be estimated from data on past A/B test, using Stan to fit a hierarchical meta model. An R notebook will be provided which shows the complete process from meta-analysis of past experiments to determining the profit-maximizing sample size for the new A/B test.\nA full paper is available at https://arxiv.org/abs/1811.00457.\n\n The Currency of Place and the Short-Term Rental Market. Mikael Brunila.  Video \nAbstract: Airbnb and short-term rentals are raising rents in cities through the use of new technologies and by catering to culturally savvy populations. As a phenomenon of the attention economy, Airbnb is a platform where meaning becomes priced, as efficient and attractive communication is awarded by more bookings. In this paper, we look at how this capitalization of meaning can be understood by modelling the use of neighbourhood names. Using Natural Language Processing techniques and Bayesian hierarchical logit models with Intrinsic Auto-Regressive priors, we explore how listings draw upon the value placed on well-known neighbourhoods to promote themselves. Our findings separate different spatial effects as well as neighbourhood and listing level patterns that help us explain how neighbourhood names are deployed to promote short-term rentals on Airbnb.\n\n Fast Forward Like a Lambo (skrrt skrrt). Daniel Lee. Generable Video\nAbstract: Exploring simple, automatic within-chain parallelization. For any (well-behaved) statistical model written in the Stan language, the Stan Math library (Math) provides the gradient of the log joint probability distribution function specified. It currently provides the gradient with reverse-mode automatic differentiation. Math also provides forward-mode automatic differentiation, which isn’t as well tested as reverse-mode, but is available none-the less. Reverse-mode automatic differentiation scales well as it can tackle an arbitrary number of parameters with one sweep. However, this can’t be parallelized easily. Forward-mode requires N sweeps to evaluate N directional derivatives, but each of these sweeps can be done in parallel. With the adoption of C++14 capable compilers, we’re now able to use threading as an easy paradigm to coordinate within-chain parallelization. We’ll show some of the performance considerations and some preliminary results.\n\n Prediction and causal inference for time-to-event outcomes truncated by death. Leah Comment.  Video\nAbstract: Predicting customer behaviour is crucial for making decisions such as the cost of acquisition or planning for production or service capacity. In the model being presented individual purchase data of a fashion retailer is utilized to describe and predict their behaviour using Bayeasian multi-layered architecture to allow for heterogeneity and latent variables, such as customer state of activity.\n\n–&gt;\n When seasonality meets Bayesian: Decomposing seasonalities in Stan. Hyunji Moon, SNU, Hyeonseop Lee, PUBG.   Video\nAbstract: Multiple seasonalities play a key role in time series forecasting, especially for business time series where seasonal effects are often dramatic. Previous approaches including Fourier decomposition, exponential smoothing, and Seasonal ARIMA do not reflect distinct characteristics of each period in seasonal patterns such as unique behavior of specific day of the week in business data. We propose a multi-dimensional hierarchical model. Intermediate parameters for each seasonal period are first estimated, then mixture of intermediate parameters are then taken, resulting in the model which successfully reflects interactions between multiple seasonalities. Although this process leads to the reduction of data available for each parameter, a robust estimation can be obtained through a hierarchical Bayesian model. Consideration of not only the characteristics of each seasonal periods but also the interactions between characteristics from multiple seasonalities becomes possible through this model. Our new model is implemented in Stan and considerable improvements in prediction accuracy compared to previous models are achieved. Previous models include Fourier decomposition which Prophet uses to model seasonalities. Comparison has been performed on real-world dataset from a nation-scale logistic network.\n\n Approximate leave-future-out cross-validation for Bayesian time series models. Paul Bürkner, Jonah Gabry, Aki Vehtari.  Video\nAbstract: One of the common goals of time series analysis is to use the observed series to inform predictions for future observations. In the absence of any actual new data to predict, cross-validation can be used to estimate a model’s future predictive accuracy, for instance, for the purpose of model comparison or selection. As exact cross-validation for Bayesian models is often computationally expensive, approximate cross-validation methods have been developed; most notably methods for leave-one-out cross-validation (LOO-CV). If the actual prediction task is to predict the future given the past, LOO-CV provides an overly optimistic estimate as the information from future observations is available to influence predictions of the past. To tackle the prediction task properly and account for the time series structure, we can use leave-future-out cross-validation (LFO-CV). Like exact LOO-CV, exact LFO-CV requires refitting the model many times to different subsets of the data. Using Pareto smoothed importance sampling, we propose a method for approximating exact LFO-CV that drastically reduces the computational costs while also providing informative diagnostics about the quality of the approximation. We provide examples using Bayesian time-series models fitted with Stan.\n\n Handling missing data, censored values and measurement error in machine learning models using multiple imputation for early stage drug discovery. Rowan Swiers. AstraZeneca  Video\nAbstract: Multiple imputation is a technique for handling missing data, censored values and measurement error. Currently it is underused in the machine learning field due to lack of familiarity and experience with the technique, whilst other missing data solutions such as full Bayesian models can be hard to set up. However, randomization-based evaluations of Bayesianly derived repeated imputations can provide approximately valid inference of the posterior distributions and allow use of techniques which rely upon complete data such as SVMs and random Forest models.\nThis paper, using simulated data sets inspired by AstraZeneca drug data, shows how multiple imputation techniques can improve the analysis of data with missing values or with uncertainty. We pay close attention to the prediction of Bayesian posterior coverage due its importance in industrial applications. Comparisons are made to other commonly used methods of handling missing data such as single uniform imputation and data removal. Furthermore, we review several standard multiple imputation models and compare them on our simulated data sets. We provide recommendations on when to use each technique and where extra care is needed based upon data distributions. Finally, using simulated data, we give examples of how correct use of multiple imputation can affect investment decisions in the early stages of drug discovery.\nAnalysis was performed using both Python and Stan and is provided in a Jupyter notebook.\n\n The emergence of HIV resistance to antiretroviral therapy in southern Africa: a mechanistic meta-analysis of survey data. Julien Riou, Matthias Egger, Christian Althaus. Institute of Social and Preventive Medicine, University of Bern, Switzerland Video\nAbstract: Large-scale campaigns providing access to antiretroviral therapy (ART) to people living with HIV in southern Africa have been ongoing since the early 2000s. The success of these campaigns is now threatened by the emergence of HIV drug resistance, most of all resistance to non-nucleoside reverse-transcriptase inhibitors (NNRTI), a class of drugs constitutive of ART. Systematic reviews of cross-sectional surveys have provided insights into the temporal trends of NNRTI resistance among HIV-infected individuals. However, these simple temporal trends fail to account for the local dynamics of HIV transmission and treatment that create the evolutionary pressure generating resistance mutations. Such approaches limit our general understanding of the phenomena of resistance emergence in response to drug introduction and disallow any between-country comparison. Here, we propose a mechanistic approach linking the observed levels of NNRTI resistance to the underlying dynamics of HIV in each country.\nWe developed a SIR-like model consisting of a hierarchical system of ordinary differential equations in Stan. The model considered the infection of susceptible individuals with HIV, the treatment of diagnosed individuals with ART from the early 2000s, the occurrence of resistance mutations in response to the evolutionary pressure created by ART, and the transmission of mutant, resistant viruses to susceptible individuals. The model was fitted jointly to country-level data regarding different aspects of the HIV epidemic (prevalence of HIV, number of people under ART and population size in 8 countries of southern Africa from 2000 to 2016) and to measurements of NNRTI resistance in cross-sectional surveys (60 surveys from 2000 to 2016). Partial pooling was allowed by introducing a hierarchical structure by country on the parameters governing the occurrence of resistance, as well as a hierarchical structure by survey on resistance data.\nThe model could adequately reproduce the dynamics of the HIV epidemics in each country. We found substantial heterogeneity between the rates of emergence of NNRTI resistance across countries that is not explained by differences in the local dynamics of HIV transmission and treatment. Understanding the factors associated with this heterogeneity will allow public health authorities to anticipate on potential issues of drug resistance emergence related to local characteristics.\n\n Modelling enzyme kinetics with Stan.Teddy Groves. DTU BIOSUSTAIN Quantitative Modelling of Cell Metabolism Team Video\nAbstract: The advent of high-throughput technologies has transformed molecular biology into a data-rich discipline. However, integrating this data into a predictive modeling framework is not trivial because many different sources of uncertainty about the molecular processes governing cell metabolism must be taken into account. In particular, reaction fluxes and steady-state reactant concentrations can at best be measured noisily, and even for the best-understood organisms pre-experimental knowledge of kinetic parameters is incomplete and imprecise.\nWe are using Stan to overcome the existing limitations in the study of cell metabolism by combining pre-experimental knowledge about kinetic parameters with experimental measurements of reactant concentrations and reaction fluxes. The presentation and accompanying notebook show a simple but instructive case.\nWe model cell metabolism as a set of differential equations describing enzyme-catalysed reactions. Expert knowledge is taken into account in the form of priors over parameters describing the enzymes’ dynamics. Measured metabolite concentrations and reaction fluxes are treated as depending on the parameters via the differential equations, with random noise representing measurement error.\nWe will discuss how our approach compares to others in the same field, how we plan to develop our project and some of the challenges we have faced so far. The biggest challenge is that a large and complicated system of ODEs must be solved every time the joint log probability density is evaluated. We demonstrate a strategy for speeding up this calculation by exploiting the assumption that the system of reactions is at steady state.\nhttps://www.biosustain.dtu.dk/research/scientific-sections/quantitative-modelling-of-cell-metabolism/staff-quantitative-modelling-of-cell-metabolism\n\n Modeling cocoa bean fermentation processes. Mauricio Moreno-Zambrano, Sergio Grimbs, Matthias S. Ullrich and Marc-Thorsten Hütt.  Department of Life Sciences & Chemistry, Jacobs University Bremen Video\nAbstract:A key step in the production of chocolate is the fermentation of cocoa beans. This importance relies on its role in the development of chocolate’s flavor and aroma. Unlike other food fermentation processes, this specific fermentation is well known because of its lack of control and multiple ways in which it is performed. Here, a quantitative model of cocoa bean fermentation is constructed on previously available data regarding microbiological and metabolites dynamics. The model is formulated as a system of coupled ordinary differential equations (ODEs) with two different types of state variables: (1) Metabolite concentrations of glucose (Glc), fructose (Fru), ethanol (EtOH), lactic acid (LA) and acetic acid (Ac), and (2) population sizes of yeast (Y), lactic acid bacteria (LAB) and acetic acid bacteria (AAB). In total, the model comprehends 25 unknown parameters that were estimated using the Markov chain Monte Carlo No-U-Turn sampler in Rstan. Thereafter, we demonstrate that the model can quantitatively describe existing fermentation series and that the estimated parameters can be used to extract and interpret differences in environmental conditions between two independent fermentation trials [1].\nReferences\n[1] Moreno-Zambrano, M., Grimbs, S., Ullrich M. S. and Hütt, M-T. (2018). A mathematical model of cocoa bean fermantation. Royal Society Open Science, 5(10), 180 964.\n\n Bayesian analyses of time-to-event data using the rstanarm R package. Eren M. Elçi, Sam Brilleman. Public Health and Preventive Medicine, Monash University.  Video\nAbstract: Time-to-event data refers to the observed time from a defined origin (e.g. diagnosis of a disease) until a terminating event of interest (e.g. death). Time-to-event data emerges in a range of industries and scientific disciplines, although it is particularly common in medical and pharmaceutical research. In these research fields, time-to-event data is commonly known as survival data reflecting the fact that death is an event endpoint often used in clinical studies. Analyses of survival data are widely used for decision making in clinical trials, drug development and regulatory approvals.\nIn this talk we introduce a flexible family of Bayesian survival models that are being integrated into the rstanarm R package through the new stan_surv modelling function. The implementation uses a familiar formula syntax for specifying covariates and censoring mechanisms, based on the widely recognised survival R package. The stan_surv modelling function accommodates standard parametric (e.g. exponential, Weibull and Gompertz) survival models under either hazard or accelerated failure time formulations. Additionally, flexible parametric (cubic spline-based) hazard models are available. These allow the time-dependent baseline hazard and time-dependent effects of covariates to both be modelled using flexible smooth functions. We demonstrate the software using an example dataset. We put particular emphasis on functionality that allows practitioners to implement survival analyses as part of a robust Bayesian workflow, including prior and posterior checks and efficient leave-one-out cross-validation.\n\n Prior choice in logit models of discrete choice. Jim Savage. Schmidt Futures Video\nAbstract: In models of discrete choice, sensible-seeming priors on part-worth coefficients can imply priors in the choice probability space that are highly implausible, putting close to 100% prior weight on a single choice dominating all others. This problem reveals itself in problems with initialization and poor fit quality. Yet choosing priors is complicated by the research design, including the dimensionality of choice attributes, and their scale and covariance. In this talk I provide intuition for how priors and choice attributes interact to create extreme prior choice probabilities, and describe a new method to define priors that implies close-to-uniform weight in the choice probability space.\n\n Semiparametric Modeling of the Mean,Variance and Scale Parameters in Skew NormalRegression Models: A Bayesian Perspective. Héctor Zarate. \nAbstract: The goal of this paper is to estimate the location, scale and shape functions in heteroscedastic semiparametric models when the response variable comes from a skew normal distribution. We rely on the connection among smoothing methods that use basis functions with penalization, mixed models and a Bayesian Markov Chain sampling simulation methodology. The novelty of our strategy lies in its potential to contribute to a simple and unified computational methodology that takes into account the factors that affect the parameters in the responses, which in turn is important for an efficient estimation and correct inference without the requirement of fully parametric models. A simulation study investigates the performance of the estimates. Finally, an application using the forecasting predictive densities, highlights the merits of our approach.\n\n Hierarchical models for gamma-ray burst populations. J. Michael Burgess.  MPE \nAbstract: Inferring the number, rate and intrinsic properties of short gamma-ray bursts has been a long studied problem in the field. As it is closely related to the number of GW events expected for neutron star mergers, the topic has begun to be discussed int he literature again. However, the utilized techniques for GRBs still rely on improper statistical modeling V/Vmax estimators and in many cases, methods are simply guessed. I will discuss the use of Bayesian hierarchal models to infer population and object level parameters of inhomogeneous-Poisson process distributed populations. Techniques to handle high-dimensional selections effects will be introduced. The methodology will then be applied to sGRB population data with the aim of understand how many of these objects there are, where they are in the Universe and what are their properties under given modeling assumptions. The methodology is general, thus extensions to other populations can be made easily.\n\n A Decision-Theoretic Journey From Early Clinical Data to Late Stage Efficacy using Hierarchical Joint Models.. Krzysztof Sakrejda, Eric Novik.  Generable  Video\nAbstract: Most statistical problems end with estimating the quantities of interest which may be unobservable parameters or in the prediction context, potentially observable data. As statisticians we sometimes forget that models are often decision-making tools and making decisions conditional on our understanding of the uncertainties in the system is the ultimate goal of the consumers of our models. In this talk, we will introduce a decision problem of advancing therapies to late-stage clinical trials from early-stage clinical data. We do this in the context of a Bayesian Joint Model.\nIn clinical studies, it is common to measure a clinical biomarker repeatedly over time (‘longitudinal data’). It is also common to measure the patient-specific time from a defined origin, e.g. diagnosis of a disease, until a clinical event of interest, such as death or disease progression (‘survival data’). Joint Modeling as it is called in the Survival literature aims to model both the longitudinal biomarker evolutions and survival endpoints simultaneously. Commonly, this is achieved by specifying a joint likelihood formulation for longitudinal and survival outcomes.\nJoint modeling approaches provide several benefits over more traditional modeling and have applications to health through (i) improving the understanding of how biomarkers influence event endpoints; (ii) the development of dynamic risk prediction models for use in personalized medicine; and in the context of clinical trials (iii) requiring fewer patients than the event model alone.\nOnce the inferences from the Joint Model are obtained, we set up a Utility function describing the risk preferences of the trial’s sponsors and take its expectation with respect to the posterior distribution. The resulting function is then maximized.\n\n The State of GPU Computation Support for Stan. Rok Češnovar (University of Ljubljana - UL), Steve Bronder (Capital One), Davor Sluga (UL), Jure Demšar (UL), Tadej Ciglarič (UL), Sean Talts (Columbia University), Erik Štrumbelj (UL).  Video\nAbstract: Our presentations details the current state of and future work on the OpenCL-based framework that allows the Stan automatic differentiation library to utilize GPUs. Our research was initially motivated by large Gaussian Process models where the computation is dominated by the Cholesky decomposition but has since developed into an extensible framework.\nThe following GPU-optimized routines for matrix algebra primitives are already available to Stan users (including reverse mode): matrix multiplication, solving triangular systems, Cholesky decomposition and some special cases. Several support functions are available in the Math library but not exposed to Stan users: matrix initialization, input validity checking, copy, pack/unpack, multiplication by scalar, and transpose. We have made progress on implementing commonly used likelihoods - 4 Generalized Linear Model likelihoods can already be used: normal (identity), Bernoulli (logit), Poisson (log) and Negative Binomial (log). And data caching is now available and substantially reduces the overhead of transferring data to the GPU.\nWe will show how problem size, model and choice of hardware impact the speedups that we can achieve with GPU computation in Stan. Finally, we will discuss directions for future work, routines to implement next, autotuning tunable GPU parameters and advanced data caching.\n\n Extending Stan’s Automatic Differentiation (AD) capabilities using dco/c++. Philip Maybank. Numerical Algorithms Group (NAG)  Video\nAbstract: Tape-based AD Libraries, such as NAG’s dco/c++ tool, keep a record of calculations that are executed by a program in order to evaluate derivatives. They are applicable to a wider range of numerical codes than tape-free AD libraries, which are typically written to compute derivatives for a specific library of functions. The Stan Math Library is a tape-free AD library. The basic idea of the work in this presentation is that dco/c++ can be used to supply derivatives to Stan. This extends the range of functions which can be used by Stan’s MCMC samplers. We illustrate this idea on a toy problem: inferring the parameters of a damped harmonic oscillator driven by white noise using Stan’s NUTS.\n\n Estimating the prevalence of HIV infection in England using Bayesian evidence synthesis. Anne Presanis, Christopher Jackson [presenting author], Daniela De Angelis (MRC Biostatistics Unit, University of Cambridge); Peter Kirwan, Alison Brown, Ada Miltz, Ross Harris, Cuong Chau, Stephanie Migchelsen, Hamish Mohammed, Katy Davison, Sara Croxford, Sarika Desai, Kathy Lowndes, Valerie Delpech, Noel Gill (Public Health England).  Video\nAbstract: We present a substantive application of Stan that has informed national health policy.\nAnnual estimation of the number of people living with HIV in England, including those who are unaware of their infection, has, for several years, been based on a Bayesian model that combines evidence from multiple sources of data. For several demographic and risk groups, the model estimates the number of people in each group, the prevalence of HIV, and the proportion of HIV infections that are diagnosed.\nIn the 2018 version of this model, implemented in Stan, the strata are defined by age, gender, sexual behaviour, injecting drug use, ethnicity and region. Changes between years are also modelled. Routinely-collected data sources include a register of diagnosed HIV infections, a register of attendances at genitourinary medicine (GUM) clinics, and the national census. These are combined with data from several surveys of health and sexual behaviour among different groups, HIV testing data from unlinked anonymous surveys of drug users, and data from HIV testing of donated blood.\nThis is an example of a ““multiparameter evidence synthesis”“, where the quantities of interest cannot be estimated directly, but can be inferred indirectly through a network of model assumptions. Potential biases due to selection, under-reporting and missing data are represented explicitly through structural assumptions and informative priors. A four-level hierarchical model is used to borrow strength between stratum-specific parameters. Stan’s model description language makes the assumptions explicit, and its inference engine provides posterior estimates efficiently.\nThe estimates from 2018 demonstrate that the UN-AIDS target of 90% of infections diagnosed by 2020 has been met in England, and the estimates continue to inform policies around HIV testing, treatment and prevention.\n\n Estimating the effect of age and league on scoring rate in professional soccer. Benjamin Torvaney. Wefarm\nAbstract: Understanding the effect of different factors on player output is critical to accurately evaluating player performance. In particular, it is useful to be able to project performance into the future, whether to assess a potential new signing, or to aid in squad management. To do this, we must account for footballing context. Intuitively, we know that scoring goals in the Norwegian Eliteserien is less impressive than scoring in the Premier League; however, this is rarely quantified.\nIf we propose a model in which a player’s expected goalscoring rate is the product of their ability, the difficulty of the competition, and a relative age effect, we can estimate the effect of each parameter from historical goalscoring tallies (accompanied by minutes played). We can extend the model to allow competition factors to vary over time, to reflect the changing dynamics of professional soccer.\nSuch a model yields promising results: high profile soccer stars have the highest model estimates; a clear age curve for goalscoring is produced; competition strengths vary over time in accordance with popular perception.\n\n Multi-channel Gaussian Processes as flexible alternatives to linear models: perspectives and challenges to scaling up Bayesian inference to genomic-scale data. Caetano Souto-Maior, Susan T. Harbison. Laboratory of Systems Genetics, National Heart Lung and Blood Institute, NIH.\nAbstract:\n\n One weird trick: Non-parametric Bayesian updating by kernels. Robert Grant.  BayesCamp\nAbstract: One of the big attractions for people adopting Bayesian methods is the promise of ““updating”” their parameter estimates and predictions as more data arrive. Yesterday’s posterior becomes today’s prior. In practice, this is not always simple, requiring at the very least a complete set of sufficient statistics, random samples from an unchanging population, and no changes of mind about the probability distribution for the priors. Sometimes, one would like to update without imposing an a priori distribution on yesterday’s posterior and without estimating lots of statistics. I discuss a kernel approach, which is easily incorporated in Stan by an additional target+= statement, looping over yesterday’s posterior draws, and uniform proposal densities. I compare this with parametric updates, and explore the potential to reduce computation by using kernels weighted by counts of posterior draws inside hypercubes of parameter space.\n\n Making Stan Faster using Sequential Monte Carlo samplers. Simon Maskell (University of Liverpool), Alessandro Varsi (University of Liverpool), Peter Green (University of Liverpool), Paul Horridge (University of Liverpool), Alejandro Diaz (University of Liverpool), Lee Devlin (University of Liverpool), Rob Moore (University of Liverpool), Katerina Chatzopoulou (University of Liverpool), Jinglai Li (University of Liverpool), Maria Sudell (University of Liverpool), Luke Mason (STFC), Robin Pinning (STFC), Jack Taylor (STFC), Vassil Alexandrov (STFC), Ed Pyzer-Knapp (IBM). \nAbstract: Stan uses the No U-Turn Sampler (NUTS), a specific instance of Markov Chain Monte Carlo (MCMC). MCMC can be slow, e.g., when dimensionality is high and it would be better if NUTS was faster. We have recently been working to improve the run-time of a solution to problems that Stan can tackle (and those that it cannot, e.g. those that would require reversible jump MCMC). Our approach has been to replace NUTS with a variant of a Sequential Monte Carlo (SMC) sampler that uses the clever ideas embodied in NUTS without coupling them to MCMC specifically. SMC samplers manipulate a population of samples, making it possible to distribute computation across each of many processors. Our work has shown that SMC samplers can be configured to exploit this parallelism (and the advances that have led to the development of, for example, the use of NUTS as a proposal distribution). This can achieve faster run-time than MCMC in terms of the number of effective samples per second (by running the SMC sampler on clusters of hundreds of cores, as are routinely used in the context of Deep Learning, for example). Furthermore, we have shown that SMC samplers can be configured to outperform MCMC by making better use of the available processing resources. This is possible because MCMC’s convergence proofs require that the single sampling chain never goes wrong while the proofs for SMC samplers only require that the samples don’t all malfunction simultaneously. Put another way, SMC samplers have an additional degree of freedom in their design and this degree of freedom can be exploited to offer improved performance relative to MCMC. This talk will explain how SMC samplers can outperform MCMC per second and per flop. We will also describe our progress to date on integrating SMC samplers into Stan: our intent is to make it possible to use all Stan files. Thus far we’re able to achieve a runtime that is over an order of magnitude faster than MCMC.\n\n Gaussian process modeling and covariate selection for longitudinal data. Juho Timonen, Aki Vehtari, Harri Lähdesmäki. Alto University\nAbstract: Longitudinal data arises when the same observational units are measured repeatedly, and is common in clinical studies. Such data is often modeled using generalized linear mixed effect models with off-the-shelf software packages. These are, however, restricted to a parametric form and cannot model non-stationary disease effects. We demonstrate our new R-package for interpretable Bayesian non-parametric modeling of longitudinal data using additive Gaussian processes. Like the R-packages and brms, our goal is to provide an interface to Stan with a simple and intuitive syntax. However, our Stan program is specifically designed for Gaussian process modeling of longitudinal data, allowing the user to specify a model that mixes group and individual-specific age effects or effects of other continuous or categorical covariates. We show how our package uses Stan to model non-stationary disease effects and uncertainty of the observed disease onsets, identify heterogeneous effects present in only a subset of study subjects, and handles general non-Gaussian likelihoods. Furthermore, we define a way of resolving the relevance of any continuous or categorical covariate by sampling only one full model with all covariates included. Our focus is on biomedical applications, where is often vital to determine which covariates affect the response variable, in order to reduce future measurement costs or have a better interpretation about the progression of a disease.\n\n Computing prediction and tolerance intervals for a mixture of normal distributions. Jean-francois Michiels, Timothy Mutsvari, Oussama Errazi. Pharmalex\nAbstract: For the submission of a Biosimilar product, Biosimilarity assessment is the first step to achieve in the “Totality of Evidence” strategy as required by Authorities (e.g. FDA). The main objective of biosimilarity is to give evidence that the test biological product is as similar as possible to the reference product. The definition of ‘similar’ remains a critical component that needs to be addressed and justified. For biologicals, it is the process and its capability that should be evaluated, i.e. the risk of producing batches outside defendable limits. Thus, the first step is to set the acceptance limits. β-expectation and (β,γ), also known as Beta-Gamma, tolerance intervals are useful metrics to demonstrate that a test product (i.e. the biosimilar) is similar to a reference product. Biosimilarity is concluded if the β-expectation of the biosimilar product is within the (β,γ) of the reference. β-expectation interval is constructed to contain a β proportion of the population on average. A (β,γ) tolerance interval on the other hand is built to contain at least a β proportion of the population with a confidence level γ. In general, the pharmaceutical company producing the biosimilar has no access to the data of the reference product. Buying boxes of the reference product from several drugstores and analysing them is nevertheless one possible strategy to acquire knowledge on the process variability. Due to that sampling strategy, the distribution of the reference product can be quite exotic and it is likely that the distribution of the reference product is a mixture of normal distributions. Fitting a mixture of 2 normal distributions on data is performed using Stan. The output are the posterior distributions of the mean and standard deviation of the 2 normal distributions and the posterior distribution of the relative proportion of the 2 distributions. We present different algorithms to derive β-expectation and (β,γ) tolerance intervals for a mixture of 2 normal distributions. Using simulations, the operating characteristics of the intervals are shown (e.g. the capability to conclude similarity when it is actually similar).\n\n Parallel numerical ODE solution in Torsten for population models. Yi Zhang, William R. Gillespie. Metrum LLC\nAbstract: Torsten is a collection of functions to facilitate analysis of pharmacometric data using Stan. To seek an alternative to the ““map_rect”” function for within-chain parallel computation in Stan, we have implemented numerical ODE solution functions for population models with functional signatures that specify schedules of events such as doses and observations in a manner consistent with NONMEM compatible.\nThe population solution function feature is designed toward multi-level parallelization using Message Passing Interface(MPI). For that we first implemented Torsten’s own ODE integrators based on CVODES library. Table 1 shows MPI performance results of such an integrator on a group of 1000 Lorenz systems.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n10986\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n5505\n\n\n2.00\n\n\n1.00\n\n\n\n\n4\n\n\n3091\n\n\n3.55\n\n\n0.89\n\n\n\n\n8\n\n\n1459\n\n\n7.53\n\n\n0.94\n\n\n\n\n16\n\n\n1355\n\n\n8.11\n\n\n0.51\n\n\n\n\n32\n\n\n739\n\n\n14.87\n\n\n0.46\n\n\n\n\n64\n\n\n424\n\n\n25.91\n\n\n0.40\n\n\n\n\n128\n\n\n382\n\n\n28.76\n\n\n0.22\n\n\n\n\n256\n\n\n284\n\n\n38.68\n\n\n0.15\n\n\n\n\n512\n\n\n293\n\n\n37.49\n\n\n0.07\n\n\n\nTable 1: MPI performance of the Lorenz model solved by Torsten’s BDF integrator. (n_population = 1000)\nThen we developed MPI-based population solvers that are specifically designed for PKPD applications, for which ODE system size \\(n\\) is typically in the scale of \\(10^0\\sim 10^2\\). We employ the latest standard(MPI-3) functionalities for latency hiding, and test the implementation on two MPI implementations (OpenMPI and MPICH). Tables 2-5 show performance results of one such function on a simple two-compartment PK model(\\(n=3\\)) and a more complex PKPD model(\\(n=8\\)), run on a METWORX workflow.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n2966\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n1544\n\n\n1.92\n\n\n0.96\n\n\n\n\n4\n\n\n866\n\n\n3.42\n\n\n0.85\n\n\n\n\n8\n\n\n887\n\n\n3.34\n\n\n0.42\n\n\n\nTable 2: Parallel performance of solving a two-compartment population model using pmx_solve_group_bdf and OpenMPI.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n45791\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n23532\n\n\n1.95\n\n\n0.97\n\n\n\n\n4\n\n\n13421\n\n\n3.41\n\n\n0.85\n\n\n\n\n8\n\n\n10394\n\n\n4.41\n\n\n0.55\n\n\n\nTable 3: Parallel performance of solving a Neutropenia population model using pmx_solve_group_bdf and OpenMPI.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n2470\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n1419\n\n\n1.74\n\n\n0.87\n\n\n\n\n4\n\n\n1170\n\n\n2.11\n\n\n0.53\n\n\n\n\n8\n\n\n860\n\n\n2.87\n\n\n0.36\n\n\n\nTable 4: Parallel performance of solving a two-compartment population model using pmx_solve_group_bdf and MPICH.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n45087\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n22976\n\n\n1.96\n\n\n0.98\n\n\n\n\n4\n\n\n14158\n\n\n3.18\n\n\n0.80\n\n\n\n\n8\n\n\n10523\n\n\n4.28\n\n\n0.54\n\n\n\nTable 5: Parallel performance of solving a Neutropenia population model using pmx_solve_group_bdf and MPICH.\nIn addtional to population-level parallelization, we are also implementing individual-level parallelization based on parallel time integration with multigrid. This will enables us to reduce the solution time of a single ODE system, and create a multi-level parallelization for ODE-based population models. The results of a preliminary implementation are shown in Table 6.\n\n\n\nn_processor\n\n\nWalltime(ms)\n\n\nSpeedup\n\n\nefficiency\n\n\n\n\n1\n\n\n2.8\n\n\n1.00\n\n\n1.00\n\n\n\n\n2\n\n\n1.7\n\n\n1.65\n\n\n0.82\n\n\n\n\n4\n\n\n1.2\n\n\n2.33\n\n\n0.58\n\n\n\nTable 6: Parallel performance of solving 10^4 steps of a single Neutropenia ODE system using parallel-in-time technique."
  }
]